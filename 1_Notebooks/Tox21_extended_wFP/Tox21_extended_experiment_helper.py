#!/usr/bin/env python3
# File name: Tox21_extended_experiment_helper.py
## Helper functions for Tox21_extended dataset.
import pandas as pd
import numpy as np
import os, sys
import csv

import dill as pickle

from sklearn.metrics import roc_auc_score

def load_data():
	df = pd.read_csv(os.path.join("data", "XY_Tox21_extended_wFP_filtered.csv"), index_col=0)
	label_start, feat_start = 2, 70
	labels = df.columns[label_start:feat_start]

	train_features = df[df['Type']=="Train"].iloc[:, feat_start:]
	test_features = df[df['Type']=="Test"].iloc[:, feat_start:]
	train_targets= df[df['Type']=="Train"].iloc[:, label_start:feat_start]
	test_targets = df[df['Type']=="Test"].iloc[:, label_start:feat_start]	

	# Fweights_train = pd.read_csv(os.path.join("data", "XY_CoMPARA_CDKPaDELFP_filtered2_Fweights_train.csv")).values
	# Fweights_test = pd.read_csv(os.path.join("data", "XY_CoMPARA_CDKPaDELFP_filtered2_Fweights_test.csv")).values


	r = {'labels': labels,
	'train_features': train_features,
	'test_features': test_features, 
	'train_targets': train_targets,
	'test_targets': test_targets,
	# 'Fweights_train': Fweights_train,
	# 'Fweights_test': Fweights_test,
	}

	return r

def get_valid_ind(y):
    ind = np.where(~np.isnan(y.astype(np.float32)))[0]
    return ind

def file_updater(file_path, rows, mode='a'):
	with open(file_path, mode, newline='', encoding='utf-8') as f:
		writer=csv.writer(f)
		for row in rows:
			writer.writerow(row)

def data_scaling(feature_scaler,
                 target_scaler,
                 train_features,
                 test_features,
                 train_target,
                 test_target
                 ):
    # Scaling
    feature_scaler.fit(train_features)
    train_features_scaled = feature_scaler.transform(train_features)
    test_features_scaled = feature_scaler.transform(test_features)

    if target_scaler is not None:
        target_scaler.fit(np.expand_dims(train_target, axis=1))
        train_target_scaled = target_scaler.transform(
            np.expand_dims(train_target, axis=1)).flatten()
        test_target_scaled = target_scaler.transform(
            np.expand_dims(test_target, axis=1)).flatten()
    else:
        train_target_scaled = train_target
        test_target_scaled = test_target
    return train_features_scaled, test_features_scaled, train_target_scaled, test_target_scaled, feature_scaler, target_scaler

def to_multiclass(y_train):
    #To obtain multiclass class, take dot product of labels (e.g. 0,1,0,1) with vector of 2^x (e.g. 1,2,4,8)
    n_labels = y_train.shape[1]
    n_classes = 2**n_labels
    y_train = np.dot(y_train, np.logspace(0,n_labels-1, num=n_labels, base=2))
    return y_train, n_classes

import itertools
def binarize(y_train, n_labels, label_idx):
    #Given multiclass output, find binary output for desired label_idx
    all_classes = list(itertools.product((0,1), repeat=n_labels))
    pos_classes = np.where([i[label_idx]==1 for i in all_classes])[0]
    y_train = np.isin(y_train, pos_classes).astype(int)
    return y_train

def remove_empty_classes(y_train, y_test = None):
    #Returns y_train with maximum multiclass label and n_classes-1. Returns dictionary for the conversion
    uniques=np.unique(y_train).astype(int)
    ret_dict={}
    y = np.empty_like(y_train)
    if y_test is not None:
        y2=np.empty_like(y_test)
    for i in range(len(uniques)):
        ret_dict[i] = uniques[i]
        y[y_train==uniques[i]]=i
        if y_test is not None:
            y2[y_test==uniques[i]]=i
    if y_test is not None:
        return y, ret_dict, y2
    return y, ret_dict

def return_empty_classes(y_train, ret_dict):
    #Opposite of remove_empty_classes
    uniques=np.unique(y_train).astype(int)
    y=np.empty_like(y_train)
    for i in uniques:
        y[y_train==i]=ret_dict[i]
    return y

import xgboost as xgb
from sklearn.utils.class_weight import compute_sample_weight
def tune_xgbc(params, X_train, y_train, X_test, y_test, multiclass=False):
# Implementation learned on a lesson of Mario Filho (Kagle Grandmaster) for parametes optmization.
# Link to the video: https://www.youtube.com/watch?v=WhnkeasZNHI

    """Function to be passed as scikit-optimize minimizer/maximizer input

    Parameters:
    Tuples with information about the range that the optimizer should use for that parameter, 
    as well as the behaviour that it should follow in that range.

    Returns:
    float: the metric that should be minimized. If the objective is maximization, then the negative 
    of the desired metric must be returned. In this case, the negative AUC average generated by CV is returned.
    """

    #Hyperparameters to be optimized
    print(params)
    learning_rate = params[0] 
    n_estimators = params[1] 
    max_depth = params[2]
    min_child_weight = params[3]
    gamma = params[4]
    subsample = params[5]
    colsample_bytree = params[6]


    #Model to be optimized
    objective= "reg:logistic"
    if multiclass:
        objective = "multi:softprob"
    mdl = xgb.XGBClassifier(learning_rate = learning_rate, 
                            n_estimators = n_estimators, 
                            max_depth = max_depth, 
                            min_child_weight = min_child_weight, 
                            gamma = gamma, 
                            subsample = subsample, 
                            colsample_bytree = colsample_bytree, 
                            objective=objective,
                            seed = 42)

    sample_weight = compute_sample_weight("balanced", y_train)

    # #Cross-Validation in order to avoid overfitting
    # auc = cross_val_score(mdl, X_train_selected, y_train, cv = 10, scoring = 'roc_auc')

    mdl.fit(X_train, y_train, sample_weight = sample_weight)
    test_predict = mdl.predict_proba(X_test)
    if not multiclass:
        test_predict = test_predict[:, 1]
    y_test = y_test.astype(int)
    if not multiclass:
        test_auc = roc_auc_score(y_test, test_predict, multi_class = 'ovr')
    else:
        test_auc = []
        for idx in np.unique(y_test):
            test_auc.append(roc_auc_score((y_test==idx).astype(int), 
                test_predict[:,idx],
                multi_class = 'ovr'))
    print(np.average(test_auc))
    # as the function is minimization (forest_minimize), we need to use the negative of the desired metric (AUC)
    return -np.average(test_auc)
    #return -test_auc.mean()

# Creating a sample space in which the initial randomic search should be performed
XGB_tune_space = [(1e-3, 1e-1, 'log-uniform'), # learning rate
          (100, 2000), # n_estimators
          (1, 10), # max_depth 
          (1, 6.), # min_child_weight 
          (0, 0.5), # gamma 
          (0.5, 1.), # subsample 
          (0.5, 1.)] # colsample_bytree 
from functools import partial
from skopt import forest_minimize

def XGBTuneFit(X_train, X_test, y_train, y_test, target_scaler, 
    model_name, label_idx, label, model_path, 
    multiclass=False, n_classes = None, 
    X_train_all = None, X_test_all=None, 
    y_test_all = None, 
    forest_minimize_kwargs={}):
    train_sample_weight = compute_sample_weight("balanced", y_train)
    #Find optimal params
    func = partial(
        tune_xgbc,
        X_train = X_train,
        y_train = y_train, 
        X_test = X_test, 
        y_test = y_test, 
        multiclass=multiclass,
    )
    kwargs = {
       "func": func,
       "dimensions": XGB_tune_space,
       "random_state": 42,
       "n_random_starts": 20, 
       "n_calls": 25,
       "verbose": 1
    }
    kwargs.update(forest_minimize_kwargs)
    results = forest_minimize(**kwargs)
    with open(os.path.join(model_path+"_params_optimizer.ob"), 'wb') as f:
        pickle.dump(results, f)
    # with open(os.path.join(model_path+"_params_optimizer.ob"), 'rb') as f:
    #     results = pickle.load(f)
    params = results['x']

    learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree = params

    objective = "binary:logistic"
    eval_metric="logloss"
    if multiclass: 
        objective = "multi:softprob"
        eval_metric="mlogloss"

    clf = xgb.XGBClassifier(
        use_label_encoder = False,
        objective = objective,
        eval_metric = eval_metric,
        learning_rate = learning_rate, 
        n_estimators = n_estimators, 
        max_depth = max_depth, 
        min_child_weight = min_child_weight, 
        gamma = gamma, 
        subsample = subsample, 
        colsample_bytree = colsample_bytree, 
        n_jobs = -1, 
    )
    clf.fit(
        X_train, 
        y_train, 
        sample_weight = train_sample_weight
    )
    clf.save_model(os.path.join(model_path+"_model"))
    # clf.load_model(os.path.join(model_path+"_model"))
    if multiclass:
        train_predict = clf.predict(X_train_all)
        test_predict = clf.predict(X_test_all)
        return train_predict, test_predict
        # train_results = multiclass_model_eval(
        #     y_train, train_predict, target_scaler, model_name, label, label_idx, split_name = "Train")
        # test_results = multiclass_model_eval(
        #     y_test_all, test_predict, target_scaler, model_name, label, label_idx, split_name = "Test"
        #     )
    else:
        train_predict = clf.predict_proba(X_train)[:,1]
        test_predict = clf.predict_proba(X_test)[:,1]
        train_results = model_eval(y_train, train_predict, target_scaler, model_name, label, "Train")
        test_results = model_eval(y_test, test_predict, target_scaler, model_name, label, "Test")
        train_results[0].append(label_idx)
        test_results[0].append(label_idx)

    if X_train_all is not None:
        train_predict_all = clf.predict_proba(X_train_all)
        test_predict_all = clf.predict_proba(X_test_all)
        if not multiclass:
            train_predict_all = train_predict_all[:,1]
            test_predict_all = test_predict_all[:,1]
        return train_results, test_results, train_predict_all, test_predict_all
    return train_results, test_results

def XGBLoadFit(X_train, X_test, y_train, y_test, target_scaler, params_path,
    model_name, label_idx, label, model_path, 
    multiclass=False, n_classes = None, 
    X_train_all=None,
    X_test_all = None, y_test_all = None, 
    ):
    train_sample_weight = compute_sample_weight("balanced", y_train)
    #Load optimal params
    with open(os.path.join(params_path), 'rb') as f:
        results=pickle.load(f)
    params = results['x']

    learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree = params

    clf = xgb.XGBClassifier(
        use_label_encoder = False,
        objective = "binary:logistic",
        eval_metric = "logloss",
        learning_rate = learning_rate, 
        n_estimators = n_estimators, 
        max_depth = max_depth, 
        min_child_weight = min_child_weight, 
        gamma = gamma, 
        subsample = subsample, 
        colsample_bytree = colsample_bytree, 
        n_jobs = -1, 
    )
    clf.fit(
        X_train, 
        y_train, 
        sample_weight = train_sample_weight
    )
    clf.save_model(os.path.join(model_path+"_model"))


    if multiclass:
        train_predict = clf.predict_proba(X_train)
        test_predict = clf.predict_proba(X_test_all)
        return train_predict, test_predict
        # train_results = multiclass_model_eval(
        #     y_train, train_predict, target_scaler, model_name, label, label_idx, split_name = "Train")
        # test_results = multiclass_model_eval(
        #     y_test_all, test_predict, target_scaler, model_name, label, label_idx, split_name = "Test"
        #     )
    else:
        train_predict = clf.predict_proba(X_train)[:,1]
        test_predict = clf.predict_proba(X_test)[:,1]
        train_results = model_eval(y_train, train_predict, target_scaler, model_name, label, "Train")
        test_results = model_eval(y_test, test_predict, target_scaler, model_name, label, "Test")
        train_results[0].append(label_idx)
        test_results[0].append(label_idx)

    if X_train_all is not None:
        train_predict_all = clf.predict_proba(X_train_all)
        test_predict_all = clf.predict_proba(X_test_all)
        if not multiclass:
            train_predict_all = train_predict_all[:,1]
            test_predict_all = test_predict_all[:,1]
        return train_results, test_results, train_predict_all, test_predict_all        
    return train_results, test_results

#Evaluation by AUC
def model_eval(y_true, y_predict, target_scaler, model_name, label, split_name="Train"):
	if target_scaler is not None:
		y_predict = target_scaler.inverse_transform(y_predict).flatten()
	auc = roc_auc_score(y_true, y_predict)

	results = [
		[model_name, label, "AUC", split_name, auc]
	]
	return results

def multiclass_model_eval(y_true, predict_proba, target_scaler, model_name, labels, label_ind, split_name="Train"):
    results = []
    for idx, label_idx in enumerate(label_ind):
        y_predict = predict_proba[:, idx]
        target = y_true[:, idx]
        if target_scaler is not None:
            y_predict = target_scaler.inverse_transform(y_predict.flatten()) 
        valid_ind = get_valid_ind(target)       
        auc = roc_auc_score(target[valid_ind], y_predict)

        results.append([model_name, labels[idx], "AUC", split_name, auc, label_idx])
    return results