{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally-Linear Deep Learner on Synthetic Data\n",
    "Uses attention_model (v3). Additions:  \n",
    "    - Batch Normalization to the hidden layer after concat.    \n",
    "    - Similarity batching (to be implemented)  \n",
    "    - Softmax layer/Activity Regularizer (?) for Concat layer  \n",
    "Synth data: (10-dimension with cluster-specific noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TMPDIR=/tmp/temp\n"
     ]
    }
   ],
   "source": [
    "%env TMPDIR=/tmp/temp \n",
    "#For joblib multi-threading\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "code_folder=os.path.join(os.getcwd(), \"..\", \"..\", \"0_code\")\n",
    "sys.path.append(code_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"SynthData_10dim_clusternoise.csv\",\n",
    "               index_col=0\n",
    "              )\n",
    "Fweights_df=pd.read_csv(\"Fweights.csv\")\n",
    "Fweights_test_df=pd.read_csv(\"Fweights_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>cluster_labels</th>\n",
       "      <th>Class</th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.578484</td>\n",
       "      <td>-1.959433</td>\n",
       "      <td>-0.637669</td>\n",
       "      <td>1.834927</td>\n",
       "      <td>-0.795933</td>\n",
       "      <td>-0.989418</td>\n",
       "      <td>-0.975603</td>\n",
       "      <td>0.837159</td>\n",
       "      <td>-0.806057</td>\n",
       "      <td>-1.151965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300661</td>\n",
       "      <td>1.084462</td>\n",
       "      <td>1.453934</td>\n",
       "      <td>1.188252</td>\n",
       "      <td>-1.842271</td>\n",
       "      <td>0.979053</td>\n",
       "      <td>-0.662538</td>\n",
       "      <td>1.058707</td>\n",
       "      <td>-0.204496</td>\n",
       "      <td>0.741998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Training</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.473968</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-0.504463</td>\n",
       "      <td>0.676847</td>\n",
       "      <td>-1.335443</td>\n",
       "      <td>-0.068589</td>\n",
       "      <td>0.397734</td>\n",
       "      <td>-1.835240</td>\n",
       "      <td>-0.494869</td>\n",
       "      <td>-0.483825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Training</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.386544</td>\n",
       "      <td>0.385803</td>\n",
       "      <td>0.315255</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.041539</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>-1.616635</td>\n",
       "      <td>-0.335498</td>\n",
       "      <td>1.192081</td>\n",
       "      <td>0.282956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Training</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994573</td>\n",
       "      <td>1.794217</td>\n",
       "      <td>-0.156877</td>\n",
       "      <td>1.806804</td>\n",
       "      <td>-0.165868</td>\n",
       "      <td>-0.320657</td>\n",
       "      <td>1.014013</td>\n",
       "      <td>1.703030</td>\n",
       "      <td>-1.216164</td>\n",
       "      <td>1.149043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type  cluster_labels  Class  feat_000  feat_001  feat_002  feat_003  \\\n",
       "0  Training               1      1 -1.578484 -1.959433 -0.637669  1.834927   \n",
       "1  Training               3      1 -0.300661  1.084462  1.453934  1.188252   \n",
       "2  Training               0      1  1.473968 -0.632456 -0.504463  0.676847   \n",
       "3  Training               4      0  1.386544  0.385803  0.315255  1.005859   \n",
       "4  Training               3      0 -0.994573  1.794217 -0.156877  1.806804   \n",
       "\n",
       "   feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  \n",
       "0 -0.795933 -0.989418 -0.975603  0.837159 -0.806057 -1.151965  \n",
       "1 -1.842271  0.979053 -0.662538  1.058707 -0.204496  0.741998  \n",
       "2 -1.335443 -0.068589  0.397734 -1.835240 -0.494869 -0.483825  \n",
       "3  0.041539  0.130468 -1.616635 -0.335498  1.192081  0.282956  \n",
       "4 -0.165868 -0.320657  1.014013  1.703030 -1.216164  1.149043  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fweights_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'cluster_labels', 'Class', 'feat_000', 'feat_001', 'feat_002',\n",
       "       'feat_003', 'feat_004', 'feat_005', 'feat_006', 'feat_007', 'feat_008',\n",
       "       'feat_009'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df[df['Type']==\"Training\"].iloc[:, 3:]\n",
    "X_test=df[df['Type']==\"Testing\"].iloc[:,3:]\n",
    "y_train=df[df['Type']==\"Training\"].iloc[:,2]\n",
    "y_test=df[df['Type']==\"Testing\"].iloc[:,2]\n",
    "Fweights_train=Fweights_df.values\n",
    "Fweights_test=Fweights_test_df.values\n",
    "\n",
    "cluster_labels_train=df[df['Type']=='Training']['cluster_labels']\n",
    "cluster_labels_test=df[df['Type']=='Testing']['cluster_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=X_train.values\n",
    "train_targets=np.expand_dims(y_train, axis=1).astype(np.float32)\n",
    "test_data=X_test.values\n",
    "test_targets=np.expand_dims(y_test, axis=1).astype(np.float32)\n",
    "train_Fweights=Fweights_train\n",
    "test_Fweights=Fweights_test\n",
    "\n",
    "train_tensor=np.hstack([train_data, train_Fweights])\n",
    "test_tensor=np.hstack([test_data, test_Fweights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LLDL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated 6th Jan 2021 (Edited Line 71 to Line 72. Reduce_mean instead of mean, to preserve the required rank)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "epsilon=K.epsilon\n",
    "\n",
    "def get_weights_dicts(Y):\n",
    "    weights_dicts=[]\n",
    "    for j in range(Y.shape[1]):\n",
    "        weight_zero, weight_one = _get_label_weights(Y[:,j])\n",
    "        d={'weight_zero':weight_zero,\n",
    "           'weight_one':weight_one\n",
    "          }\n",
    "        weights_dicts.append(d)\n",
    "    return weights_dicts\n",
    "def _get_label_weights(y):\n",
    "    #Get label weights for majority and minority class using the following:\n",
    "        #major_weight=n/(n_major*2)\n",
    "        #minor_weight=n*(n_major/n_minor)/(n_major*2)\n",
    "        #NaN weights are set to zero\n",
    "    y1=y[~np.isnan(y)]\n",
    "    n=len(y1)\n",
    "    n_zero=np.count_nonzero(np.isclose(y1,0))\n",
    "    n_one=np.count_nonzero(np.isclose(y1,1))\n",
    "    if n_zero>n_one:\n",
    "        weight_zero=n/(n_zero*2)\n",
    "        weight_one=n*(n_zero/n_one)/(n_zero*2)\n",
    "    else:\n",
    "        weight_zero=n*(n_one/n_zero)/(n_one*2)\n",
    "        weight_one=n/(n_one*2)\n",
    "    return weight_zero, weight_one    \n",
    "\n",
    "class BinaryCrossEntropyIgnoreNaN(tf.keras.losses.Loss):\n",
    "    def __init__(self, weights_dicts=None, axis=0, **kwargs):\n",
    "        super(BinaryCrossEntropyIgnoreNaN, self).__init__(**kwargs)\n",
    "        self.weights_dicts=weights_dicts\n",
    "        self.axis=axis        \n",
    "\n",
    "    def __call__(self, target, output, sample_weight=None):\n",
    "        #Binary cross entropy that ignores Nan and replaces with mini-batch Nan with 0\n",
    "        #modified from tf.python.keras.backend.binary_crossentropy\n",
    "        \n",
    "        ##NEED TO TEST THIS CODE MORE THOROUGHLY\n",
    "        target=tf.convert_to_tensor(target)\n",
    "        output=tf.convert_to_tensor(output)\n",
    "        if len(target.shape)==1:\n",
    "            target=tf.expand_dims(target, 1)\n",
    "            output=tf.expand_dims(output, 1)\n",
    "        epsilon_ = tf.constant(epsilon(), dtype=output.dtype.base_dtype)\n",
    "        output=tf.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
    "\n",
    "        #Compute cross entropy from probabilities\n",
    "        bce=target * tf.math.log(output+epsilon_)\n",
    "        bce+=(1-target)* tf.math.log(1-output+epsilon_)\n",
    "\n",
    "        bce=tf.where(tf.math.is_nan(-bce), epsilon(), -bce)\n",
    "        if self.weights_dicts is not None:\n",
    "            sample_weight=tf.cast(tf.where(target==0.,1.,0.)*[self.weights_dicts[i]['weight_zero'] for i in range(len(self.weights_dicts))], dtype=target.dtype)\n",
    "            sample_weight+=tf.cast(tf.where(target==1., 1., 0.)*[self.weights_dicts[i]['weight_one'] for i in range(len(self.weights_dicts))], dtype=target.dtype)\n",
    "            bce=tf.multiply(sample_weight, bce)\n",
    "#         return tf.keras.backend.mean(bce, axis=self.axis)  \n",
    "        return tf.math.reduce_mean(bce)\n",
    "\n",
    "    def call(self, target, output, sample_weight=None):\n",
    "        return self(target, output, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = train_data.shape[1]\n",
    "n_attention = 10 #Reduced from 20 to 10. 10 works better\n",
    "n_attention_hidden=40\n",
    "n_attention_out=1\n",
    "n_concat_hidden=128\n",
    "n_hidden1 =64\n",
    "n_hidden2 = 64\n",
    "momentum=0.8\n",
    "learning_rate=0.01\n",
    "\n",
    "n_batch=32\n",
    "\n",
    "label=\"SynthData\"\n",
    "\n",
    "save_folder=os.path.join(time.strftime(\"%y%m%d_TrainingLocalitySensitivewFW\",\n",
    "                                       time.localtime()))\n",
    "checkpoint_path = os.path.join(save_folder, \n",
    "                               \"LocalitySensitivewFW_{}\".format(\"label\"),\n",
    "                               )\n",
    "\n",
    "try: \n",
    "    os.mkdir(save_folder) \n",
    "except OSError as error: \n",
    "    print(error) \n",
    "    \n",
    "try:\n",
    "    os.mkdir(checkpoint_path)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "concat_activation=\"selu\"\n",
    "attention_hidden_activation=\"selu\"\n",
    "attention_output_activation=\"sigmoid\"\n",
    "kernel_initializer=VarianceScaling()\n",
    "hidden_activation=\"selu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-11a37aafa2a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'attention_model' is not defined"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(attention_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import attention_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_layer=Input(shape=(n_feat*2, ))\n",
    "\n",
    "attentions_layer=attention_model.ConcatAttentionswFeatWeights(\n",
    "    n_attention=n_attention,\n",
    "    n_attention_hidden=n_attention_hidden,\n",
    "    n_attention_out=n_attention_out,\n",
    "    n_feat=n_feat,\n",
    "    n_hidden=n_concat_hidden,\n",
    "    activation=concat_activation, \n",
    "    kernel_initializer=kernel_initializer,\n",
    "    kernel_regularizer=l2(1E-5),\n",
    "    bias_regularizer=l2(1E-5),\n",
    "    attention_initializer=kernel_initializer,\n",
    "    attention_hidden_activation=attention_hidden_activation,\n",
    "    attention_output_activation=attention_output_activation,\n",
    "    batch_norm_kwargs={\"trainable\":False, \"renorm\":False},\n",
    ")(input_layer)\n",
    "##Removed dropout for attentions_layer because of Batch normalization\n",
    "# dropout0=Dropout(0.1)(attentions_layer)\n",
    "dense_layer1=Dense(n_hidden1, \n",
    "                   activation=hidden_activation, \n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5),\n",
    "                  )(attentions_layer)\n",
    "# dropout1=Dropout(0.1)(dense_layer1)\n",
    "dense_layer2=Dense(n_hidden2,\n",
    "                   activation=hidden_activation,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5)\n",
    "                  )(dense_layer1)\n",
    "# dropout2=Dropout(0.1)(dense_layer2)\n",
    "output_layer=Dense(1, activation=\"sigmoid\")(dense_layer2)\n",
    "\n",
    "LSwFW_model=Model(inputs=input_layer, \n",
    "                  outputs=output_layer\n",
    "                 )\n",
    "\n",
    "weights_dicts=get_weights_dicts(np.expand_dims(train_targets,1))\n",
    "loss_fn=BinaryCrossEntropyIgnoreNaN(weights_dicts=weights_dicts)\n",
    "\n",
    "# loss_fn=tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "LSwFW_model.compile(loss=loss_fn,\n",
    "    #loss=BinaryCrossentropy(from_logits=False, \n",
    "#                                             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "#                                            ), \n",
    "              optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy',]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "concat_attentionsw_feat_weig (None, 128)               18350     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 30,831\n",
      "Trainable params: 30,319\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSwFW_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 - 5s - loss: 0.7421 - accuracy: 0.6478 - val_loss: 0.8628 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 2/200\n",
      "29/29 - 1s - loss: 0.5585 - accuracy: 0.7289 - val_loss: 0.5914 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.53000 to 0.68000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 3/200\n",
      "29/29 - 1s - loss: 0.4658 - accuracy: 0.7967 - val_loss: 0.7351 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.68000\n",
      "Epoch 4/200\n",
      "29/29 - 1s - loss: 0.4525 - accuracy: 0.7956 - val_loss: 0.6440 - val_accuracy: 0.7300\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.68000 to 0.73000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 5/200\n",
      "29/29 - 1s - loss: 0.3584 - accuracy: 0.8533 - val_loss: 0.7748 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.73000\n",
      "Epoch 6/200\n",
      "29/29 - 1s - loss: 0.4365 - accuracy: 0.8200 - val_loss: 0.6562 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.73000 to 0.75000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 7/200\n",
      "29/29 - 1s - loss: 0.4054 - accuracy: 0.8200 - val_loss: 0.6805 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.75000 to 0.78000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 8/200\n",
      "29/29 - 1s - loss: 0.3976 - accuracy: 0.8400 - val_loss: 0.6140 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.78000\n",
      "Epoch 9/200\n",
      "29/29 - 1s - loss: 0.3266 - accuracy: 0.8544 - val_loss: 0.4981 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.78000 to 0.79000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 10/200\n",
      "29/29 - 1s - loss: 0.3289 - accuracy: 0.8656 - val_loss: 0.6530 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.79000\n",
      "Epoch 11/200\n",
      "29/29 - 1s - loss: 0.3267 - accuracy: 0.8600 - val_loss: 0.5430 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.79000\n",
      "Epoch 12/200\n",
      "29/29 - 1s - loss: 0.3028 - accuracy: 0.8867 - val_loss: 0.6613 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.79000\n",
      "Epoch 13/200\n",
      "29/29 - 1s - loss: 0.2435 - accuracy: 0.9033 - val_loss: 0.5754 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.79000\n",
      "Epoch 14/200\n",
      "29/29 - 1s - loss: 0.3158 - accuracy: 0.8611 - val_loss: 0.6344 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.79000\n",
      "Epoch 15/200\n",
      "29/29 - 1s - loss: 0.2597 - accuracy: 0.8978 - val_loss: 0.6935 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.79000\n",
      "Epoch 16/200\n",
      "29/29 - 1s - loss: 0.2550 - accuracy: 0.8956 - val_loss: 0.6226 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.79000 to 0.80000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 17/200\n",
      "29/29 - 1s - loss: 0.2663 - accuracy: 0.8956 - val_loss: 0.5450 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.80000 to 0.82000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 18/200\n",
      "29/29 - 1s - loss: 0.2628 - accuracy: 0.9033 - val_loss: 0.6445 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.82000\n",
      "Epoch 19/200\n",
      "29/29 - 1s - loss: 0.2867 - accuracy: 0.8811 - val_loss: 0.5938 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.82000\n",
      "Epoch 20/200\n",
      "29/29 - 1s - loss: 0.2184 - accuracy: 0.9089 - val_loss: 0.7510 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.82000\n",
      "Epoch 21/200\n",
      "29/29 - 1s - loss: 0.2638 - accuracy: 0.8944 - val_loss: 0.7200 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.82000\n",
      "Epoch 22/200\n",
      "29/29 - 1s - loss: 0.2515 - accuracy: 0.9033 - val_loss: 0.6487 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.82000\n",
      "Epoch 23/200\n",
      "29/29 - 1s - loss: 0.2528 - accuracy: 0.9111 - val_loss: 0.5771 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.82000 to 0.84000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 24/200\n",
      "29/29 - 1s - loss: 0.2078 - accuracy: 0.9200 - val_loss: 0.6067 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.84000 to 0.85000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 25/200\n",
      "29/29 - 1s - loss: 0.1865 - accuracy: 0.9278 - val_loss: 0.6824 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.85000\n",
      "Epoch 26/200\n",
      "29/29 - 1s - loss: 0.2188 - accuracy: 0.9222 - val_loss: 0.5677 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.85000\n",
      "Epoch 27/200\n",
      "29/29 - 1s - loss: 0.1952 - accuracy: 0.9300 - val_loss: 0.6770 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.85000\n",
      "Epoch 28/200\n",
      "29/29 - 1s - loss: 0.2601 - accuracy: 0.8967 - val_loss: 0.7216 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.85000\n",
      "Epoch 29/200\n",
      "29/29 - 1s - loss: 0.2256 - accuracy: 0.9222 - val_loss: 0.6631 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.85000\n",
      "Epoch 30/200\n",
      "29/29 - 1s - loss: 0.1858 - accuracy: 0.9356 - val_loss: 0.5844 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.85000\n",
      "Epoch 31/200\n",
      "29/29 - 1s - loss: 0.2013 - accuracy: 0.9222 - val_loss: 0.5204 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.85000\n",
      "Epoch 32/200\n",
      "29/29 - 1s - loss: 0.2115 - accuracy: 0.9222 - val_loss: 0.6603 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.85000\n",
      "Epoch 33/200\n",
      "29/29 - 1s - loss: 0.1798 - accuracy: 0.9344 - val_loss: 0.8654 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.85000\n",
      "Epoch 34/200\n",
      "29/29 - 1s - loss: 0.2429 - accuracy: 0.9233 - val_loss: 0.5798 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.85000\n",
      "Epoch 35/200\n",
      "29/29 - 1s - loss: 0.2279 - accuracy: 0.9167 - val_loss: 0.6506 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.85000\n",
      "Epoch 36/200\n",
      "29/29 - 1s - loss: 0.2205 - accuracy: 0.9222 - val_loss: 0.8078 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.85000\n",
      "Epoch 37/200\n",
      "29/29 - 1s - loss: 0.1544 - accuracy: 0.9411 - val_loss: 0.6984 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.85000\n",
      "Epoch 38/200\n",
      "29/29 - 1s - loss: 0.1917 - accuracy: 0.9378 - val_loss: 0.5835 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.85000\n",
      "Epoch 39/200\n",
      "29/29 - 1s - loss: 0.1592 - accuracy: 0.9444 - val_loss: 0.5335 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.85000\n",
      "Epoch 40/200\n",
      "29/29 - 1s - loss: 0.1624 - accuracy: 0.9422 - val_loss: 0.6451 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85000\n",
      "Epoch 41/200\n",
      "29/29 - 1s - loss: 0.1556 - accuracy: 0.9411 - val_loss: 0.6084 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85000\n",
      "Epoch 42/200\n",
      "29/29 - 1s - loss: 0.2211 - accuracy: 0.9267 - val_loss: 0.6298 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85000\n",
      "Epoch 43/200\n",
      "29/29 - 1s - loss: 0.1476 - accuracy: 0.9500 - val_loss: 0.6186 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85000\n",
      "Epoch 44/200\n",
      "29/29 - 1s - loss: 0.2090 - accuracy: 0.9344 - val_loss: 0.6106 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.85000\n",
      "Epoch 45/200\n",
      "29/29 - 1s - loss: 0.1382 - accuracy: 0.9500 - val_loss: 0.6667 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.85000\n",
      "Epoch 46/200\n",
      "29/29 - 1s - loss: 0.1347 - accuracy: 0.9611 - val_loss: 0.6123 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.85000\n",
      "Epoch 47/200\n",
      "29/29 - 1s - loss: 0.1709 - accuracy: 0.9378 - val_loss: 0.6501 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.85000\n",
      "Epoch 48/200\n",
      "29/29 - 1s - loss: 0.1758 - accuracy: 0.9422 - val_loss: 0.6544 - val_accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.85000\n",
      "Epoch 49/200\n",
      "29/29 - 1s - loss: 0.1503 - accuracy: 0.9411 - val_loss: 0.6484 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.85000\n",
      "Epoch 50/200\n",
      "29/29 - 1s - loss: 0.1516 - accuracy: 0.9522 - val_loss: 0.5435 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00050: val_accuracy improved from 0.85000 to 0.88000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 51/200\n",
      "29/29 - 1s - loss: 0.1783 - accuracy: 0.9367 - val_loss: 0.5013 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.88000\n",
      "Epoch 52/200\n",
      "29/29 - 1s - loss: 0.1676 - accuracy: 0.9433 - val_loss: 0.5412 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.88000\n",
      "Epoch 53/200\n",
      "29/29 - 1s - loss: 0.1171 - accuracy: 0.9589 - val_loss: 0.6692 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.88000\n",
      "Epoch 54/200\n",
      "29/29 - 1s - loss: 0.1420 - accuracy: 0.9556 - val_loss: 0.5610 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.88000\n",
      "Epoch 55/200\n",
      "29/29 - 1s - loss: 0.1085 - accuracy: 0.9633 - val_loss: 0.6556 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.88000\n",
      "Epoch 56/200\n",
      "29/29 - 1s - loss: 0.1530 - accuracy: 0.9511 - val_loss: 0.7882 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.88000\n",
      "Epoch 57/200\n",
      "29/29 - 1s - loss: 0.1480 - accuracy: 0.9544 - val_loss: 0.6235 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.88000\n",
      "Epoch 58/200\n",
      "29/29 - 1s - loss: 0.1717 - accuracy: 0.9456 - val_loss: 0.7347 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.88000\n",
      "Epoch 59/200\n",
      "29/29 - 1s - loss: 0.1428 - accuracy: 0.9556 - val_loss: 0.6464 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.88000\n",
      "Epoch 60/200\n",
      "29/29 - 1s - loss: 0.3070 - accuracy: 0.8856 - val_loss: 0.6877 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.88000\n",
      "Epoch 61/200\n",
      "29/29 - 1s - loss: 0.2096 - accuracy: 0.9200 - val_loss: 0.6549 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.88000\n",
      "Epoch 62/200\n",
      "29/29 - 1s - loss: 0.3008 - accuracy: 0.9100 - val_loss: 0.7959 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.88000\n",
      "Epoch 63/200\n",
      "29/29 - 1s - loss: 0.2160 - accuracy: 0.9200 - val_loss: 0.8506 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.88000\n",
      "Epoch 64/200\n",
      "29/29 - 1s - loss: 0.1655 - accuracy: 0.9356 - val_loss: 0.7954 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.88000\n",
      "Epoch 65/200\n",
      "29/29 - 1s - loss: 0.1654 - accuracy: 0.9433 - val_loss: 0.8400 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.88000\n",
      "Epoch 66/200\n",
      "29/29 - 1s - loss: 0.2153 - accuracy: 0.9211 - val_loss: 0.6197 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.88000\n",
      "Epoch 67/200\n",
      "29/29 - 1s - loss: 0.2198 - accuracy: 0.9267 - val_loss: 0.6592 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.88000\n",
      "Epoch 68/200\n",
      "29/29 - 1s - loss: 0.2377 - accuracy: 0.9056 - val_loss: 0.9925 - val_accuracy: 0.7200\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.88000\n",
      "Epoch 69/200\n",
      "29/29 - 1s - loss: 0.2344 - accuracy: 0.9167 - val_loss: 0.6768 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.88000\n",
      "Epoch 70/200\n",
      "29/29 - 1s - loss: 0.1890 - accuracy: 0.9333 - val_loss: 0.7896 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.88000\n",
      "Epoch 71/200\n",
      "29/29 - 1s - loss: 0.1735 - accuracy: 0.9278 - val_loss: 0.7718 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.88000\n",
      "Epoch 72/200\n",
      "29/29 - 1s - loss: 0.1643 - accuracy: 0.9444 - val_loss: 0.8467 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.88000\n",
      "Epoch 73/200\n",
      "29/29 - 1s - loss: 0.1522 - accuracy: 0.9433 - val_loss: 0.8451 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.88000\n",
      "Epoch 74/200\n",
      "29/29 - 1s - loss: 0.1688 - accuracy: 0.9422 - val_loss: 0.8953 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.88000\n",
      "Epoch 75/200\n",
      "29/29 - 1s - loss: 0.1259 - accuracy: 0.9600 - val_loss: 0.8594 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.88000\n",
      "Epoch 76/200\n",
      "29/29 - 1s - loss: 0.1974 - accuracy: 0.9133 - val_loss: 0.8196 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.88000\n",
      "Epoch 77/200\n",
      "29/29 - 1s - loss: 0.1595 - accuracy: 0.9411 - val_loss: 0.9045 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.88000\n",
      "Epoch 78/200\n",
      "29/29 - 1s - loss: 0.1563 - accuracy: 0.9433 - val_loss: 0.6979 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.88000\n",
      "Epoch 79/200\n",
      "29/29 - 1s - loss: 0.1451 - accuracy: 0.9522 - val_loss: 0.7334 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.88000\n",
      "Epoch 80/200\n",
      "29/29 - 1s - loss: 0.1310 - accuracy: 0.9567 - val_loss: 1.2020 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.88000\n",
      "Epoch 81/200\n",
      "29/29 - 1s - loss: 0.1469 - accuracy: 0.9589 - val_loss: 0.9358 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.88000\n",
      "Epoch 82/200\n",
      "29/29 - 1s - loss: 0.1378 - accuracy: 0.9622 - val_loss: 0.7973 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.88000\n",
      "Epoch 83/200\n",
      "29/29 - 1s - loss: 0.1778 - accuracy: 0.9478 - val_loss: 0.7340 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.88000\n",
      "Epoch 84/200\n",
      "29/29 - 1s - loss: 0.1221 - accuracy: 0.9600 - val_loss: 0.9778 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.88000\n",
      "Epoch 85/200\n",
      "29/29 - 1s - loss: 0.0941 - accuracy: 0.9756 - val_loss: 0.9250 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.88000\n",
      "Epoch 86/200\n",
      "29/29 - 1s - loss: 0.0770 - accuracy: 0.9744 - val_loss: 1.1435 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.88000\n",
      "Epoch 87/200\n",
      "29/29 - 1s - loss: 0.0827 - accuracy: 0.9811 - val_loss: 1.1018 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.88000\n",
      "Epoch 88/200\n",
      "29/29 - 1s - loss: 0.0942 - accuracy: 0.9722 - val_loss: 0.8040 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.88000\n",
      "Epoch 89/200\n",
      "29/29 - 1s - loss: 0.2177 - accuracy: 0.9433 - val_loss: 0.7803 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.88000\n",
      "Epoch 90/200\n",
      "29/29 - 1s - loss: 0.2002 - accuracy: 0.9544 - val_loss: 0.7165 - val_accuracy: 0.7700\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.88000\n",
      "Epoch 91/200\n",
      "29/29 - 1s - loss: 0.1180 - accuracy: 0.9667 - val_loss: 0.8180 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.88000\n",
      "Epoch 92/200\n",
      "29/29 - 1s - loss: 0.1021 - accuracy: 0.9722 - val_loss: 0.8571 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.88000\n",
      "Epoch 93/200\n",
      "29/29 - 1s - loss: 0.1192 - accuracy: 0.9678 - val_loss: 0.7596 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.88000\n",
      "Epoch 94/200\n",
      "29/29 - 1s - loss: 0.1086 - accuracy: 0.9700 - val_loss: 0.7270 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.88000\n",
      "Epoch 95/200\n",
      "29/29 - 1s - loss: 0.0840 - accuracy: 0.9800 - val_loss: 0.6269 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.88000\n",
      "Epoch 96/200\n",
      "29/29 - 1s - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.7674 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.88000\n",
      "Epoch 97/200\n",
      "29/29 - 1s - loss: 0.0504 - accuracy: 0.9878 - val_loss: 0.9831 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.88000\n",
      "Epoch 98/200\n",
      "29/29 - 1s - loss: 0.0809 - accuracy: 0.9822 - val_loss: 0.8143 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.88000\n",
      "Epoch 99/200\n",
      "29/29 - 1s - loss: 0.0698 - accuracy: 0.9811 - val_loss: 0.7438 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.88000\n",
      "Epoch 100/200\n",
      "29/29 - 1s - loss: 0.0567 - accuracy: 0.9867 - val_loss: 0.8348 - val_accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.88000\n",
      "Epoch 101/200\n",
      "29/29 - 1s - loss: 0.0474 - accuracy: 0.9900 - val_loss: 0.9684 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.88000\n",
      "Epoch 102/200\n",
      "29/29 - 1s - loss: 0.0723 - accuracy: 0.9822 - val_loss: 0.8385 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.88000\n",
      "Epoch 103/200\n",
      "29/29 - 1s - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.8763 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.88000\n",
      "Epoch 104/200\n",
      "29/29 - 1s - loss: 0.0613 - accuracy: 0.9844 - val_loss: 0.9149 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.88000\n",
      "Epoch 105/200\n",
      "29/29 - 1s - loss: 0.0582 - accuracy: 0.9856 - val_loss: 1.0258 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.88000\n",
      "Epoch 106/200\n",
      "29/29 - 1s - loss: 0.0755 - accuracy: 0.9833 - val_loss: 1.0461 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.88000\n",
      "Epoch 107/200\n",
      "29/29 - 1s - loss: 0.1355 - accuracy: 0.9533 - val_loss: 0.8946 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00107: val_accuracy improved from 0.88000 to 0.89000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_label\n",
      "Epoch 108/200\n",
      "29/29 - 1s - loss: 0.0681 - accuracy: 0.9878 - val_loss: 1.0137 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.89000\n",
      "Epoch 109/200\n",
      "29/29 - 1s - loss: 0.2883 - accuracy: 0.9422 - val_loss: 0.9712 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.89000\n",
      "Epoch 110/200\n",
      "29/29 - 1s - loss: 0.2583 - accuracy: 0.9322 - val_loss: 0.8018 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.89000\n",
      "Epoch 111/200\n",
      "29/29 - 1s - loss: 0.2986 - accuracy: 0.9100 - val_loss: 0.7660 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.89000\n",
      "Epoch 112/200\n",
      "29/29 - 1s - loss: 0.2441 - accuracy: 0.9189 - val_loss: 1.0560 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.89000\n",
      "Epoch 113/200\n",
      "29/29 - 1s - loss: 0.2105 - accuracy: 0.9389 - val_loss: 0.7958 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.89000\n",
      "Epoch 114/200\n",
      "29/29 - 1s - loss: 0.1615 - accuracy: 0.9478 - val_loss: 0.9880 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.89000\n",
      "Epoch 115/200\n",
      "29/29 - 1s - loss: 0.1523 - accuracy: 0.9544 - val_loss: 0.8117 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.89000\n",
      "Epoch 116/200\n",
      "29/29 - 1s - loss: 0.1533 - accuracy: 0.9433 - val_loss: 0.6099 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.89000\n",
      "Epoch 117/200\n",
      "29/29 - 1s - loss: 0.1569 - accuracy: 0.9500 - val_loss: 0.9259 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.89000\n",
      "Epoch 118/200\n",
      "29/29 - 1s - loss: 0.1936 - accuracy: 0.9433 - val_loss: 0.6804 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.89000\n",
      "Epoch 119/200\n",
      "29/29 - 1s - loss: 0.1172 - accuracy: 0.9633 - val_loss: 0.7177 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.89000\n",
      "Epoch 120/200\n",
      "29/29 - 1s - loss: 0.1614 - accuracy: 0.9589 - val_loss: 0.7011 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.89000\n",
      "Epoch 121/200\n",
      "29/29 - 1s - loss: 0.1698 - accuracy: 0.9422 - val_loss: 0.8125 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.89000\n",
      "Epoch 122/200\n",
      "29/29 - 1s - loss: 0.1441 - accuracy: 0.9478 - val_loss: 0.8375 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.89000\n",
      "Epoch 123/200\n",
      "29/29 - 1s - loss: 0.1149 - accuracy: 0.9678 - val_loss: 0.9835 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.89000\n",
      "Epoch 124/200\n",
      "29/29 - 1s - loss: 0.2353 - accuracy: 0.9211 - val_loss: 0.7293 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.89000\n",
      "Epoch 125/200\n",
      "29/29 - 1s - loss: 0.2238 - accuracy: 0.9200 - val_loss: 0.8755 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.89000\n",
      "Epoch 126/200\n",
      "29/29 - 1s - loss: 0.1752 - accuracy: 0.9600 - val_loss: 0.8882 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.89000\n",
      "Epoch 127/200\n",
      "29/29 - 1s - loss: 0.2167 - accuracy: 0.9178 - val_loss: 0.6815 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.89000\n",
      "Epoch 128/200\n",
      "29/29 - 1s - loss: 0.1631 - accuracy: 0.9389 - val_loss: 0.9706 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.89000\n",
      "Epoch 129/200\n",
      "29/29 - 1s - loss: 0.1689 - accuracy: 0.9511 - val_loss: 1.0139 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.89000\n",
      "Epoch 130/200\n",
      "29/29 - 1s - loss: 0.1676 - accuracy: 0.9511 - val_loss: 0.8011 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.89000\n",
      "Epoch 131/200\n",
      "29/29 - 1s - loss: 0.1880 - accuracy: 0.9456 - val_loss: 0.7227 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.89000\n",
      "Epoch 132/200\n",
      "29/29 - 1s - loss: 0.1257 - accuracy: 0.9633 - val_loss: 0.7662 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.89000\n",
      "Epoch 133/200\n",
      "29/29 - 1s - loss: 0.1374 - accuracy: 0.9544 - val_loss: 0.7519 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.89000\n",
      "Epoch 134/200\n",
      "29/29 - 1s - loss: 0.2058 - accuracy: 0.9456 - val_loss: 0.6249 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.89000\n",
      "Epoch 135/200\n",
      "29/29 - 1s - loss: 0.1119 - accuracy: 0.9589 - val_loss: 0.7699 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.89000\n",
      "Epoch 136/200\n",
      "29/29 - 1s - loss: 0.1516 - accuracy: 0.9500 - val_loss: 0.9488 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.89000\n",
      "Epoch 137/200\n",
      "29/29 - 1s - loss: 0.1334 - accuracy: 0.9656 - val_loss: 0.8780 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.89000\n",
      "Epoch 138/200\n",
      "29/29 - 1s - loss: 0.0998 - accuracy: 0.9778 - val_loss: 0.8973 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.89000\n",
      "Epoch 139/200\n",
      "29/29 - 1s - loss: 0.0846 - accuracy: 0.9767 - val_loss: 0.9401 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.89000\n",
      "Epoch 140/200\n",
      "29/29 - 1s - loss: 0.0768 - accuracy: 0.9856 - val_loss: 0.8017 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.89000\n",
      "Epoch 141/200\n",
      "29/29 - 1s - loss: 0.0541 - accuracy: 0.9911 - val_loss: 0.8740 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.89000\n",
      "Epoch 142/200\n",
      "29/29 - 1s - loss: 0.0517 - accuracy: 0.9900 - val_loss: 0.9892 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.89000\n",
      "Epoch 143/200\n",
      "29/29 - 1s - loss: 0.0733 - accuracy: 0.9844 - val_loss: 0.8588 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.89000\n",
      "Epoch 144/200\n",
      "29/29 - 1s - loss: 0.0969 - accuracy: 0.9822 - val_loss: 0.9533 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.89000\n",
      "Epoch 145/200\n",
      "29/29 - 1s - loss: 0.1043 - accuracy: 0.9733 - val_loss: 0.9443 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.89000\n",
      "Epoch 146/200\n",
      "29/29 - 1s - loss: 0.1594 - accuracy: 0.9633 - val_loss: 1.0610 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.89000\n",
      "Epoch 147/200\n",
      "29/29 - 1s - loss: 0.1627 - accuracy: 0.9622 - val_loss: 0.9047 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.89000\n",
      "Epoch 148/200\n",
      "29/29 - 1s - loss: 0.2125 - accuracy: 0.9467 - val_loss: 0.8524 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.89000\n",
      "Epoch 149/200\n",
      "29/29 - 1s - loss: 0.1864 - accuracy: 0.9444 - val_loss: 0.9941 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.89000\n",
      "Epoch 150/200\n",
      "29/29 - 1s - loss: 0.1983 - accuracy: 0.9489 - val_loss: 0.7493 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.89000\n",
      "Epoch 151/200\n",
      "29/29 - 1s - loss: 0.1278 - accuracy: 0.9667 - val_loss: 1.0256 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.89000\n",
      "Epoch 152/200\n",
      "29/29 - 1s - loss: 0.0934 - accuracy: 0.9733 - val_loss: 0.9225 - val_accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.89000\n",
      "Epoch 153/200\n",
      "29/29 - 1s - loss: 0.1110 - accuracy: 0.9778 - val_loss: 0.8549 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.89000\n",
      "Epoch 154/200\n",
      "29/29 - 1s - loss: 0.0854 - accuracy: 0.9767 - val_loss: 0.8656 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.89000\n",
      "Epoch 155/200\n",
      "29/29 - 1s - loss: 0.0685 - accuracy: 0.9822 - val_loss: 0.9773 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.89000\n",
      "Epoch 156/200\n",
      "29/29 - 1s - loss: 0.0880 - accuracy: 0.9767 - val_loss: 0.7757 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.89000\n",
      "Epoch 157/200\n",
      "29/29 - 1s - loss: 0.1717 - accuracy: 0.9411 - val_loss: 0.9358 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.89000\n",
      "Epoch 158/200\n",
      "29/29 - 1s - loss: 0.1786 - accuracy: 0.9456 - val_loss: 1.0136 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.89000\n",
      "Epoch 159/200\n",
      "29/29 - 1s - loss: 0.1335 - accuracy: 0.9567 - val_loss: 0.7704 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.89000\n",
      "Epoch 160/200\n",
      "29/29 - 1s - loss: 0.1680 - accuracy: 0.9633 - val_loss: 0.8263 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.89000\n",
      "Epoch 161/200\n",
      "29/29 - 1s - loss: 0.1618 - accuracy: 0.9489 - val_loss: 1.0154 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.89000\n",
      "Epoch 162/200\n",
      "29/29 - 1s - loss: 0.1737 - accuracy: 0.9578 - val_loss: 0.9799 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.89000\n",
      "Epoch 163/200\n",
      "29/29 - 1s - loss: 0.3396 - accuracy: 0.9211 - val_loss: 0.7090 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.89000\n",
      "Epoch 164/200\n",
      "29/29 - 1s - loss: 0.1575 - accuracy: 0.9556 - val_loss: 0.6636 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.89000\n",
      "Epoch 165/200\n",
      "29/29 - 1s - loss: 0.1149 - accuracy: 0.9678 - val_loss: 0.6910 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.89000\n",
      "Epoch 166/200\n",
      "29/29 - 1s - loss: 0.1032 - accuracy: 0.9689 - val_loss: 0.7748 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.89000\n",
      "Epoch 167/200\n",
      "29/29 - 1s - loss: 0.0719 - accuracy: 0.9833 - val_loss: 0.8769 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.89000\n",
      "Epoch 168/200\n",
      "29/29 - 1s - loss: 0.0828 - accuracy: 0.9778 - val_loss: 0.6782 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.89000\n",
      "Epoch 169/200\n",
      "29/29 - 1s - loss: 0.0752 - accuracy: 0.9844 - val_loss: 0.8021 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.89000\n",
      "Epoch 170/200\n",
      "29/29 - 1s - loss: 0.0784 - accuracy: 0.9822 - val_loss: 0.6759 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.89000\n",
      "Epoch 171/200\n",
      "29/29 - 1s - loss: 0.0772 - accuracy: 0.9833 - val_loss: 0.6656 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.89000\n",
      "Epoch 172/200\n",
      "29/29 - 1s - loss: 0.1485 - accuracy: 0.9667 - val_loss: 0.5726 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.89000\n",
      "Epoch 173/200\n",
      "29/29 - 1s - loss: 0.0882 - accuracy: 0.9756 - val_loss: 0.7777 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.89000\n",
      "Epoch 174/200\n",
      "29/29 - 1s - loss: 0.1495 - accuracy: 0.9711 - val_loss: 1.1024 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.89000\n",
      "Epoch 175/200\n",
      "29/29 - 1s - loss: 0.1893 - accuracy: 0.9533 - val_loss: 0.7460 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.89000\n",
      "Epoch 176/200\n",
      "29/29 - 1s - loss: 0.1318 - accuracy: 0.9667 - val_loss: 0.7541 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.89000\n",
      "Epoch 177/200\n",
      "29/29 - 1s - loss: 0.1794 - accuracy: 0.9611 - val_loss: 0.9945 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.89000\n",
      "Epoch 178/200\n",
      "29/29 - 1s - loss: 0.1662 - accuracy: 0.9567 - val_loss: 0.6542 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.89000\n",
      "Epoch 179/200\n",
      "29/29 - 1s - loss: 0.0972 - accuracy: 0.9733 - val_loss: 0.7150 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.89000\n",
      "Epoch 180/200\n",
      "29/29 - 1s - loss: 0.0709 - accuracy: 0.9878 - val_loss: 0.8812 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.89000\n",
      "Epoch 181/200\n",
      "29/29 - 1s - loss: 0.1269 - accuracy: 0.9800 - val_loss: 0.6311 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.89000\n",
      "Epoch 182/200\n",
      "29/29 - 1s - loss: 0.0781 - accuracy: 0.9844 - val_loss: 1.1795 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.89000\n",
      "Epoch 183/200\n",
      "29/29 - 1s - loss: 0.2071 - accuracy: 0.9511 - val_loss: 0.6254 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.89000\n",
      "Epoch 184/200\n",
      "29/29 - 1s - loss: 0.1579 - accuracy: 0.9622 - val_loss: 0.7152 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.89000\n",
      "Epoch 185/200\n",
      "29/29 - 1s - loss: 0.0734 - accuracy: 0.9889 - val_loss: 0.8961 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.89000\n",
      "Epoch 186/200\n",
      "29/29 - 1s - loss: 0.0804 - accuracy: 0.9833 - val_loss: 1.0808 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.89000\n",
      "Epoch 187/200\n",
      "29/29 - 1s - loss: 0.0946 - accuracy: 0.9778 - val_loss: 0.7451 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.89000\n",
      "Epoch 188/200\n",
      "29/29 - 1s - loss: 0.0784 - accuracy: 0.9867 - val_loss: 0.9697 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.89000\n",
      "Epoch 189/200\n",
      "29/29 - 1s - loss: 0.0933 - accuracy: 0.9778 - val_loss: 0.6922 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.89000\n",
      "Epoch 190/200\n",
      "29/29 - 1s - loss: 0.0563 - accuracy: 0.9922 - val_loss: 0.8245 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.89000\n",
      "Epoch 191/200\n",
      "29/29 - 1s - loss: 0.0487 - accuracy: 0.9922 - val_loss: 0.9450 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.89000\n",
      "Epoch 192/200\n",
      "29/29 - 1s - loss: 0.0507 - accuracy: 0.9922 - val_loss: 1.0433 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.89000\n",
      "Epoch 193/200\n",
      "29/29 - 1s - loss: 0.1013 - accuracy: 0.9778 - val_loss: 1.0249 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.89000\n",
      "Epoch 194/200\n",
      "29/29 - 1s - loss: 0.1218 - accuracy: 0.9733 - val_loss: 0.7346 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.89000\n",
      "Epoch 195/200\n",
      "29/29 - 1s - loss: 0.0706 - accuracy: 0.9867 - val_loss: 0.9677 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.89000\n",
      "Epoch 196/200\n",
      "29/29 - 1s - loss: 0.0640 - accuracy: 0.9922 - val_loss: 0.9611 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.89000\n",
      "Epoch 197/200\n",
      "29/29 - 1s - loss: 0.0621 - accuracy: 0.9900 - val_loss: 0.8825 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.89000\n",
      "Epoch 198/200\n",
      "29/29 - 1s - loss: 0.0473 - accuracy: 0.9956 - val_loss: 0.9252 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.89000\n",
      "Epoch 199/200\n",
      "29/29 - 1s - loss: 0.0402 - accuracy: 0.9978 - val_loss: 1.0132 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.89000\n",
      "Epoch 200/200\n",
      "29/29 - 1s - loss: 0.0367 - accuracy: 0.9989 - val_loss: 1.0532 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.89000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1859d65c0a0>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "csv_filename = os.path.join(checkpoint_path,\n",
    "                            \"training_log.csv\"\n",
    "                            )\n",
    "csvlogger_callback = tf.keras.callbacks.CSVLogger(filename=csv_filename, append=True)\n",
    "\n",
    "# LSwFW_model.fit(train_tensor, \n",
    "#                 train_targets, \n",
    "#                 epochs=1,\n",
    "#                 batch_size=n_batch,\n",
    "#                 shuffle=True,\n",
    "#                 verbose=2, \n",
    "#                )\n",
    "# #Set feat weights\n",
    "# # Fweights=cosa_mdl.Fweight\n",
    "# sampled_Fweights=train_Fweights[np.random.choice(range(len(train_Fweights)), n_attention)]\n",
    "# for i in range(n_attention):\n",
    "#     weights=LSwFW_model.layers[1].attention_layers[i].get_weights()\n",
    "#     weights[0]=np.reshape(sampled_Fweights[i], (1,n_feat))\n",
    "#     LSwFW_model.layers[1].attention_layers[i].set_weights(weights)\n",
    "\n",
    "\n",
    "n_epoch=200\n",
    "\n",
    "\n",
    "LSwFW_model.fit(train_tensor, \n",
    "                train_targets, \n",
    "                epochs=n_epoch,\n",
    "                batch_size=n_batch,\n",
    "                validation_data=(test_tensor, test_targets),\n",
    "                shuffle=True,\n",
    "                verbose=2, \n",
    "                callbacks=[csvlogger_callback,\n",
    "                           cp_callback\n",
    "                          ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25b0a911ac0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSwFW_model.load_weights(os.path.join(\"210210_TrainingLocalitySensitivewFW\",\n",
    "                                      \"LocalitySensitivewFW_label\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions=[LSwFW_model.layers[1].attention_layers[i](train_tensor).numpy() for i in range(10)]\n",
    "# attentions=np.reshape(attentions, (900,10))\n",
    "\n",
    "attentions=LSwFW_model.layers[1](train_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "dr=TSNE()\n",
    "embed_attentions=dr.fit_transform(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x299d8f287c0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "scatter=ax.scatter(embed_attentions[:,0], embed_attentions[:,1],\n",
    "                   c=cluster_labels_train,\n",
    "                   s=3\n",
    "                  )\n",
    "\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x299d8fbe4f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "mapper=UMAP()\n",
    "embed_attentions=dr.fit_transform(attentions)\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "scatter=ax.scatter(embed_attentions[:,0], embed_attentions[:,1],\n",
    "                   c=cluster_labels_train,\n",
    "                   s=3\n",
    "                  )\n",
    "\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLDL without Feature weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = train_data.shape[1]\n",
    "n_attention = 10 #Reduced from 20 to 10. 10 works better\n",
    "n_attention_hidden=40\n",
    "n_attention_out=1\n",
    "n_concat_hidden=128\n",
    "n_hidden1 =64\n",
    "n_hidden2 = 64\n",
    "momentum=0.8\n",
    "learning_rate=0.01\n",
    "\n",
    "n_batch=32\n",
    "\n",
    "label=\"SynthData\"\n",
    "\n",
    "save_folder=os.path.join(time.strftime(\"%y%m%d_TrainingLocalitySensitivewoFW\",\n",
    "                                       time.localtime()))\n",
    "checkpoint_path = os.path.join(save_folder, \n",
    "                               \"LocalitySensitivewoFW_{}\".format(\"label\"),\n",
    "                               )\n",
    "\n",
    "try: \n",
    "    os.mkdir(save_folder) \n",
    "except OSError as error: \n",
    "    print(error) \n",
    "    \n",
    "try:\n",
    "    os.mkdir(checkpoint_path)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "concat_activation=\"selu\"\n",
    "attention_hidden_activation=\"selu\"\n",
    "attention_output_activation=\"sigmoid\"\n",
    "kernel_initializer=VarianceScaling()\n",
    "hidden_activation=\"selu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import attention_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_layer=Input(shape=(n_feat, ))\n",
    "\n",
    "attentions_layer=attention_model.ConcatAttentions(\n",
    "    n_attention=n_attention,\n",
    "    n_attention_hidden=n_attention_hidden,\n",
    "    n_attention_out=n_attention_out,\n",
    "    n_feat=n_feat,\n",
    "    n_hidden=n_concat_hidden,\n",
    "    activation=concat_activation, \n",
    "    kernel_initializer=kernel_initializer,\n",
    "    kernel_regularizer=l2(1E-5),\n",
    "    bias_regularizer=l2(1E-5),\n",
    "    attention_initializer=kernel_initializer,\n",
    "    attention_hidden_activation=attention_hidden_activation,\n",
    "    attention_output_activation=attention_output_activation,\n",
    "    batch_norm_kwargs={\"trainable\":False, \"renorm\":False},\n",
    ")(input_layer)\n",
    "##Removed dropout for attentions_layer because of Batch normalization\n",
    "# dropout0=Dropout(0.1)(attentions_layer)\n",
    "dense_layer1=Dense(n_hidden1, \n",
    "                   activation=hidden_activation, \n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5),\n",
    "                  )(attentions_layer)\n",
    "# dropout1=Dropout(0.1)(dense_layer1)\n",
    "dense_layer2=Dense(n_hidden2,\n",
    "                   activation=hidden_activation,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5)\n",
    "                  )(dense_layer1)\n",
    "# dropout2=Dropout(0.1)(dense_layer2)\n",
    "output_layer=Dense(1, activation=\"sigmoid\")(dense_layer2)\n",
    "\n",
    "LSwoFW_model=Model(inputs=input_layer, \n",
    "                  outputs=output_layer\n",
    "                 )\n",
    "\n",
    "weights_dicts=get_weights_dicts(np.expand_dims(train_targets,1))\n",
    "loss_fn=BinaryCrossEntropyIgnoreNaN(weights_dicts=weights_dicts)\n",
    "\n",
    "# loss_fn=tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "LSwoFW_model.compile(loss=loss_fn,\n",
    "    #loss=BinaryCrossentropy(from_logits=False, \n",
    "#                                             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "#                                            ), \n",
    "              optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy',]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "concat_attentions (ConcatAtt (None, 128)               6730      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 19,211\n",
      "Trainable params: 18,699\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSwoFW_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 - 3s - loss: 1.0274 - accuracy: 0.4989 - val_loss: 0.7782 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.48000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 2/200\n",
      "29/29 - 0s - loss: 0.7565 - accuracy: 0.5044 - val_loss: 0.7514 - val_accuracy: 0.3800\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.48000\n",
      "Epoch 3/200\n",
      "29/29 - 0s - loss: 0.7054 - accuracy: 0.5200 - val_loss: 0.7033 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.48000\n",
      "Epoch 4/200\n",
      "29/29 - 0s - loss: 0.7113 - accuracy: 0.5267 - val_loss: 0.7405 - val_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.48000\n",
      "Epoch 5/200\n",
      "29/29 - 0s - loss: 0.7216 - accuracy: 0.5300 - val_loss: 0.7754 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.48000 to 0.51000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 6/200\n",
      "29/29 - 0s - loss: 0.6943 - accuracy: 0.5689 - val_loss: 0.7312 - val_accuracy: 0.4200\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.51000\n",
      "Epoch 7/200\n",
      "29/29 - 0s - loss: 0.6649 - accuracy: 0.5900 - val_loss: 0.7026 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.51000 to 0.58000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 8/200\n",
      "29/29 - 0s - loss: 0.7038 - accuracy: 0.5644 - val_loss: 0.8647 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.58000\n",
      "Epoch 9/200\n",
      "29/29 - 0s - loss: 0.6885 - accuracy: 0.6056 - val_loss: 0.7333 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.58000\n",
      "Epoch 10/200\n",
      "29/29 - 0s - loss: 0.6582 - accuracy: 0.6211 - val_loss: 0.7804 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.58000\n",
      "Epoch 11/200\n",
      "29/29 - 0s - loss: 0.6456 - accuracy: 0.6344 - val_loss: 0.7217 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.58000 to 0.60000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 12/200\n",
      "29/29 - 0s - loss: 0.6366 - accuracy: 0.6522 - val_loss: 0.8274 - val_accuracy: 0.4600\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.60000\n",
      "Epoch 13/200\n",
      "29/29 - 0s - loss: 0.6343 - accuracy: 0.6511 - val_loss: 0.7495 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.60000\n",
      "Epoch 14/200\n",
      "29/29 - 0s - loss: 0.6299 - accuracy: 0.6489 - val_loss: 0.7581 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.60000\n",
      "Epoch 15/200\n",
      "29/29 - 0s - loss: 0.6063 - accuracy: 0.6678 - val_loss: 0.7380 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.60000\n",
      "Epoch 16/200\n",
      "29/29 - 0s - loss: 0.5876 - accuracy: 0.6878 - val_loss: 0.7989 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.60000\n",
      "Epoch 17/200\n",
      "29/29 - 0s - loss: 0.6481 - accuracy: 0.6400 - val_loss: 0.8535 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.60000\n",
      "Epoch 18/200\n",
      "29/29 - 0s - loss: 0.6354 - accuracy: 0.6567 - val_loss: 0.8351 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.60000\n",
      "Epoch 19/200\n",
      "29/29 - 0s - loss: 0.5731 - accuracy: 0.6956 - val_loss: 0.8937 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.60000\n",
      "Epoch 20/200\n",
      "29/29 - 0s - loss: 0.5782 - accuracy: 0.6833 - val_loss: 0.8112 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.60000\n",
      "Epoch 21/200\n",
      "29/29 - 0s - loss: 0.5544 - accuracy: 0.7311 - val_loss: 0.7035 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.60000 to 0.62000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 22/200\n",
      "29/29 - 0s - loss: 0.5116 - accuracy: 0.7344 - val_loss: 0.7989 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.62000\n",
      "Epoch 23/200\n",
      "29/29 - 0s - loss: 0.5559 - accuracy: 0.7167 - val_loss: 0.7951 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.62000\n",
      "Epoch 24/200\n",
      "29/29 - 0s - loss: 0.5447 - accuracy: 0.7333 - val_loss: 0.9281 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.62000\n",
      "Epoch 25/200\n",
      "29/29 - 0s - loss: 0.5187 - accuracy: 0.7467 - val_loss: 0.8192 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.62000\n",
      "Epoch 26/200\n",
      "29/29 - 0s - loss: 0.4784 - accuracy: 0.7678 - val_loss: 0.8287 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.62000\n",
      "Epoch 27/200\n",
      "29/29 - 0s - loss: 0.4671 - accuracy: 0.7833 - val_loss: 0.9343 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.62000\n",
      "Epoch 28/200\n",
      "29/29 - 0s - loss: 0.4970 - accuracy: 0.7467 - val_loss: 0.7762 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.62000\n",
      "Epoch 29/200\n",
      "29/29 - 0s - loss: 0.4728 - accuracy: 0.7722 - val_loss: 0.9729 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.62000\n",
      "Epoch 30/200\n",
      "29/29 - 0s - loss: 0.5477 - accuracy: 0.7467 - val_loss: 0.7439 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.62000\n",
      "Epoch 31/200\n",
      "29/29 - 0s - loss: 0.5872 - accuracy: 0.7067 - val_loss: 0.7703 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.62000\n",
      "Epoch 32/200\n",
      "29/29 - 0s - loss: 0.5196 - accuracy: 0.7578 - val_loss: 0.7764 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.62000\n",
      "Epoch 33/200\n",
      "29/29 - 0s - loss: 0.4924 - accuracy: 0.7567 - val_loss: 0.8192 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.62000\n",
      "Epoch 34/200\n",
      "29/29 - 0s - loss: 0.4814 - accuracy: 0.7689 - val_loss: 0.7343 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.62000 to 0.63000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 35/200\n",
      "29/29 - 0s - loss: 0.4593 - accuracy: 0.7933 - val_loss: 1.0344 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.63000\n",
      "Epoch 36/200\n",
      "29/29 - 0s - loss: 0.4533 - accuracy: 0.7856 - val_loss: 0.9178 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.63000\n",
      "Epoch 37/200\n",
      "29/29 - 0s - loss: 0.4362 - accuracy: 0.7889 - val_loss: 0.7801 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.63000\n",
      "Epoch 38/200\n",
      "29/29 - 0s - loss: 0.4307 - accuracy: 0.8067 - val_loss: 0.7785 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.63000\n",
      "Epoch 39/200\n",
      "29/29 - 0s - loss: 0.4310 - accuracy: 0.8033 - val_loss: 0.9555 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.63000 to 0.66000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 40/200\n",
      "29/29 - 0s - loss: 0.4115 - accuracy: 0.8222 - val_loss: 0.8698 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.66000\n",
      "Epoch 41/200\n",
      "29/29 - 0s - loss: 0.4094 - accuracy: 0.8100 - val_loss: 0.9242 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.66000\n",
      "Epoch 42/200\n",
      "29/29 - 0s - loss: 0.3610 - accuracy: 0.8478 - val_loss: 1.0331 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.66000\n",
      "Epoch 43/200\n",
      "29/29 - 0s - loss: 0.3654 - accuracy: 0.8444 - val_loss: 1.0519 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00043: val_accuracy improved from 0.66000 to 0.70000, saving model to 210210_TrainingLocalitySensitivewoFW\\LocalitySensitivewoFW_label\n",
      "Epoch 44/200\n",
      "29/29 - 0s - loss: 0.3186 - accuracy: 0.8500 - val_loss: 1.0483 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.70000\n",
      "Epoch 45/200\n",
      "29/29 - 0s - loss: 0.3587 - accuracy: 0.8344 - val_loss: 0.9837 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.70000\n",
      "Epoch 46/200\n",
      "29/29 - 0s - loss: 0.3510 - accuracy: 0.8489 - val_loss: 1.0265 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.70000\n",
      "Epoch 47/200\n",
      "29/29 - 0s - loss: 0.3754 - accuracy: 0.8422 - val_loss: 1.0684 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.70000\n",
      "Epoch 48/200\n",
      "29/29 - 0s - loss: 0.3512 - accuracy: 0.8467 - val_loss: 0.8918 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.70000\n",
      "Epoch 49/200\n",
      "29/29 - 0s - loss: 0.3487 - accuracy: 0.8533 - val_loss: 1.0548 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.70000\n",
      "Epoch 50/200\n",
      "29/29 - 0s - loss: 0.3327 - accuracy: 0.8644 - val_loss: 0.9969 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.70000\n",
      "Epoch 51/200\n",
      "29/29 - 0s - loss: 0.3131 - accuracy: 0.8600 - val_loss: 1.3694 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.70000\n",
      "Epoch 52/200\n",
      "29/29 - 0s - loss: 0.3134 - accuracy: 0.8644 - val_loss: 1.1746 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.70000\n",
      "Epoch 53/200\n",
      "29/29 - 0s - loss: 0.3721 - accuracy: 0.8267 - val_loss: 1.0270 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.70000\n",
      "Epoch 54/200\n",
      "29/29 - 0s - loss: 0.3124 - accuracy: 0.8689 - val_loss: 1.1853 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.70000\n",
      "Epoch 55/200\n",
      "29/29 - 0s - loss: 0.3081 - accuracy: 0.8633 - val_loss: 1.2689 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.70000\n",
      "Epoch 56/200\n",
      "29/29 - 0s - loss: 0.3418 - accuracy: 0.8567 - val_loss: 1.1144 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.70000\n",
      "Epoch 57/200\n",
      "29/29 - 0s - loss: 0.3066 - accuracy: 0.8667 - val_loss: 1.2384 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.70000\n",
      "Epoch 58/200\n",
      "29/29 - 0s - loss: 0.2654 - accuracy: 0.8867 - val_loss: 1.6731 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.70000\n",
      "Epoch 59/200\n",
      "29/29 - 0s - loss: 0.3372 - accuracy: 0.8667 - val_loss: 1.2585 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.70000\n",
      "Epoch 60/200\n",
      "29/29 - 0s - loss: 0.3331 - accuracy: 0.8700 - val_loss: 1.3032 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.70000\n",
      "Epoch 61/200\n",
      "29/29 - 0s - loss: 0.3415 - accuracy: 0.8600 - val_loss: 1.2966 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.70000\n",
      "Epoch 62/200\n",
      "29/29 - 0s - loss: 0.3047 - accuracy: 0.8678 - val_loss: 1.0912 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.70000\n",
      "Epoch 63/200\n",
      "29/29 - 0s - loss: 0.3134 - accuracy: 0.8733 - val_loss: 1.1978 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.70000\n",
      "Epoch 64/200\n",
      "29/29 - 0s - loss: 0.2666 - accuracy: 0.8944 - val_loss: 1.4561 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.70000\n",
      "Epoch 65/200\n",
      "29/29 - 0s - loss: 0.2397 - accuracy: 0.9067 - val_loss: 1.2675 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.70000\n",
      "Epoch 66/200\n",
      "29/29 - 0s - loss: 0.2938 - accuracy: 0.8878 - val_loss: 1.1948 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.70000\n",
      "Epoch 67/200\n",
      "29/29 - 0s - loss: 0.3158 - accuracy: 0.8722 - val_loss: 1.0343 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.70000\n",
      "Epoch 68/200\n",
      "29/29 - 0s - loss: 0.2775 - accuracy: 0.8933 - val_loss: 1.2981 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.70000\n",
      "Epoch 69/200\n",
      "29/29 - 0s - loss: 0.2326 - accuracy: 0.9089 - val_loss: 1.6587 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.70000\n",
      "Epoch 70/200\n",
      "29/29 - 0s - loss: 0.2199 - accuracy: 0.9111 - val_loss: 1.5098 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.70000\n",
      "Epoch 71/200\n",
      "29/29 - 0s - loss: 0.2147 - accuracy: 0.9122 - val_loss: 1.8605 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.70000\n",
      "Epoch 72/200\n",
      "29/29 - 0s - loss: 0.2444 - accuracy: 0.9022 - val_loss: 1.4563 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.70000\n",
      "Epoch 73/200\n",
      "29/29 - 0s - loss: 0.2636 - accuracy: 0.8933 - val_loss: 1.2579 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.70000\n",
      "Epoch 74/200\n",
      "29/29 - 0s - loss: 0.2347 - accuracy: 0.9100 - val_loss: 1.5082 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.70000\n",
      "Epoch 75/200\n",
      "29/29 - 0s - loss: 0.2239 - accuracy: 0.9222 - val_loss: 1.3764 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.70000\n",
      "Epoch 76/200\n",
      "29/29 - 0s - loss: 0.2134 - accuracy: 0.9044 - val_loss: 1.8845 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.70000\n",
      "Epoch 77/200\n",
      "29/29 - 0s - loss: 0.2136 - accuracy: 0.9211 - val_loss: 2.0877 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.70000\n",
      "Epoch 78/200\n",
      "29/29 - 0s - loss: 0.4610 - accuracy: 0.8078 - val_loss: 1.2332 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.70000\n",
      "Epoch 79/200\n",
      "29/29 - 0s - loss: 0.4586 - accuracy: 0.8033 - val_loss: 0.9020 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.70000\n",
      "Epoch 80/200\n",
      "29/29 - 0s - loss: 0.4004 - accuracy: 0.8322 - val_loss: 1.4171 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.70000\n",
      "Epoch 81/200\n",
      "29/29 - 0s - loss: 0.3490 - accuracy: 0.8567 - val_loss: 1.2119 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.70000\n",
      "Epoch 82/200\n",
      "29/29 - 0s - loss: 0.3033 - accuracy: 0.8800 - val_loss: 1.2466 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.70000\n",
      "Epoch 83/200\n",
      "29/29 - 0s - loss: 0.2514 - accuracy: 0.9011 - val_loss: 1.4642 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.70000\n",
      "Epoch 84/200\n",
      "29/29 - 0s - loss: 0.2695 - accuracy: 0.8778 - val_loss: 1.6879 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.70000\n",
      "Epoch 85/200\n",
      "29/29 - 0s - loss: 0.2370 - accuracy: 0.9178 - val_loss: 1.4931 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.70000\n",
      "Epoch 86/200\n",
      "29/29 - 0s - loss: 0.1907 - accuracy: 0.9278 - val_loss: 1.7145 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.70000\n",
      "Epoch 87/200\n",
      "29/29 - 0s - loss: 0.2235 - accuracy: 0.9122 - val_loss: 1.6012 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.70000\n",
      "Epoch 88/200\n",
      "29/29 - 0s - loss: 0.2628 - accuracy: 0.8900 - val_loss: 1.2624 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.70000\n",
      "Epoch 89/200\n",
      "29/29 - 0s - loss: 0.4159 - accuracy: 0.8322 - val_loss: 0.8108 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.70000\n",
      "Epoch 90/200\n",
      "29/29 - 0s - loss: 0.4137 - accuracy: 0.8167 - val_loss: 1.0359 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.70000\n",
      "Epoch 91/200\n",
      "29/29 - 0s - loss: 0.3617 - accuracy: 0.8544 - val_loss: 1.1220 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.70000\n",
      "Epoch 92/200\n",
      "29/29 - 0s - loss: 0.2998 - accuracy: 0.8767 - val_loss: 1.3019 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.70000\n",
      "Epoch 93/200\n",
      "29/29 - 0s - loss: 0.2500 - accuracy: 0.9033 - val_loss: 1.4709 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.70000\n",
      "Epoch 94/200\n",
      "29/29 - 0s - loss: 0.2184 - accuracy: 0.9144 - val_loss: 1.5603 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.70000\n",
      "Epoch 95/200\n",
      "29/29 - 0s - loss: 0.2095 - accuracy: 0.9278 - val_loss: 1.7353 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.70000\n",
      "Epoch 96/200\n",
      "29/29 - 0s - loss: 0.1876 - accuracy: 0.9267 - val_loss: 1.6577 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.70000\n",
      "Epoch 97/200\n",
      "29/29 - 0s - loss: 0.1864 - accuracy: 0.9311 - val_loss: 1.8800 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.70000\n",
      "Epoch 98/200\n",
      "29/29 - 0s - loss: 0.2014 - accuracy: 0.9322 - val_loss: 1.7913 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.70000\n",
      "Epoch 99/200\n",
      "29/29 - 0s - loss: 0.1688 - accuracy: 0.9389 - val_loss: 1.9110 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.70000\n",
      "Epoch 100/200\n",
      "29/29 - 0s - loss: 0.1472 - accuracy: 0.9511 - val_loss: 2.1200 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.70000\n",
      "Epoch 101/200\n",
      "29/29 - 0s - loss: 0.2521 - accuracy: 0.9167 - val_loss: 1.9706 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.70000\n",
      "Epoch 102/200\n",
      "29/29 - 0s - loss: 0.3021 - accuracy: 0.8856 - val_loss: 1.4288 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.70000\n",
      "Epoch 103/200\n",
      "29/29 - 0s - loss: 0.2977 - accuracy: 0.8878 - val_loss: 1.6877 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.70000\n",
      "Epoch 104/200\n",
      "29/29 - 0s - loss: 0.1951 - accuracy: 0.9311 - val_loss: 1.8415 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.70000\n",
      "Epoch 105/200\n",
      "29/29 - 0s - loss: 0.1674 - accuracy: 0.9433 - val_loss: 1.7096 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.70000\n",
      "Epoch 106/200\n",
      "29/29 - 0s - loss: 0.1642 - accuracy: 0.9389 - val_loss: 2.0748 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.70000\n",
      "Epoch 107/200\n",
      "29/29 - 0s - loss: 0.1738 - accuracy: 0.9433 - val_loss: 2.0433 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.70000\n",
      "Epoch 108/200\n",
      "29/29 - 0s - loss: 0.1622 - accuracy: 0.9400 - val_loss: 2.3420 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.70000\n",
      "Epoch 109/200\n",
      "29/29 - 0s - loss: 0.1729 - accuracy: 0.9333 - val_loss: 1.7657 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.70000\n",
      "Epoch 110/200\n",
      "29/29 - 0s - loss: 0.2476 - accuracy: 0.9078 - val_loss: 1.8718 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.70000\n",
      "Epoch 111/200\n",
      "29/29 - 0s - loss: 0.3658 - accuracy: 0.8656 - val_loss: 1.2573 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.70000\n",
      "Epoch 112/200\n",
      "29/29 - 0s - loss: 0.2701 - accuracy: 0.8967 - val_loss: 1.6773 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.70000\n",
      "Epoch 113/200\n",
      "29/29 - 0s - loss: 0.2043 - accuracy: 0.9178 - val_loss: 1.5042 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.70000\n",
      "Epoch 114/200\n",
      "29/29 - 0s - loss: 0.1849 - accuracy: 0.9333 - val_loss: 1.6014 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.70000\n",
      "Epoch 115/200\n",
      "29/29 - 0s - loss: 0.2283 - accuracy: 0.9200 - val_loss: 1.6516 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.70000\n",
      "Epoch 116/200\n",
      "29/29 - 0s - loss: 0.2323 - accuracy: 0.9100 - val_loss: 1.6585 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.70000\n",
      "Epoch 117/200\n",
      "29/29 - 0s - loss: 0.1854 - accuracy: 0.9322 - val_loss: 1.7736 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.70000\n",
      "Epoch 118/200\n",
      "29/29 - 0s - loss: 0.1478 - accuracy: 0.9444 - val_loss: 2.2774 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.70000\n",
      "Epoch 119/200\n",
      "29/29 - 0s - loss: 0.1855 - accuracy: 0.9311 - val_loss: 2.0794 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.70000\n",
      "Epoch 120/200\n",
      "29/29 - 0s - loss: 0.1424 - accuracy: 0.9411 - val_loss: 2.1895 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.70000\n",
      "Epoch 121/200\n",
      "29/29 - 0s - loss: 0.1645 - accuracy: 0.9456 - val_loss: 2.0543 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.70000\n",
      "Epoch 122/200\n",
      "29/29 - 0s - loss: 0.1534 - accuracy: 0.9400 - val_loss: 2.3093 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.70000\n",
      "Epoch 123/200\n",
      "29/29 - 0s - loss: 0.2085 - accuracy: 0.9222 - val_loss: 1.6868 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.70000\n",
      "Epoch 124/200\n",
      "29/29 - 0s - loss: 0.2666 - accuracy: 0.8967 - val_loss: 1.5966 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.70000\n",
      "Epoch 125/200\n",
      "29/29 - 0s - loss: 0.2412 - accuracy: 0.9100 - val_loss: 1.5234 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.70000\n",
      "Epoch 126/200\n",
      "29/29 - 0s - loss: 0.2148 - accuracy: 0.9156 - val_loss: 1.6849 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.70000\n",
      "Epoch 127/200\n",
      "29/29 - 0s - loss: 0.1656 - accuracy: 0.9367 - val_loss: 1.7700 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.70000\n",
      "Epoch 128/200\n",
      "29/29 - 0s - loss: 0.2525 - accuracy: 0.9133 - val_loss: 1.8923 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.70000\n",
      "Epoch 129/200\n",
      "29/29 - 0s - loss: 0.2170 - accuracy: 0.9233 - val_loss: 1.6721 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.70000\n",
      "Epoch 130/200\n",
      "29/29 - 0s - loss: 0.1943 - accuracy: 0.9244 - val_loss: 1.8885 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.70000\n",
      "Epoch 131/200\n",
      "29/29 - 0s - loss: 0.1922 - accuracy: 0.9356 - val_loss: 2.0951 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.70000\n",
      "Epoch 132/200\n",
      "29/29 - 0s - loss: 0.1749 - accuracy: 0.9311 - val_loss: 1.8032 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.70000\n",
      "Epoch 133/200\n",
      "29/29 - 0s - loss: 0.1539 - accuracy: 0.9411 - val_loss: 2.0057 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.70000\n",
      "Epoch 134/200\n",
      "29/29 - 0s - loss: 0.1522 - accuracy: 0.9533 - val_loss: 2.2133 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.70000\n",
      "Epoch 135/200\n",
      "29/29 - 0s - loss: 0.1981 - accuracy: 0.9322 - val_loss: 1.8911 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.70000\n",
      "Epoch 136/200\n",
      "29/29 - 0s - loss: 0.3063 - accuracy: 0.8911 - val_loss: 1.4323 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.70000\n",
      "Epoch 137/200\n",
      "29/29 - 0s - loss: 0.2110 - accuracy: 0.9278 - val_loss: 1.2488 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.70000\n",
      "Epoch 138/200\n",
      "29/29 - 0s - loss: 0.2194 - accuracy: 0.9344 - val_loss: 1.4486 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.70000\n",
      "Epoch 139/200\n",
      "29/29 - 0s - loss: 0.1774 - accuracy: 0.9433 - val_loss: 1.8647 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.70000\n",
      "Epoch 140/200\n",
      "29/29 - 0s - loss: 0.1427 - accuracy: 0.9500 - val_loss: 2.2881 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.70000\n",
      "Epoch 141/200\n",
      "29/29 - 0s - loss: 0.1262 - accuracy: 0.9611 - val_loss: 2.1241 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.70000\n",
      "Epoch 142/200\n",
      "29/29 - 0s - loss: 0.1458 - accuracy: 0.9567 - val_loss: 2.1480 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.70000\n",
      "Epoch 143/200\n",
      "29/29 - 0s - loss: 0.1387 - accuracy: 0.9478 - val_loss: 1.9917 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.70000\n",
      "Epoch 144/200\n",
      "29/29 - 0s - loss: 0.1578 - accuracy: 0.9444 - val_loss: 2.0741 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.70000\n",
      "Epoch 145/200\n",
      "29/29 - 0s - loss: 0.1789 - accuracy: 0.9389 - val_loss: 2.1256 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.70000\n",
      "Epoch 146/200\n",
      "29/29 - 0s - loss: 0.1579 - accuracy: 0.9511 - val_loss: 1.5371 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.70000\n",
      "Epoch 147/200\n",
      "29/29 - 0s - loss: 0.2143 - accuracy: 0.9233 - val_loss: 1.7110 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.70000\n",
      "Epoch 148/200\n",
      "29/29 - 0s - loss: 0.2131 - accuracy: 0.9233 - val_loss: 1.8031 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.70000\n",
      "Epoch 149/200\n",
      "29/29 - 0s - loss: 0.1662 - accuracy: 0.9522 - val_loss: 2.0661 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.70000\n",
      "Epoch 150/200\n",
      "29/29 - 0s - loss: 0.1652 - accuracy: 0.9500 - val_loss: 1.7920 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.70000\n",
      "Epoch 151/200\n",
      "29/29 - 0s - loss: 0.1434 - accuracy: 0.9511 - val_loss: 2.3836 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.70000\n",
      "Epoch 152/200\n",
      "29/29 - 0s - loss: 0.1204 - accuracy: 0.9656 - val_loss: 2.8541 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.70000\n",
      "Epoch 153/200\n",
      "29/29 - 0s - loss: 0.1197 - accuracy: 0.9633 - val_loss: 2.2137 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.70000\n",
      "Epoch 154/200\n",
      "29/29 - 0s - loss: 0.1332 - accuracy: 0.9644 - val_loss: 2.4826 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.70000\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 0.1253 - accuracy: 0.9589 - val_loss: 2.4980 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.70000\n",
      "Epoch 156/200\n",
      "29/29 - 0s - loss: 0.1522 - accuracy: 0.9489 - val_loss: 2.2132 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.70000\n",
      "Epoch 157/200\n",
      "29/29 - 0s - loss: 0.2204 - accuracy: 0.9256 - val_loss: 1.5006 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.70000\n",
      "Epoch 158/200\n",
      "29/29 - 0s - loss: 0.1688 - accuracy: 0.9411 - val_loss: 2.6706 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.70000\n",
      "Epoch 159/200\n",
      "29/29 - 0s - loss: 0.4330 - accuracy: 0.8356 - val_loss: 1.4082 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.70000\n",
      "Epoch 160/200\n",
      "29/29 - 0s - loss: 0.3599 - accuracy: 0.8556 - val_loss: 1.4711 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.70000\n",
      "Epoch 161/200\n",
      "29/29 - 0s - loss: 0.2512 - accuracy: 0.8978 - val_loss: 1.8428 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.70000\n",
      "Epoch 162/200\n",
      "29/29 - 0s - loss: 0.2411 - accuracy: 0.9244 - val_loss: 1.5634 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.70000\n",
      "Epoch 163/200\n",
      "29/29 - 0s - loss: 0.1913 - accuracy: 0.9311 - val_loss: 1.9894 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.70000\n",
      "Epoch 164/200\n",
      "29/29 - 0s - loss: 0.2209 - accuracy: 0.9322 - val_loss: 1.7281 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.70000\n",
      "Epoch 165/200\n",
      "29/29 - 0s - loss: 0.1827 - accuracy: 0.9367 - val_loss: 2.3542 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.70000\n",
      "Epoch 166/200\n",
      "29/29 - 0s - loss: 0.2262 - accuracy: 0.9278 - val_loss: 1.7080 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.70000\n",
      "Epoch 167/200\n",
      "29/29 - 0s - loss: 0.1685 - accuracy: 0.9433 - val_loss: 2.1801 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.70000\n",
      "Epoch 168/200\n",
      "29/29 - 0s - loss: 0.1796 - accuracy: 0.9367 - val_loss: 2.3360 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.70000\n",
      "Epoch 169/200\n",
      "29/29 - 0s - loss: 0.1678 - accuracy: 0.9467 - val_loss: 2.6242 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.70000\n",
      "Epoch 170/200\n",
      "29/29 - 0s - loss: 0.1793 - accuracy: 0.9500 - val_loss: 2.3042 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.70000\n",
      "Epoch 171/200\n",
      "29/29 - 0s - loss: 0.1659 - accuracy: 0.9456 - val_loss: 2.4355 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.70000\n",
      "Epoch 172/200\n",
      "29/29 - 0s - loss: 0.1979 - accuracy: 0.9389 - val_loss: 1.6854 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.70000\n",
      "Epoch 173/200\n",
      "29/29 - 0s - loss: 0.1763 - accuracy: 0.9378 - val_loss: 2.0763 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.70000\n",
      "Epoch 174/200\n",
      "29/29 - 0s - loss: 0.1548 - accuracy: 0.9522 - val_loss: 2.3547 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.70000\n",
      "Epoch 175/200\n",
      "29/29 - 0s - loss: 0.1506 - accuracy: 0.9578 - val_loss: 2.4049 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.70000\n",
      "Epoch 176/200\n",
      "29/29 - 0s - loss: 0.1444 - accuracy: 0.9544 - val_loss: 2.4251 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.70000\n",
      "Epoch 177/200\n",
      "29/29 - 0s - loss: 0.1065 - accuracy: 0.9644 - val_loss: 2.5413 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.70000\n",
      "Epoch 178/200\n",
      "29/29 - 0s - loss: 0.1009 - accuracy: 0.9722 - val_loss: 2.9824 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.70000\n",
      "Epoch 179/200\n",
      "29/29 - 0s - loss: 0.0872 - accuracy: 0.9744 - val_loss: 3.1608 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.70000\n",
      "Epoch 180/200\n",
      "29/29 - 0s - loss: 0.0997 - accuracy: 0.9722 - val_loss: 3.3973 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.70000\n",
      "Epoch 181/200\n",
      "29/29 - 0s - loss: 0.1384 - accuracy: 0.9578 - val_loss: 2.2176 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.70000\n",
      "Epoch 182/200\n",
      "29/29 - 0s - loss: 0.1259 - accuracy: 0.9656 - val_loss: 2.5757 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.70000\n",
      "Epoch 183/200\n",
      "29/29 - 0s - loss: 0.1266 - accuracy: 0.9678 - val_loss: 2.1768 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.70000\n",
      "Epoch 184/200\n",
      "29/29 - 0s - loss: 0.0966 - accuracy: 0.9744 - val_loss: 2.9194 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.70000\n",
      "Epoch 185/200\n",
      "29/29 - 0s - loss: 0.0898 - accuracy: 0.9744 - val_loss: 2.8569 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.70000\n",
      "Epoch 186/200\n",
      "29/29 - 0s - loss: 0.0881 - accuracy: 0.9789 - val_loss: 2.8195 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.70000\n",
      "Epoch 187/200\n",
      "29/29 - 0s - loss: 0.0841 - accuracy: 0.9789 - val_loss: 3.0837 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.70000\n",
      "Epoch 188/200\n",
      "29/29 - 0s - loss: 0.0926 - accuracy: 0.9678 - val_loss: 3.3701 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.70000\n",
      "Epoch 189/200\n",
      "29/29 - 0s - loss: 0.1319 - accuracy: 0.9622 - val_loss: 2.3497 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.70000\n",
      "Epoch 190/200\n",
      "29/29 - 0s - loss: 0.1407 - accuracy: 0.9600 - val_loss: 2.3857 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.70000\n",
      "Epoch 191/200\n",
      "29/29 - 0s - loss: 0.1003 - accuracy: 0.9700 - val_loss: 2.9373 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.70000\n",
      "Epoch 192/200\n",
      "29/29 - 0s - loss: 0.1231 - accuracy: 0.9678 - val_loss: 2.8918 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.70000\n",
      "Epoch 193/200\n",
      "29/29 - 0s - loss: 0.1194 - accuracy: 0.9656 - val_loss: 2.9311 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.70000\n",
      "Epoch 194/200\n",
      "29/29 - 0s - loss: 0.1051 - accuracy: 0.9700 - val_loss: 2.9299 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.70000\n",
      "Epoch 195/200\n",
      "29/29 - 0s - loss: 0.1257 - accuracy: 0.9667 - val_loss: 2.8939 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.70000\n",
      "Epoch 196/200\n",
      "29/29 - 0s - loss: 0.1367 - accuracy: 0.9589 - val_loss: 2.4509 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.70000\n",
      "Epoch 197/200\n",
      "29/29 - 0s - loss: 0.1555 - accuracy: 0.9500 - val_loss: 2.6002 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.70000\n",
      "Epoch 198/200\n",
      "29/29 - 0s - loss: 0.1997 - accuracy: 0.9433 - val_loss: 1.7383 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.70000\n",
      "Epoch 199/200\n",
      "29/29 - 0s - loss: 0.2739 - accuracy: 0.9111 - val_loss: 1.6953 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.70000\n",
      "Epoch 200/200\n",
      "29/29 - 0s - loss: 0.2188 - accuracy: 0.9322 - val_loss: 1.6319 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.70000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x185ac27aa90>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "csv_filename = os.path.join(checkpoint_path,\n",
    "                            \"training_log.csv\"\n",
    "                            )\n",
    "csvlogger_callback = tf.keras.callbacks.CSVLogger(filename=csv_filename, append=True)\n",
    "\n",
    "n_epoch=200\n",
    "\n",
    "\n",
    "LSwoFW_model.fit(train_data, \n",
    "                train_targets, \n",
    "                epochs=n_epoch,\n",
    "                batch_size=n_batch,\n",
    "                validation_data=(test_data, test_targets),\n",
    "                shuffle=True,\n",
    "                verbose=2, \n",
    "                callbacks=[csvlogger_callback,\n",
    "                           cp_callback\n",
    "                          ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x299d8e64280>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSwoFW_model.load_weights(os.path.join(\"210210_TrainingLocalitySensitivewoFW\",\n",
    "                                      \"LocalitySensitivewoFW_label\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions=[LSwoFW_model.layers[1].attention_layers[i](train_data).numpy() for i in range(10)]\n",
    "attentions=np.reshape(attentions, (900,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions=LSwoFW_model.layers[1](train_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x299d8db92b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "dr=TSNE()\n",
    "embed_attentions=dr.fit_transform(attentions)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "scatter=ax.scatter(embed_attentions[:,0], embed_attentions[:,1],\n",
    "                   c=cluster_labels_train,\n",
    "                   s=3\n",
    "                  )\n",
    "\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: '210210_TrainingDense'\n",
      "[WinError 183] Cannot create a file when that file already exists: '210210_TrainingDense\\\\Dense_label'\n"
     ]
    }
   ],
   "source": [
    "n_feat = train_data.shape[1]\n",
    "n_attention = 10 #Reduced from 20 to 10. 10 works better\n",
    "n_attention_hidden=40\n",
    "n_attention_out=1\n",
    "n_concat_hidden=128\n",
    "n_hidden1 =64\n",
    "n_hidden2 = 64\n",
    "momentum=0.8\n",
    "learning_rate=0.01\n",
    "\n",
    "n_batch=32\n",
    "\n",
    "label=\"SynthData\"\n",
    "\n",
    "save_folder=os.path.join(time.strftime(\"%y%m%d_TrainingDense\",\n",
    "                                       time.localtime()))\n",
    "checkpoint_path = os.path.join(save_folder, \n",
    "                               \"Dense_{}\".format(\"label\"),\n",
    "                               )\n",
    "\n",
    "try: \n",
    "    os.mkdir(save_folder) \n",
    "except OSError as error: \n",
    "    print(error) \n",
    "    \n",
    "try:\n",
    "    os.mkdir(checkpoint_path)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "concat_activation=\"selu\"\n",
    "attention_hidden_activation=\"selu\"\n",
    "attention_output_activation=\"sigmoid\"\n",
    "kernel_initializer=VarianceScaling()\n",
    "hidden_activation=\"selu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import attention_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_layer=Input(shape=(n_feat, ))\n",
    "\n",
    "dense_layer0=Dense(n_attention_hidden*n_attention,\n",
    "                   activation=\"sigmoid\", \n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5),\n",
    "                  )(input_layer)\n",
    "# attentions_layer=attention_model.ConcatAttentions(\n",
    "#     n_attention=n_attention,\n",
    "#     n_attention_hidden=n_attention_hidden,\n",
    "#     n_attention_out=n_attention_out,\n",
    "#     n_feat=n_feat,\n",
    "#     n_hidden=n_concat_hidden,\n",
    "#     activation=concat_activation, \n",
    "#     kernel_initializer=kernel_initializer,\n",
    "#     kernel_regularizer=l2(1E-5),\n",
    "#     bias_regularizer=l2(1E-5),\n",
    "#     attention_initializer=kernel_initializer,\n",
    "#     attention_hidden_activation=attention_hidden_activation,\n",
    "#     attention_output_activation=attention_output_activation,\n",
    "#     batch_norm_kwargs={\"trainable\":False, \"renorm\":False},\n",
    "# )(input_layer)\n",
    "##Removed dropout for attentions_layer because of Batch normalization\n",
    "# dropout0=Dropout(0.1)(attentions_layer)\n",
    "dense_layer1=Dense(n_hidden1, \n",
    "                   activation=hidden_activation, \n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5),\n",
    "                  )(dense_layer0)\n",
    "# dropout1=Dropout(0.1)(dense_layer1)\n",
    "dense_layer2=Dense(n_hidden2,\n",
    "                   activation=hidden_activation,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2(1E-5),\n",
    "                   bias_regularizer=l2(1E-5)\n",
    "                  )(dense_layer1)\n",
    "# dropout2=Dropout(0.1)(dense_layer2)\n",
    "output_layer=Dense(1, activation=\"sigmoid\")(dense_layer2)\n",
    "\n",
    "dense_model=Model(inputs=input_layer, \n",
    "                  outputs=output_layer\n",
    "                 )\n",
    "\n",
    "weights_dicts=get_weights_dicts(np.expand_dims(train_targets,1))\n",
    "loss_fn=BinaryCrossEntropyIgnoreNaN(weights_dicts=weights_dicts)\n",
    "\n",
    "# loss_fn=tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "dense_model.compile(loss=loss_fn,\n",
    "    #loss=BinaryCrossentropy(from_logits=False, \n",
    "#                                             reduction=tf.keras.losses.Reduction.AUTO,\n",
    "#                                            ), \n",
    "              optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy',]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 400)               4400      \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 34,289\n",
      "Trainable params: 34,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 - 1s - loss: 1.0864 - accuracy: 0.4978 - val_loss: 0.7268 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.52000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 2/200\n",
      "29/29 - 0s - loss: 0.7300 - accuracy: 0.5167 - val_loss: 0.8068 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.52000\n",
      "Epoch 3/200\n",
      "29/29 - 0s - loss: 0.7551 - accuracy: 0.5367 - val_loss: 0.7080 - val_accuracy: 0.4600\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.52000\n",
      "Epoch 4/200\n",
      "29/29 - 0s - loss: 0.7097 - accuracy: 0.5122 - val_loss: 0.7051 - val_accuracy: 0.3900\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.52000\n",
      "Epoch 5/200\n",
      "29/29 - 0s - loss: 0.7062 - accuracy: 0.5244 - val_loss: 0.7313 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.52000\n",
      "Epoch 6/200\n",
      "29/29 - 0s - loss: 0.7193 - accuracy: 0.5111 - val_loss: 0.7015 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.52000\n",
      "Epoch 7/200\n",
      "29/29 - 0s - loss: 0.7017 - accuracy: 0.4978 - val_loss: 0.7065 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.52000\n",
      "Epoch 8/200\n",
      "29/29 - 0s - loss: 0.7066 - accuracy: 0.5156 - val_loss: 0.7087 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.52000\n",
      "Epoch 9/200\n",
      "29/29 - 0s - loss: 0.7026 - accuracy: 0.5411 - val_loss: 0.7189 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.52000\n",
      "Epoch 10/200\n",
      "29/29 - 0s - loss: 0.7138 - accuracy: 0.4956 - val_loss: 0.7133 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.52000\n",
      "Epoch 11/200\n",
      "29/29 - 0s - loss: 0.7070 - accuracy: 0.5044 - val_loss: 0.7040 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.52000\n",
      "Epoch 12/200\n",
      "29/29 - 0s - loss: 0.6982 - accuracy: 0.5167 - val_loss: 0.7228 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.52000\n",
      "Epoch 13/200\n",
      "29/29 - 0s - loss: 0.7057 - accuracy: 0.5378 - val_loss: 0.7061 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.52000\n",
      "Epoch 14/200\n",
      "29/29 - 0s - loss: 0.7240 - accuracy: 0.5178 - val_loss: 0.7072 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.52000 to 0.53000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 15/200\n",
      "29/29 - 0s - loss: 0.7031 - accuracy: 0.5011 - val_loss: 0.7205 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.53000\n",
      "Epoch 16/200\n",
      "29/29 - 0s - loss: 0.7089 - accuracy: 0.5000 - val_loss: 0.7057 - val_accuracy: 0.4700\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.53000\n",
      "Epoch 17/200\n",
      "29/29 - 0s - loss: 0.7140 - accuracy: 0.5033 - val_loss: 0.7240 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.53000\n",
      "Epoch 18/200\n",
      "29/29 - 0s - loss: 0.7224 - accuracy: 0.4700 - val_loss: 0.7005 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.53000\n",
      "Epoch 19/200\n",
      "29/29 - 0s - loss: 0.7089 - accuracy: 0.5344 - val_loss: 0.7267 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.53000\n",
      "Epoch 20/200\n",
      "29/29 - 0s - loss: 0.7097 - accuracy: 0.4967 - val_loss: 0.7532 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.53000\n",
      "Epoch 21/200\n",
      "29/29 - 0s - loss: 0.7442 - accuracy: 0.5156 - val_loss: 0.7128 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.53000\n",
      "Epoch 22/200\n",
      "29/29 - 0s - loss: 0.7064 - accuracy: 0.5200 - val_loss: 0.7080 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.53000\n",
      "Epoch 23/200\n",
      "29/29 - 0s - loss: 0.7001 - accuracy: 0.5067 - val_loss: 0.7245 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.53000\n",
      "Epoch 24/200\n",
      "29/29 - 0s - loss: 0.7042 - accuracy: 0.5044 - val_loss: 0.6987 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.53000\n",
      "Epoch 25/200\n",
      "29/29 - 0s - loss: 0.6968 - accuracy: 0.5289 - val_loss: 0.7121 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.53000\n",
      "Epoch 26/200\n",
      "29/29 - 0s - loss: 0.6928 - accuracy: 0.5367 - val_loss: 0.7421 - val_accuracy: 0.4300\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.53000\n",
      "Epoch 27/200\n",
      "29/29 - 0s - loss: 0.6902 - accuracy: 0.5289 - val_loss: 0.7517 - val_accuracy: 0.4300\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.53000\n",
      "Epoch 28/200\n",
      "29/29 - 0s - loss: 0.7000 - accuracy: 0.5367 - val_loss: 0.7586 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.53000\n",
      "Epoch 29/200\n",
      "29/29 - 0s - loss: 0.7175 - accuracy: 0.5211 - val_loss: 0.7111 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.53000\n",
      "Epoch 30/200\n",
      "29/29 - 0s - loss: 0.6995 - accuracy: 0.5378 - val_loss: 0.7245 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.53000\n",
      "Epoch 31/200\n",
      "29/29 - 0s - loss: 0.7099 - accuracy: 0.5156 - val_loss: 0.7695 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.53000\n",
      "Epoch 32/200\n",
      "29/29 - 0s - loss: 0.7095 - accuracy: 0.5400 - val_loss: 0.8438 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.53000\n",
      "Epoch 33/200\n",
      "29/29 - 0s - loss: 0.7284 - accuracy: 0.4889 - val_loss: 0.6954 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.53000\n",
      "Epoch 34/200\n",
      "29/29 - 0s - loss: 0.7124 - accuracy: 0.5311 - val_loss: 0.7346 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.53000\n",
      "Epoch 35/200\n",
      "29/29 - 0s - loss: 0.7026 - accuracy: 0.5533 - val_loss: 0.9823 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.53000\n",
      "Epoch 36/200\n",
      "29/29 - 0s - loss: 0.7856 - accuracy: 0.5056 - val_loss: 0.7089 - val_accuracy: 0.4800\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.53000\n",
      "Epoch 37/200\n",
      "29/29 - 0s - loss: 0.7189 - accuracy: 0.5189 - val_loss: 0.7061 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.53000\n",
      "Epoch 38/200\n",
      "29/29 - 0s - loss: 0.6859 - accuracy: 0.5478 - val_loss: 0.7113 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.53000\n",
      "Epoch 39/200\n",
      "29/29 - 0s - loss: 0.7222 - accuracy: 0.5456 - val_loss: 0.7160 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.53000\n",
      "Epoch 40/200\n",
      "29/29 - 0s - loss: 0.7057 - accuracy: 0.5344 - val_loss: 0.7127 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.53000 to 0.58000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 41/200\n",
      "29/29 - 0s - loss: 0.6673 - accuracy: 0.6033 - val_loss: 0.7315 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.58000\n",
      "Epoch 42/200\n",
      "29/29 - 0s - loss: 0.6660 - accuracy: 0.6200 - val_loss: 0.7625 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.58000\n",
      "Epoch 43/200\n",
      "29/29 - 0s - loss: 0.6886 - accuracy: 0.5778 - val_loss: 0.7022 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.58000\n",
      "Epoch 44/200\n",
      "29/29 - 0s - loss: 0.6989 - accuracy: 0.5811 - val_loss: 0.6986 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00044: val_accuracy improved from 0.58000 to 0.59000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 45/200\n",
      "29/29 - 0s - loss: 0.7306 - accuracy: 0.5489 - val_loss: 0.7096 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.59000\n",
      "Epoch 46/200\n",
      "29/29 - 0s - loss: 0.6897 - accuracy: 0.5767 - val_loss: 0.7664 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.59000\n",
      "Epoch 47/200\n",
      "29/29 - 0s - loss: 0.6755 - accuracy: 0.6067 - val_loss: 0.7562 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.59000\n",
      "Epoch 48/200\n",
      "29/29 - 0s - loss: 0.6985 - accuracy: 0.5644 - val_loss: 0.8358 - val_accuracy: 0.4700\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.59000\n",
      "Epoch 49/200\n",
      "29/29 - 0s - loss: 0.7342 - accuracy: 0.5444 - val_loss: 0.7047 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.59000\n",
      "Epoch 50/200\n",
      "29/29 - 0s - loss: 0.6702 - accuracy: 0.6211 - val_loss: 0.6796 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00050: val_accuracy improved from 0.59000 to 0.61000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 51/200\n",
      "29/29 - 0s - loss: 0.6512 - accuracy: 0.6322 - val_loss: 0.7020 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.61000\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 0.6457 - accuracy: 0.6400 - val_loss: 0.7034 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.61000\n",
      "Epoch 53/200\n",
      "29/29 - 0s - loss: 0.6385 - accuracy: 0.6367 - val_loss: 0.6824 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.61000\n",
      "Epoch 54/200\n",
      "29/29 - 0s - loss: 0.6480 - accuracy: 0.6511 - val_loss: 0.7128 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.61000\n",
      "Epoch 55/200\n",
      "29/29 - 0s - loss: 0.6429 - accuracy: 0.6644 - val_loss: 0.7366 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.61000\n",
      "Epoch 56/200\n",
      "29/29 - 0s - loss: 0.6552 - accuracy: 0.6100 - val_loss: 0.7410 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00056: val_accuracy improved from 0.61000 to 0.62000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 57/200\n",
      "29/29 - 0s - loss: 0.6391 - accuracy: 0.6556 - val_loss: 0.7756 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.62000\n",
      "Epoch 58/200\n",
      "29/29 - 0s - loss: 0.6511 - accuracy: 0.6356 - val_loss: 0.7329 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.62000\n",
      "Epoch 59/200\n",
      "29/29 - 0s - loss: 0.6225 - accuracy: 0.6689 - val_loss: 0.7857 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.62000\n",
      "Epoch 60/200\n",
      "29/29 - 0s - loss: 0.6250 - accuracy: 0.6800 - val_loss: 0.8118 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.62000\n",
      "Epoch 61/200\n",
      "29/29 - 0s - loss: 0.6219 - accuracy: 0.6467 - val_loss: 0.7044 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.62000\n",
      "Epoch 62/200\n",
      "29/29 - 0s - loss: 0.6480 - accuracy: 0.6611 - val_loss: 0.7219 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00062: val_accuracy improved from 0.62000 to 0.64000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 63/200\n",
      "29/29 - 0s - loss: 0.6307 - accuracy: 0.6644 - val_loss: 0.7362 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.64000\n",
      "Epoch 64/200\n",
      "29/29 - 0s - loss: 0.5814 - accuracy: 0.7167 - val_loss: 0.7382 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.64000\n",
      "Epoch 65/200\n",
      "29/29 - 0s - loss: 0.5691 - accuracy: 0.7122 - val_loss: 0.7702 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.64000\n",
      "Epoch 66/200\n",
      "29/29 - 0s - loss: 0.5772 - accuracy: 0.7144 - val_loss: 0.6781 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00066: val_accuracy improved from 0.64000 to 0.66000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 67/200\n",
      "29/29 - 0s - loss: 0.6283 - accuracy: 0.6844 - val_loss: 0.6706 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00067: val_accuracy improved from 0.66000 to 0.68000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 68/200\n",
      "29/29 - 0s - loss: 0.5877 - accuracy: 0.7144 - val_loss: 0.7266 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.68000\n",
      "Epoch 69/200\n",
      "29/29 - 0s - loss: 0.5581 - accuracy: 0.7278 - val_loss: 0.7801 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.68000\n",
      "Epoch 70/200\n",
      "29/29 - 0s - loss: 0.5469 - accuracy: 0.7333 - val_loss: 0.8105 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.68000\n",
      "Epoch 71/200\n",
      "29/29 - 0s - loss: 0.5492 - accuracy: 0.7233 - val_loss: 0.6904 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.68000\n",
      "Epoch 72/200\n",
      "29/29 - 0s - loss: 0.5953 - accuracy: 0.6933 - val_loss: 0.7124 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.68000\n",
      "Epoch 73/200\n",
      "29/29 - 0s - loss: 0.5433 - accuracy: 0.7467 - val_loss: 1.1711 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.68000\n",
      "Epoch 74/200\n",
      "29/29 - 0s - loss: 0.5637 - accuracy: 0.7456 - val_loss: 0.8015 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.68000\n",
      "Epoch 75/200\n",
      "29/29 - 0s - loss: 0.5161 - accuracy: 0.7644 - val_loss: 0.7411 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.68000\n",
      "Epoch 76/200\n",
      "29/29 - 0s - loss: 0.5717 - accuracy: 0.7222 - val_loss: 0.7464 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.68000\n",
      "Epoch 77/200\n",
      "29/29 - 0s - loss: 0.5398 - accuracy: 0.7633 - val_loss: 0.7368 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.68000\n",
      "Epoch 78/200\n",
      "29/29 - 0s - loss: 0.5122 - accuracy: 0.7644 - val_loss: 0.7707 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.68000\n",
      "Epoch 79/200\n",
      "29/29 - 0s - loss: 0.4849 - accuracy: 0.7844 - val_loss: 0.7837 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.68000\n",
      "Epoch 80/200\n",
      "29/29 - 0s - loss: 0.5024 - accuracy: 0.7700 - val_loss: 0.7436 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.68000\n",
      "Epoch 81/200\n",
      "29/29 - 0s - loss: 0.4821 - accuracy: 0.7867 - val_loss: 0.8301 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.68000\n",
      "Epoch 82/200\n",
      "29/29 - 0s - loss: 0.4788 - accuracy: 0.7822 - val_loss: 0.9302 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.68000\n",
      "Epoch 83/200\n",
      "29/29 - 0s - loss: 0.5104 - accuracy: 0.7611 - val_loss: 0.9880 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.68000\n",
      "Epoch 84/200\n",
      "29/29 - 0s - loss: 0.4554 - accuracy: 0.7933 - val_loss: 0.7465 - val_accuracy: 0.7100\n",
      "\n",
      "Epoch 00084: val_accuracy improved from 0.68000 to 0.71000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 85/200\n",
      "29/29 - 0s - loss: 0.4379 - accuracy: 0.8189 - val_loss: 0.9258 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.71000\n",
      "Epoch 86/200\n",
      "29/29 - 0s - loss: 0.5037 - accuracy: 0.7833 - val_loss: 0.8292 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.71000\n",
      "Epoch 87/200\n",
      "29/29 - 0s - loss: 0.4248 - accuracy: 0.8311 - val_loss: 0.8612 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.71000\n",
      "Epoch 88/200\n",
      "29/29 - 0s - loss: 0.4271 - accuracy: 0.8211 - val_loss: 0.9037 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.71000\n",
      "Epoch 89/200\n",
      "29/29 - 0s - loss: 0.4368 - accuracy: 0.8167 - val_loss: 1.0346 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.71000\n",
      "Epoch 90/200\n",
      "29/29 - 0s - loss: 0.4491 - accuracy: 0.8211 - val_loss: 0.9542 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.71000\n",
      "Epoch 91/200\n",
      "29/29 - 0s - loss: 0.4336 - accuracy: 0.8200 - val_loss: 0.9384 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.71000\n",
      "Epoch 92/200\n",
      "29/29 - 0s - loss: 0.3817 - accuracy: 0.8611 - val_loss: 0.9163 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.71000\n",
      "Epoch 93/200\n",
      "29/29 - 0s - loss: 0.3912 - accuracy: 0.8556 - val_loss: 1.0033 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.71000\n",
      "Epoch 94/200\n",
      "29/29 - 0s - loss: 0.3624 - accuracy: 0.8656 - val_loss: 1.0101 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.71000\n",
      "Epoch 95/200\n",
      "29/29 - 0s - loss: 0.3916 - accuracy: 0.8456 - val_loss: 0.9495 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.71000\n",
      "Epoch 96/200\n",
      "29/29 - 0s - loss: 0.3797 - accuracy: 0.8556 - val_loss: 1.0009 - val_accuracy: 0.7200\n",
      "\n",
      "Epoch 00096: val_accuracy improved from 0.71000 to 0.72000, saving model to 210210_TrainingDense\\Dense_label\n",
      "Epoch 97/200\n",
      "29/29 - 0s - loss: 0.4169 - accuracy: 0.8256 - val_loss: 0.8758 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.72000\n",
      "Epoch 98/200\n",
      "29/29 - 0s - loss: 0.3400 - accuracy: 0.8733 - val_loss: 0.8674 - val_accuracy: 0.7100\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.72000\n",
      "Epoch 99/200\n",
      "29/29 - 0s - loss: 0.3684 - accuracy: 0.8711 - val_loss: 0.8667 - val_accuracy: 0.6900\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.72000\n",
      "Epoch 100/200\n",
      "29/29 - 0s - loss: 0.3493 - accuracy: 0.8756 - val_loss: 1.1272 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.72000\n",
      "Epoch 101/200\n",
      "29/29 - 0s - loss: 0.3380 - accuracy: 0.8756 - val_loss: 1.0750 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.72000\n",
      "Epoch 102/200\n",
      "29/29 - 0s - loss: 0.3645 - accuracy: 0.8678 - val_loss: 0.9778 - val_accuracy: 0.6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.72000\n",
      "Epoch 103/200\n",
      "29/29 - 0s - loss: 0.3111 - accuracy: 0.9000 - val_loss: 1.2577 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.72000\n",
      "Epoch 104/200\n",
      "29/29 - 0s - loss: 0.3080 - accuracy: 0.8978 - val_loss: 1.1082 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.72000\n",
      "Epoch 105/200\n",
      "29/29 - 0s - loss: 0.2582 - accuracy: 0.9267 - val_loss: 1.2056 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.72000\n",
      "Epoch 106/200\n",
      "29/29 - 0s - loss: 0.2601 - accuracy: 0.9144 - val_loss: 1.5020 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.72000\n",
      "Epoch 107/200\n",
      "29/29 - 0s - loss: 0.2708 - accuracy: 0.9233 - val_loss: 1.5759 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.72000\n",
      "Epoch 108/200\n",
      "29/29 - 0s - loss: 0.3259 - accuracy: 0.8978 - val_loss: 1.1255 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.72000\n",
      "Epoch 109/200\n",
      "29/29 - 0s - loss: 0.3757 - accuracy: 0.8544 - val_loss: 1.3499 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.72000\n",
      "Epoch 110/200\n",
      "29/29 - 0s - loss: 0.3018 - accuracy: 0.9067 - val_loss: 1.5319 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.72000\n",
      "Epoch 111/200\n",
      "29/29 - 0s - loss: 0.2970 - accuracy: 0.9011 - val_loss: 1.2956 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.72000\n",
      "Epoch 112/200\n",
      "29/29 - 0s - loss: 0.2473 - accuracy: 0.9289 - val_loss: 1.3784 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.72000\n",
      "Epoch 113/200\n",
      "29/29 - 0s - loss: 0.2605 - accuracy: 0.9189 - val_loss: 1.4900 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.72000\n",
      "Epoch 114/200\n",
      "29/29 - 0s - loss: 0.2813 - accuracy: 0.9133 - val_loss: 1.4981 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.72000\n",
      "Epoch 115/200\n",
      "29/29 - 0s - loss: 0.2338 - accuracy: 0.9289 - val_loss: 1.5706 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.72000\n",
      "Epoch 116/200\n",
      "29/29 - 0s - loss: 0.2404 - accuracy: 0.9333 - val_loss: 1.6877 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.72000\n",
      "Epoch 117/200\n",
      "29/29 - 0s - loss: 0.2721 - accuracy: 0.9244 - val_loss: 1.5469 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.72000\n",
      "Epoch 118/200\n",
      "29/29 - 0s - loss: 0.2206 - accuracy: 0.9422 - val_loss: 1.4533 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.72000\n",
      "Epoch 119/200\n",
      "29/29 - 0s - loss: 0.1720 - accuracy: 0.9611 - val_loss: 1.6167 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.72000\n",
      "Epoch 120/200\n",
      "29/29 - 0s - loss: 0.1861 - accuracy: 0.9533 - val_loss: 1.7339 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.72000\n",
      "Epoch 121/200\n",
      "29/29 - 0s - loss: 0.1757 - accuracy: 0.9622 - val_loss: 1.7833 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.72000\n",
      "Epoch 122/200\n",
      "29/29 - 0s - loss: 0.1835 - accuracy: 0.9533 - val_loss: 1.9593 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.72000\n",
      "Epoch 123/200\n",
      "29/29 - 0s - loss: 0.1566 - accuracy: 0.9711 - val_loss: 1.6830 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.72000\n",
      "Epoch 124/200\n",
      "29/29 - 0s - loss: 0.1521 - accuracy: 0.9656 - val_loss: 2.0322 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.72000\n",
      "Epoch 125/200\n",
      "29/29 - 0s - loss: 0.1339 - accuracy: 0.9789 - val_loss: 1.7762 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.72000\n",
      "Epoch 126/200\n",
      "29/29 - 0s - loss: 0.1316 - accuracy: 0.9778 - val_loss: 1.9441 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.72000\n",
      "Epoch 127/200\n",
      "29/29 - 0s - loss: 0.1899 - accuracy: 0.9644 - val_loss: 2.3184 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.72000\n",
      "Epoch 128/200\n",
      "29/29 - 0s - loss: 0.2457 - accuracy: 0.9333 - val_loss: 1.4604 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.72000\n",
      "Epoch 129/200\n",
      "29/29 - 0s - loss: 0.1971 - accuracy: 0.9467 - val_loss: 1.9376 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.72000\n",
      "Epoch 130/200\n",
      "29/29 - 0s - loss: 0.2180 - accuracy: 0.9456 - val_loss: 1.9570 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.72000\n",
      "Epoch 131/200\n",
      "29/29 - 0s - loss: 0.2239 - accuracy: 0.9456 - val_loss: 1.6020 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.72000\n",
      "Epoch 132/200\n",
      "29/29 - 0s - loss: 0.1957 - accuracy: 0.9533 - val_loss: 1.8511 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.72000\n",
      "Epoch 133/200\n",
      "29/29 - 0s - loss: 0.1656 - accuracy: 0.9644 - val_loss: 1.8463 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.72000\n",
      "Epoch 134/200\n",
      "29/29 - 0s - loss: 0.1590 - accuracy: 0.9678 - val_loss: 1.5895 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.72000\n",
      "Epoch 135/200\n",
      "29/29 - 0s - loss: 0.2247 - accuracy: 0.9389 - val_loss: 2.0958 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.72000\n",
      "Epoch 136/200\n",
      "29/29 - 0s - loss: 0.2251 - accuracy: 0.9467 - val_loss: 1.7849 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.72000\n",
      "Epoch 137/200\n",
      "29/29 - 0s - loss: 0.1311 - accuracy: 0.9789 - val_loss: 1.9680 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.72000\n",
      "Epoch 138/200\n",
      "29/29 - 0s - loss: 0.1154 - accuracy: 0.9833 - val_loss: 2.1112 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.72000\n",
      "Epoch 139/200\n",
      "29/29 - 0s - loss: 0.1016 - accuracy: 0.9922 - val_loss: 1.8960 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.72000\n",
      "Epoch 140/200\n",
      "29/29 - 0s - loss: 0.1121 - accuracy: 0.9878 - val_loss: 2.0119 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.72000\n",
      "Epoch 141/200\n",
      "29/29 - 0s - loss: 0.0926 - accuracy: 0.9911 - val_loss: 2.2786 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.72000\n",
      "Epoch 142/200\n",
      "29/29 - 0s - loss: 0.0890 - accuracy: 0.9944 - val_loss: 2.2863 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.72000\n",
      "Epoch 143/200\n",
      "29/29 - 0s - loss: 0.0826 - accuracy: 0.9978 - val_loss: 2.2754 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.72000\n",
      "Epoch 144/200\n",
      "29/29 - 0s - loss: 0.0798 - accuracy: 0.9989 - val_loss: 2.5412 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.72000\n",
      "Epoch 145/200\n",
      "29/29 - 0s - loss: 0.0770 - accuracy: 0.9978 - val_loss: 2.5131 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.72000\n",
      "Epoch 146/200\n",
      "29/29 - 0s - loss: 0.1225 - accuracy: 0.9822 - val_loss: 2.3184 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.72000\n",
      "Epoch 147/200\n",
      "29/29 - 0s - loss: 0.1333 - accuracy: 0.9711 - val_loss: 2.0200 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.72000\n",
      "Epoch 148/200\n",
      "29/29 - 0s - loss: 0.2251 - accuracy: 0.9433 - val_loss: 1.8348 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.72000\n",
      "Epoch 149/200\n",
      "29/29 - 0s - loss: 0.2318 - accuracy: 0.9411 - val_loss: 2.0262 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.72000\n",
      "Epoch 150/200\n",
      "29/29 - 0s - loss: 0.1884 - accuracy: 0.9544 - val_loss: 1.8672 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.72000\n",
      "Epoch 151/200\n",
      "29/29 - 0s - loss: 0.1139 - accuracy: 0.9900 - val_loss: 2.0557 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00151: val_accuracy did not improve from 0.72000\n",
      "Epoch 152/200\n",
      "29/29 - 0s - loss: 0.0966 - accuracy: 0.9922 - val_loss: 2.0494 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00152: val_accuracy did not improve from 0.72000\n",
      "Epoch 153/200\n",
      "29/29 - 0s - loss: 0.0812 - accuracy: 0.9978 - val_loss: 2.1485 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00153: val_accuracy did not improve from 0.72000\n",
      "Epoch 154/200\n",
      "29/29 - 0s - loss: 0.0800 - accuracy: 0.9978 - val_loss: 2.4106 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00154: val_accuracy did not improve from 0.72000\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 0.0733 - accuracy: 1.0000 - val_loss: 2.2944 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00155: val_accuracy did not improve from 0.72000\n",
      "Epoch 156/200\n",
      "29/29 - 0s - loss: 0.0710 - accuracy: 1.0000 - val_loss: 2.4330 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00156: val_accuracy did not improve from 0.72000\n",
      "Epoch 157/200\n",
      "29/29 - 0s - loss: 0.0707 - accuracy: 1.0000 - val_loss: 2.3356 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00157: val_accuracy did not improve from 0.72000\n",
      "Epoch 158/200\n",
      "29/29 - 0s - loss: 0.0823 - accuracy: 0.9944 - val_loss: 2.8058 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00158: val_accuracy did not improve from 0.72000\n",
      "Epoch 159/200\n",
      "29/29 - 0s - loss: 0.1587 - accuracy: 0.9633 - val_loss: 2.5495 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00159: val_accuracy did not improve from 0.72000\n",
      "Epoch 160/200\n",
      "29/29 - 0s - loss: 0.1626 - accuracy: 0.9678 - val_loss: 2.5109 - val_accuracy: 0.6700\n",
      "\n",
      "Epoch 00160: val_accuracy did not improve from 0.72000\n",
      "Epoch 161/200\n",
      "29/29 - 0s - loss: 0.2436 - accuracy: 0.9522 - val_loss: 2.4414 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00161: val_accuracy did not improve from 0.72000\n",
      "Epoch 162/200\n",
      "29/29 - 0s - loss: 0.2115 - accuracy: 0.9556 - val_loss: 2.2969 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00162: val_accuracy did not improve from 0.72000\n",
      "Epoch 163/200\n",
      "29/29 - 0s - loss: 0.3189 - accuracy: 0.9189 - val_loss: 1.8055 - val_accuracy: 0.6100\n",
      "\n",
      "Epoch 00163: val_accuracy did not improve from 0.72000\n",
      "Epoch 164/200\n",
      "29/29 - 0s - loss: 0.1947 - accuracy: 0.9578 - val_loss: 1.8498 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00164: val_accuracy did not improve from 0.72000\n",
      "Epoch 165/200\n",
      "29/29 - 0s - loss: 0.1112 - accuracy: 0.9878 - val_loss: 2.0686 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00165: val_accuracy did not improve from 0.72000\n",
      "Epoch 166/200\n",
      "29/29 - 0s - loss: 0.0921 - accuracy: 0.9956 - val_loss: 2.4391 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00166: val_accuracy did not improve from 0.72000\n",
      "Epoch 167/200\n",
      "29/29 - 0s - loss: 0.1095 - accuracy: 0.9889 - val_loss: 2.6423 - val_accuracy: 0.5900\n",
      "\n",
      "Epoch 00167: val_accuracy did not improve from 0.72000\n",
      "Epoch 168/200\n",
      "29/29 - 0s - loss: 0.0993 - accuracy: 0.9911 - val_loss: 2.4173 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00168: val_accuracy did not improve from 0.72000\n",
      "Epoch 169/200\n",
      "29/29 - 0s - loss: 0.0857 - accuracy: 0.9978 - val_loss: 2.6270 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00169: val_accuracy did not improve from 0.72000\n",
      "Epoch 170/200\n",
      "29/29 - 0s - loss: 0.0781 - accuracy: 1.0000 - val_loss: 2.7164 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00170: val_accuracy did not improve from 0.72000\n",
      "Epoch 171/200\n",
      "29/29 - 0s - loss: 0.0743 - accuracy: 1.0000 - val_loss: 2.8132 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00171: val_accuracy did not improve from 0.72000\n",
      "Epoch 172/200\n",
      "29/29 - 0s - loss: 0.0732 - accuracy: 1.0000 - val_loss: 2.8545 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00172: val_accuracy did not improve from 0.72000\n",
      "Epoch 173/200\n",
      "29/29 - 0s - loss: 0.0725 - accuracy: 1.0000 - val_loss: 2.8495 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00173: val_accuracy did not improve from 0.72000\n",
      "Epoch 174/200\n",
      "29/29 - 0s - loss: 0.0720 - accuracy: 1.0000 - val_loss: 2.8547 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00174: val_accuracy did not improve from 0.72000\n",
      "Epoch 175/200\n",
      "29/29 - 0s - loss: 0.0714 - accuracy: 1.0000 - val_loss: 2.8929 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00175: val_accuracy did not improve from 0.72000\n",
      "Epoch 176/200\n",
      "29/29 - 0s - loss: 0.0709 - accuracy: 1.0000 - val_loss: 2.9604 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00176: val_accuracy did not improve from 0.72000\n",
      "Epoch 177/200\n",
      "29/29 - 0s - loss: 0.0704 - accuracy: 1.0000 - val_loss: 2.9185 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00177: val_accuracy did not improve from 0.72000\n",
      "Epoch 178/200\n",
      "29/29 - 0s - loss: 0.0698 - accuracy: 1.0000 - val_loss: 3.0249 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00178: val_accuracy did not improve from 0.72000\n",
      "Epoch 179/200\n",
      "29/29 - 0s - loss: 0.0694 - accuracy: 1.0000 - val_loss: 2.9901 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00179: val_accuracy did not improve from 0.72000\n",
      "Epoch 180/200\n",
      "29/29 - 0s - loss: 0.0689 - accuracy: 1.0000 - val_loss: 3.0732 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00180: val_accuracy did not improve from 0.72000\n",
      "Epoch 181/200\n",
      "29/29 - 0s - loss: 0.0684 - accuracy: 1.0000 - val_loss: 3.0733 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00181: val_accuracy did not improve from 0.72000\n",
      "Epoch 182/200\n",
      "29/29 - 0s - loss: 0.0680 - accuracy: 1.0000 - val_loss: 3.0862 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00182: val_accuracy did not improve from 0.72000\n",
      "Epoch 183/200\n",
      "29/29 - 0s - loss: 0.0675 - accuracy: 1.0000 - val_loss: 3.0918 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00183: val_accuracy did not improve from 0.72000\n",
      "Epoch 184/200\n",
      "29/29 - 0s - loss: 0.0671 - accuracy: 1.0000 - val_loss: 3.0796 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00184: val_accuracy did not improve from 0.72000\n",
      "Epoch 185/200\n",
      "29/29 - 0s - loss: 0.0666 - accuracy: 1.0000 - val_loss: 3.1370 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00185: val_accuracy did not improve from 0.72000\n",
      "Epoch 186/200\n",
      "29/29 - 0s - loss: 0.0661 - accuracy: 1.0000 - val_loss: 3.1339 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00186: val_accuracy did not improve from 0.72000\n",
      "Epoch 187/200\n",
      "29/29 - 0s - loss: 0.0657 - accuracy: 1.0000 - val_loss: 3.1505 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00187: val_accuracy did not improve from 0.72000\n",
      "Epoch 188/200\n",
      "29/29 - 0s - loss: 0.0652 - accuracy: 1.0000 - val_loss: 3.2244 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00188: val_accuracy did not improve from 0.72000\n",
      "Epoch 189/200\n",
      "29/29 - 0s - loss: 0.0648 - accuracy: 1.0000 - val_loss: 3.2487 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00189: val_accuracy did not improve from 0.72000\n",
      "Epoch 190/200\n",
      "29/29 - 0s - loss: 0.0644 - accuracy: 1.0000 - val_loss: 3.1708 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00190: val_accuracy did not improve from 0.72000\n",
      "Epoch 191/200\n",
      "29/29 - 0s - loss: 0.0639 - accuracy: 1.0000 - val_loss: 3.2422 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00191: val_accuracy did not improve from 0.72000\n",
      "Epoch 192/200\n",
      "29/29 - 0s - loss: 0.0635 - accuracy: 1.0000 - val_loss: 3.1781 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00192: val_accuracy did not improve from 0.72000\n",
      "Epoch 193/200\n",
      "29/29 - 0s - loss: 0.0631 - accuracy: 1.0000 - val_loss: 3.1866 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00193: val_accuracy did not improve from 0.72000\n",
      "Epoch 194/200\n",
      "29/29 - 0s - loss: 0.0626 - accuracy: 1.0000 - val_loss: 3.2858 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00194: val_accuracy did not improve from 0.72000\n",
      "Epoch 195/200\n",
      "29/29 - 0s - loss: 0.0622 - accuracy: 1.0000 - val_loss: 3.2249 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00195: val_accuracy did not improve from 0.72000\n",
      "Epoch 196/200\n",
      "29/29 - 0s - loss: 0.0617 - accuracy: 1.0000 - val_loss: 3.2174 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00196: val_accuracy did not improve from 0.72000\n",
      "Epoch 197/200\n",
      "29/29 - 0s - loss: 0.0613 - accuracy: 1.0000 - val_loss: 3.2119 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00197: val_accuracy did not improve from 0.72000\n",
      "Epoch 198/200\n",
      "29/29 - 0s - loss: 0.0609 - accuracy: 1.0000 - val_loss: 3.2259 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00198: val_accuracy did not improve from 0.72000\n",
      "Epoch 199/200\n",
      "29/29 - 0s - loss: 0.0604 - accuracy: 1.0000 - val_loss: 3.2899 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00199: val_accuracy did not improve from 0.72000\n",
      "Epoch 200/200\n",
      "29/29 - 0s - loss: 0.0600 - accuracy: 1.0000 - val_loss: 3.2565 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00200: val_accuracy did not improve from 0.72000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x185b09f0b80>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "csv_filename = os.path.join(checkpoint_path,\n",
    "                            \"training_log.csv\"\n",
    "                            )\n",
    "csvlogger_callback = tf.keras.callbacks.CSVLogger(filename=csv_filename, append=True)\n",
    "\n",
    "n_epoch=200\n",
    "\n",
    "\n",
    "dense_model.fit(train_data, \n",
    "                train_targets, \n",
    "                epochs=n_epoch,\n",
    "                batch_size=n_batch,\n",
    "                validation_data=(test_data, test_targets),\n",
    "                shuffle=True,\n",
    "                verbose=2, \n",
    "                callbacks=[csvlogger_callback,\n",
    "                           cp_callback\n",
    "                          ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processed in excel by combining training and validation accuracy columns from individual training_log.csv\"\n",
    "import pandas as pd\n",
    "SynthDataFolder=\"SynthData_10dim_clusternoise\"\n",
    "df=pd.read_csv(os.path.join(\"SynthData10dim_results.csv\"),\n",
    "               index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>LLDLwFW_valacc</th>\n",
       "      <th>LLDLwoFW_valacc</th>\n",
       "      <th>DenseModel_valacc</th>\n",
       "      <th>LLDLwFW_trainacc</th>\n",
       "      <th>LLDLwoFW_trainacc</th>\n",
       "      <th>DenseModel_trainacc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.647778</td>\n",
       "      <td>0.498889</td>\n",
       "      <td>0.497778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.728889</td>\n",
       "      <td>0.504444</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.512222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.524444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  LLDLwFW_valacc  LLDLwoFW_valacc  DenseModel_valacc  \\\n",
       "0      0            0.53             0.48               0.52   \n",
       "1      1            0.68             0.38               0.48   \n",
       "2      2            0.63             0.48               0.46   \n",
       "3      3            0.73             0.44               0.39   \n",
       "4      4            0.70             0.51               0.48   \n",
       "\n",
       "   LLDLwFW_trainacc  LLDLwoFW_trainacc  DenseModel_trainacc  \n",
       "0          0.647778           0.498889             0.497778  \n",
       "1          0.728889           0.504444             0.516667  \n",
       "2          0.796667           0.520000             0.536667  \n",
       "3          0.795556           0.526667             0.512222  \n",
       "4          0.853333           0.530000             0.524444  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAEWCAYAAABseTM4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAACzR0lEQVR4nOydZ3gUVduA7zOzJT1AAkFCL4GEJlIERAUFBBUs6CuKotIEe8Huq4i9oWJXxIIFu6AidgRF/QBfekeQEmpCyiZbZ873Y3ZDEjbJpiGSc3PtRXb3zDnPzM7uPPNUIaVEoVAoFAqFQmGh/dMCKBQKhUKhUBxNKOVIoVAoFAqFohhKOVIoFAqFQqEohlKOFAqFQqFQKIqhlCOFQqFQKBSKYijlSKFQKBQKhaIYSjlSFCGE6C+E2FmD890lhJhRU/MdjQghXEKI1uW8v00IMbAG1qmRef7tCCHWCCH6B/8WQog3hBAHhRD/J4Q4WQix4QjL87IQ4r9Hck2FQlH7KOWoAoQQ1wohlgohvEKIN8O8f7oQYr0QolAI8ZMQokU5c10hhDCCF1SXEGJr8Mc9rQIZ7gqOdQkhdgohPqiBXUMIIYUQbWtorsMUKynlw1LKcVWYa4EQwiOEyBdC5Akhlgkh7hBCOCsxR43tW3lIKeOklH8F13xTCPFgba9ZVYQQU4QQ7/zTclQHKWVHKeWC4NN+wCCgqZSyl5RykZSy/RGWZ6KU8oEjuaZCoah9lHJUMZnAg8DM0m8IIZKBT4H/Ag2ApUBFistvUso4IBEYCLiBZUKITuEGCyEuBy4DBga36wH8ULVd+VdxrZQyHjgOuAUYCcwTQoh/Vqy6ixDC9k/LUIoWwDYpZcE/LYhCoTi2UMpRBUgpP5VSfg5khXn7fGCNlPIjKaUHmAJ0FUJ0iGBeQ0q5RUp5NfBzcNtw9AS+kVJuCW63R0r5KoAQ4kIhxLLig4UQNwsh5gT/flMI8YIQ4qugFeYPIUSb4HsLg5usCFqkLio2xy1CiH1CiN1CiCuLve4UQjwphNguhNgbdClECyFiga+BJsWsYk1KWyqEEP2EEIuFEDlCiB1CiCsiOE4FQUvBcKAPcFZwrl5CiN+Cc+0WQjwvhHCUtW9CiPpCiC+FEPuDbpgvhRBNw60phLhSCPFFseebhBAfFXu+QwhxfPBvKYRoK4SYAIwCbguu+UWxKY8XQqwUQuQKIT4QQkSVtb9CiPFCiHXBz2utEOKEMGNKWKhKW+2EELcLIXYF59gQtG4OAe4CLgrKtyI4NlEI8XrwGO4SQjwohNCD710hhPhVCPG0ECKLMOdocMxfwbW2CiFGldr2+eB+rxdCnF5suzLXLe84iKB7UQgxFpgB9Anuz/1hjkMzIcSnwc88SwjxfBnHfIoQ4kMhxNvB9dYIIXoUez9dWNbMnOB7w8N9FkKI5OB5lSOEyBZCLBJCaMH3mgghPgnKslUIcX1Z54BCofjnUcpR9egIrAg9Cd7Bbgm+Xhk+BU4u473fgdFCiFuFED2KX0CAuUArIUR6sdcuA94u9nwkcD9QH9gMPBSU9ZTg+12DrqGQxasxllUrFRgLvCCEqB9871EgDTgeaBscc29wv4cCmcG54qSUmcV3Qljuxq+B54CGwTmWl3dQiiOl3I5lmQsdJwO4CUjGUppOB64uZ9804A0sa0NzLItd2IsllrJ6shBCE0I0ARzBNRBWfFEcsLKUfK8C7wKPB9ccVuzt/wBDgFZAF+CKcIsKIS7EUkBGAwlYCmE4pbxMhBDtgWuBnkHL2xlY1pX5wMPAB0H5ugY3eRMIYH2e3YDBQHFX6InAX0AKwXOn2FqxwHRgaHCtvpT8TE/E+j4kA/cBnwohGlS0biTHQUr5OjCRoCVWSnlfKdl04Evgb6Al1rk6u+wjx/Dg+/WwvlfPB+exA18A3wKNgOuAd4PHuTS3ADuxzu8ULGVUBhWkL7B+K1KxztUbhRBnlCOPQqH4B1HKUfWIA3JLvZYLxFdynkwst9xhSCnfwfpBPgPror1PCHF78D0vlhvvUgAhREesC8GXxab4TEr5f1LKANbF+/gKZPEDU6WUfinlPMAFtBdCCGACcJOUMltKmY91sR0Z4T5eAnwvpXw/OHeWlHJ5hNuGKDpOUsplUsrfpZQBKeU24BXg1LI2DK73iZSyMCj7Q2WND8YQ5WMdq1OAb4BMYVkETwUWSSnNSsg9XUqZKaXMxrpIHl/GuHFYytUSabFZSvl3JdYBS2l0AhlCCLuUclvI6lgaIUQKcCZwY9BCtw94mpKfaaaU8rngcXaHmcYEOgkhoqWUu6WUa4q9tw94Jvh5fwBsAM6KYN2aOA69gCbArcE1PFLKX8oZ/4uUcp6U0gBmASHlsTfW9/xRKaVPSvkj1vfr4jBz+LHcwC2C+7xIWs0rewINpZRTg3P8BbxG5N8dhUJxhFHKUfVwYd3ZFicByBdW5kzIxbQmzLbFSQWyy3pTSvmulHIg1l3tROCBYnedbwGXBJWXy4APg0pTiD3F/i7E+qEvj6ygIlV6m4ZADFZ8VI4QIgeYH3w9EpphWRGqQ9FxEkKkBV0Ye4QQeViKWnJZGwohYoQQrwgh/g6OXwjUK2WJK87PQH8s5ehnYAGWYnRq8HlliPQzqPYxklJuBm7EsrzsE0LMDlq/wtECsAO7i32mr2BZSELsCP0hLDdq6Jy+K2gxvAjrnNwtLPdtcZfyLlmys/XfWApLRevWxLnSDPi71LlcHqU/oyhhxVg1AXaUUob/xjoXS/MElnX226Cr8Y7g6y2wXM45xfb3LizrkkKhOApRylH1WMOhO8yQm6ENVhzSomIuporcbOcBiypaLHg3+hGWS6dT8LXfAR+Wu+kSrLve2uAAliuqo5SyXvCRGAwSB5DlbAvWRbZNVRcXQjQDunPoOL0ErAfaSSkTsC425QVr3wK0B04Mjg+53sraJqQcnRz8+2cqVo4qOgYVEekxKsBSVEM0LiGElO9JKfthXZQl8FgZ8u0AvEBysc80odT5WrSNtDKzQuf0w8HXvpFSDsKymKzHsoiESA0q7SGaY1n/Klq3WudKsTmai+oHkWcCzUKxQ0GaA7tKD5RS5kspb5FStsZy090cjLPaAWwttq/1pJTxUsozqymbQqGoJZRyVAFCCJuwAmh1QBdCRBX7wf0My6UwIjjmXmCllHJ9BPPqQohWQojnsC7C95cx7gohxFlCiPhgDMxQrJimP4oNexsrRsJfgeugNHuBMmv0FCd45/wa8LQQolFQttRiFqy9QJIQIrGMKd4FBgoh/hM8pkkiGNRcHkGLz6nAHOD/gHnBt+KBPMAVtFZMqmDf4rGUu5xg3Mt9lM/PwAAgWkq5E0spGwIkAf8rY5uIj2cZzAAmCyG6C4u2InxpiOXAmUKIBkKIxliWIsCKORJCnCassgcerH0OWT32Ai1DF3op5W6sWJqnhBAJwfOrTfB4V4gQIkUIcU7wpsCLZUktbmFpBFwvhLAH44jSgXkRrBvpcSiP/wN2A48KIWKD39uTKjkHWN+zQqxAe7uwaiwNI0z8khDi7KCsAsu9bmAdj//DsibfLqwEBl0I0UkI0bMK8igUiiOAUo4q5h6sC8wdWLE97uBrSCn3AyOw4lcOYgWgVhRH0EcI4cK6sC/AcsP1lFKuKmN8HpZVZDuQAzwOTCqlBM3CsiRVtobNFOCtoKn/PxGMvx3LbfB70DX1PZY1hqBC+D7wV3C+Eq4caQVUn4llwcnGusB3pWyeF0LkY13QnwE+AYYUc29MxrKU5WMpbaVLKJTet2eAaCwL2O9YLsEykVJuxLrYLwo+z8MKTP41GJcSjtexYn1yhBCflzd/GWt+hHUuvRfcr88JH4s2Cyu4dxuWklF8351YgfMHsFxFjYA7g++FMu6yhBB/Bv8ejRVwvhbrHP4YywoUCRpwM5Z1JRvLqlZcSf0DaBeU5SHgAillKLC6zHUrcRzKJPgZDcMK+N6OFSh9UbkbhZ/HF5xnaHA/XgRGl3ED1A7rO+ECfgNelFL+FJTlbKxYs63BeWZgJT4oFIqjEFEyJEDxb0QIEY0V/HqClHLTPy2PQiGsMg3jgu49hUKh+FehLEfHBpOAJUoxUigUCoWi+hxtFW8VlUQIsQ0rqPjcf1YShUKhUCiODZRbTaFQKBQKhaIYyq2mUCgUCoVCUYx/nVstOTlZtmzZskrbFhQUEBsbW7MC1RBHq2xKrsqh5Ko8R6tsx5pcy5YtOyCljLRoq0JRp/nXKUctW7Zk6dKlVdp2wYIF9O/fv2YFqiGOVtmUXJVDyVV5jlbZjjW5hBCVbcGiUNRZlFtNoVAoFAqFohhKOVIoFAqFQqEohlKOFAqFQqFQKIrxr4s5UigUCkXNsGzZskY2m20GVvshdbOsqCuYwOpAIDCue/fu+8INUMqRQqFQ1FFsNtuMxo0bpzds2PCgpmmq6J2iTmCapti/f3/Gnj17ZgDDw41RdwoKhUJRd+nUsGHDPKUYKeoSmqbJhg0b5mJZTMOPOYLyKBQKheLoQlOKkaIuEjzvy9SBlHKkUCgUCoVCUQylHCkUCoXiHyMmJqZbbc+9bds2+5AhQ1oDLF68OPqDDz5IrI31pk+fnjR69OjmVdl2w4YNjpdffrlB6PnChQtjrrjiimbVlenLL7+Mj4+PPz49PT2jZcuWnXr06NH+/fffr5X9L05qamrn3bt3/2vjmpVypFAoFIpjmpYtW/rnz5//F8DSpUtjvvrqq1pXDirLpk2bnB988EGRcnTKKacUvvnmmztqYu4ePXq41q1bt3bbtm2rp0+fvn3y5MnN58yZE18Tcx8p/H5/tecIBAIRj1XKkUKhUCiOKhYvXhzdtWvXDmlpaRmDBg1qs3//fh1g9erVzr59+6a1b98+IyMjI33NmjXO3NxcrU+fPmkZGRnpaWlpGe+880690vNt2LDB0a5du44ej0c88sgjTb744ov6HTp0yHjttdfqt2jRolNmZqYNwDAMmjdvXvS8OAMHDmzTsWPH9LZt23Z88sknk0OvP/vss0ktW7bs1Llz5/TFixfHhV5/7733Ert06dIhPT09o2/fvmk7duywAdx8881Nzj333FbHH398hxYtWnR66qmnkgHuvvvu1KVLl8Z16NAh4/7772/05Zdfxg8YMKCtYRikpqZ2PnDggB6au0WLFp127Nhhy8zMtJ1xxhltOnXqlN6pU6f0b7/9tsKme3379nXfeuutmc8//3wjgLLmyMvL0y688MKWnTt3Tk9PTy86rtOnT086/fTT2/Tq1at9ixYtOt1yyy3HRfaplr3WTz/9FHP88cd3SE9Pz+jWrVuHFStWOENrnXbaaW179+6d1rdv3/bTp09PGjx4cJuTTz65XYsWLTpNnDixaWjuTz/9NOH444/vkJGRkT506NDWubm5GlgWrEmTJqVmZGSkz5w5s36ksv5rTV4KhUKhqDnGjKHZ6tXE1OScnTpROHMmlbZ+XHHFFa2efvrp7WeddZbrxhtvbHL77bc3mTlz5o5LLrmk1eTJk/eMHj06p7CwUBiGIaKiosyvvvpqc4MGDczdu3fbTjzxxA6XXHJJjqYdfu8fFRUl77zzzsylS5fGvv3229sB1q9fHzVjxowG99577745c+YkpKenu5s0aXKYieHdd9/dlpKSYrhcLtGtW7eMSy+99KDX69UeffTRJsuWLVvXoEEDo2/fvu07depUCDBo0CDXyJEj12uaxrRp05KnTp3a+LXXXtsJsG7duuhly5aty8/P17t165YxYsSI3IceemjXU089lfLTTz9tBssdBqDrOoMHD8559913691www1ZP/74Y2xqaqqvWbNmgWHDhrW6+eab955xxhmuTZs2Oc4444x2f/3115qKjm+vXr0Kp0+f3hjgqquuahZujrvuuuu4AQMG5H300UfbDhw4oPfo0SN9+PDheQArV66MXbVq1Zq4uDizW7duGeecc07uKaecUljRumWt1bVrV8+SJUvW2+12Pv/88/jbbrut6TfffLMFYM2aNTErV65ck5KSYkyfPj1p7dq1MStWrFgbHR1ttm3bttPkyZP3xsbGyocffvi4hQsXbkxISDDvvvvuxg888EDKk08+uRsgKSkpsHbt2nUVyVccpRwpFAqF4qghKytLz8/P18866ywXwPjx47MuvPDC1gcPHtT27t3rGD16dA5ATEyMBKTX6xU33nhj099//z1O0zT27dvn2Llzp6158+YR+VAmTZp0YPjw4W3vvffefTNnzky+4oorDoQb99hjj6V89dVX9QD27NljX7NmTVRmZqa9d+/e+SFl6vzzz8/euHFjFMDWrVsd5557btP9+/fbfT6f1qxZM29orqFDh+bExcXJuLi4QJ8+ffIWLVoUW79+faMsGS+55JLsqVOnNrnhhhuy3n333QYjRozIBvj1118TNm3aFB0a53K59NzcXC0xMdEsb5+lPJSgWNYcCxYsSPjmm2/qhZQor9crNm/e7ADo169fXuPGjQ2As8466+CCBQviIlGOylorOztbv+iii1pt27YtSggh/X6/CI05+eST81JSUoqOTb9+/fKSkpIMgLZt23q2bNnizM7O1rds2RLVq1evDgB+v190797dFdpm9OjRByuSrTRKOVIoFAoFVbHwHA288sorDbKysmyrVq1a53Q6ZWpqame32x1xyEjbtm39ycnJgblz58YvX7489vPPP/9r8+bN9rPPPrsdwJgxY/ZnZGR4fv755/ilS5euj4+PN3v16tW+ojWuvfba5jfccMOeUaNG5X755ZfxU6dObRJ6TwhRYmzp56U5/fTTC8aOHevMzMy0zZ8/v95DDz2UCZaS8+eff64LKooRs2TJkpi2bdt6yptDSsnHH3+8uWvXrt7ir//yyy+xlZW/+Jzh1hozZkzzU089Nf+7777bsmHDBsdpp53WPvReTExMCUXP4XAUbavruvT7/UJKSb9+/fK++OKLreHWjY+PL1dZDIeKOVIoFArFUUNSUpKRkJBgzJ8/Pw7g9ddfT+rTp4+rfv36ZuPGjX2zZs2qB+B2u0V+fr6Wm5urJycn+51Op/ziiy/iMzMzHeXNn5CQYLhcrhLXvjFjxuwfN25cq2HDhmXbbDbatm3rX79+/dr169evve222/bn5OToiYmJRnx8vPm///0vasWKFbEAp5xySsEff/wRv2fPHt3r9YrPPvusKKYlPz9fb968uR/gzTffTCq+3tdff12vsLBQ7NmzR//999/j+/XrV5CYmGi4XC6dMGiaxtChQ3OuvvrqZm3btnWHrDb9+vXLe+SRRxqFxi1evDg63PbF+eOPP6KfeOKJJtdcc82+8uYYMGBA3lNPPZVimpZe8euvvxbN/csvvyTs3btXd7lcYt68efVOPfVUFxFQ1lp5eXl606ZNfQCvvPJKclnbl0X//v0Lli5dGrd69WpncD5t5cqVzsrOUxylHCkUCoXiH8Pj8WgpKSldQo8pU6akvPHGG1tvv/32pmlpaRkrV66MfvTRRzMB3nnnna0vvPBCo7S0tIwePXp02LFjh23cuHHZK1asiE1LS8t46623klq1auUpb72hQ4fmb9y4MToUkA1w8cUX5xYWFuoTJkzICrfNiBEjcgOBgGjdunXHW2+9NbVr164FAC1atPDffvvtmb17907v0aNHh7S0tKK177777syLL764TceOHdOTkpJKuPjS09ML+/bt2/7EE09Mnzx58u6WLVv6e/Xq5dZ1XbZv3z7j/vvvb1RahlGjRmXPmTOnwQUXXFDkInr11Vd3/Pnnn7FpaWkZbdq06fj88883DCf/0qVL40Kp/FdffXXzJ554Yvs555yTX94cjz76aGYgEBAdOnTIaNu2bcd77rknNTRfly5dCoYPH96mY8eOHYcNG3awLJda165dM0Kf67hx45qWtdbtt9++Z8qUKU3T09MzKpNRFqJJkyaBV155ZdvIkSNbh86NVatWRVV6omKI4r7HfwM9evSQS5curdK2CxYsoH///jUrUA1xtMqm5KocSq7Kc7TKdqzJJYRYJqXsUfy1FStWbOvatWvYGJu6xMKFC2NuuummZsuWLdtQ22vdfPPNTeLi4oypU6fure21aoPp06cnFQ9o/zezYsWK5K5du7YM916tWo6EEEOEEBuEEJuFEHeEeb+FEOIHIcRKIcQCIUTTcPMoFAqFQlEb3HXXXY1HjhzZ5uGHH971T8uiOHqotYBsIYQOvAAMAnYCS4QQc6WUa4sNexJ4W0r5lhDiNOAR4LLakkmhUCgUiuI8/PDDex5++OE9R2q9adOmZR6ptWqD66+/PgsI6348lqhNy1EvYLOU8i8ppQ+YDZxTakwG8GPw75/CvK9QKBQKhUJxRKnNVP5UKJEauhM4sdSYFcD5wLPAeUC8ECJJSllCKxVCTAAmAKSkpLBgwYIqCeRyuaq8bW1ztMqm5KocSq7Kc7TKpuRSKOou/3Sdo8nA80KIK4CFwC7gsEJYUspXgVfBCsiuapDk0RpgCUevbEquyqHkqjxHq2xKLoWi7lKbytEuoHhH4abB14qQUmZiWY4QQsQBI6SUObUok0KhUCgUCkW51GbM0RKgnRCilRDCAYwE5hYfIIRIFkKEZLgTmFmL8igUCoXiKCMmJqZbbc+9bds2+5AhQ1qDVXjwgw8+SKyN9aZPn540evTo5lXZdsOGDY6XX365Qej5woULY6644opm5W0TCV9++WV8fHz88aE6Rz169Gj//vvv18r+Fyc1NbXz7t27/2nvVJWpNeVIShkArgW+AdYBH0op1wghpgohhgeH9Qc2CCE2AinAQ7Ulj0KhqFsYpiTP5yfP58f8F9RzM6SkwF/5AniKimnZsqV//vz5fwEsXbo05quvvqp15aCybNq0yfnBBx8UKUennHJK4ZtvvlkjLV169OjhWrdu3dpt27atnj59+vbJkyc3nzNnTnxNzH2k8Pv91Z6jMgUma7XOkZRynpQyTUrZRkr5UPC1e6WUc4N/fyylbBccM05K6S1/RoVCoYiMXL+f/W4v+z3eo1rpkFKS7/Oz0+Vmn9tLwDz6FbnaZvHixdFdu3btkJaWljFo0KA2+/fv1wFWr17t7Nu3b1r79u0zMjIy0tesWePMzc3V+vTpk5aRkZGelpaW8c4779QrPd+GDRsc7dq16+jxeMQjjzzS5IsvvqgfqpDdokWLTpmZmTYAwzBo3rx50fPiDBw4sE3Hjh3T27Zt2/HJJ58sanHx7LPPJrVs2bJT586d0xcvXhwXev29995L7NKlS4f09PSMvn37pu3YscMGVhHIc889t9Xxxx/foUWLFp2eeuqpZIC77747denSpXEdOnTIuP/++xt9+eWX8QMGDGhrGAapqamdDxw4UNRapEWLFp127Nhhy8zMtJ1xxhltOnXqlN6pU6f0b7/9NraiY9u3b1/3rbfemvn88883Aihrjry8PO3CCy9s2blz5/T09PSi4zp9+vSk008/vU2vXr3at2jRotMtt9xyXGSfatlr/fTTTzHHH398h/T09Ixu3bp1WLFihTO01mmnnda2d+/eaX379m0/ffr0pMGDB7c5+eST27Vo0aLTxIkTi2ojfvrppwnHH398h4yMjPShQ4e2zs3N1cCyYE2aNCk1IyMjfebMmfXDS3Y4qn2IQqE45jCkJM8XINqm49R0cn0BjoZuALleH5mF7hKPnQVuDnh82DQBAgJmpXtkHnNcccUVrR5++OGdGzduXNuxY0f37bff3gTgkksuaTVx4sR9GzZsWLt06dL1zZs398fExJhfffXV5rVr1677+eefN951111NzTKOYVRUlLzzzjszhw0bdnD9+vVrx48ff/CCCy7ImjFjRgOAOXPmJKSnp7ubNGlymDb97rvvbluzZs265cuXr33llVdS9uzZo//999/2Rx99tMnixYvXL1myZP3GjRuL+o8NGjTItXz58vXr1q1be8EFF2RPnTq1cei9devWRf/yyy8bfv/99/VPPPFEk23bttkfeuihXT169HCtX79+7X333bcvNFbXdQYPHpzz7rvv1gP48ccfY1NTU33NmjULXHXVVc1uvvnmvatXr1732WefbZk4cWLLSI5vr169Crds2RIFUNYcd91113EDBgzIW7Vq1bpFixZtuOeee5rm5eVpACtXroydO3fu5jVr1qyZO3dug4ULF8ZEsm5Za3Xt2tWzZMmS9evWrVt733337brtttuKlJ41a9bEzJkzZ8uSJUs2AKxduzbm888//2vdunVr5s6dW3/z5s323bt32x5++OHjFi5cuHHt2rXrTjjhhMIHHnggJTRHUlJSYO3atesmTJhw8DChyuBf6w9UKBSKcJhB95REogkNTUBhIIDHMHHqlbsfDLnjBBV3HpdSUp76le8PkOX14dBK9hbVhCDaFpRLgs80iSJs/9E6QVZWlp6fn6+fddZZLoDx48dnXXjhha0PHjyo7d271zF69OgcgGBnd+n1esWNN97Y9Pfff4/TNI19+/Y5du7caWvevHlE5sJJkyYdGD58eNt7771338yZM5OvuOKKsO1UHnvssZSvvvqqHsCePXvsa9asicrMzLT37t07P6RMnX/++dkbN26MAti6davj3HPPbbp//367z+fTmjVrVuQZGTp0aE5cXJyMi4sL9OnTJ2/RokWx9evXPyxTO8Qll1ySPXXq1CY33HBD1rvvvttgxIgR2QC//vprwqZNm4oUMpfLpefm5mqJiYnlatjFbxTKmmPBggUJ33zzTb3p06c3BvB6vWLz5s0OsBrIhprfnnXWWQcXLFgQV1Z/teKUtVZ2drZ+0UUXtdq2bVuUEEL6/f6iL9vJJ5+cl5KSUnRs+vXrl5eUlGQAtG3b1rNlyxZndna2vmXLlqhevXp1APD7/aJ79+5FzXBHjx4dsVIUQilHCoXimKEwEGBPoReQOPVDCoZd09jj9lCu9lIKn2nyd771e2/TBKmx0WjlKEh73F48gTKvb0ggStfLnUPXBF5DWY4qwyuvvNIgKyvLtmrVqnVOp1OmpqZ2drvdEWvBbdu29ScnJwfmzp0bv3z58tjPP//8r82bN9vPPvvsdgBjxozZn5GR4fn555/jly5duj4+Pt7s1atX+4rWuPbaa5vfcMMNe0aNGpX75Zdfxk+dOrVJ6L3SinZFivfpp59eMHbsWGdmZqZt/vz59R566KFMsJScP//8c11QUYyYJUuWxLRt29ZT3hxSSj7++OPNXbt2LRHu8ssvv8RWVv7ic4Zba8yYMc1PPfXU/O+++27Lhg0bHKeddlr70HsxMTElvhAOh6NoW13Xpd/vF1JK+vXrl/fFF19sDbdufHx8pb9Uyq2mUCiOGdwBA10IYmw29GI/2HZNI1rXibZF/tAQRX8HpMRfTixQwDRxG0a588XYyleMAGxC4DHKVrDqAklJSUZCQoIxf/78OIDXX389qU+fPq769eubjRs39s2aNasegNvtFvn5+Vpubq6enJzsdzqd8osvvojPzMx0lDd/QkKC4XK5Slz7xowZs3/cuHGthg0blm2z2Wjbtq1//fr1a9evX7/2tttu25+Tk6MnJiYa8fHx5v/+97+oFStWxAKccsopBX/88Uf8nj17dK/XKz777LOimJb8/Hy9efPmfoA333wzqfh6X3/9db3CwkKxZ88e/ffff4/v169fQWJiouFyucKaDDVNY+jQoTlXX311s7Zt27pDVpt+/frlPfLII41C4xYvXhwdbvvi/PHHH9FPPPFEk2uuuWZfeXMMGDAg76mnnkoJuSh//fXXorl/+eWXhL179+oul0vMmzev3qmnnuoiAspaKy8vT2/atKkP4JVXXkkua/uy6N+/f8HSpUvjVq9e7QzOp61cudJZ2XmKo5QjhaIGCLlyTAkF/kCFD0MF3dYK7oBpxe7UAr5ylBaPYSBq4CPVhCAgJUbQ7eE3TfzHeAySx+PRUlJSuoQeU6ZMSXnjjTe23n777U3T0tIyVq5cGf3oo49mArzzzjtbX3jhhUZpaWkZPXr06LBjxw7buHHjslesWBGblpaW8dZbbyW1atXKU956Q4cOzd+4cWN0KCAb4OKLL84tLCzUJ0yYELZn2IgRI3IDgYBo3bp1x1tvvTW1a9euBQAtWrTw33777Zm9e/dO79GjR4e0tLSite++++7Miy++uE3Hjh3Tk5KSSrj40tPTC/v27dv+xBNPTJ88efLuli1b+nv16uXWdV22b98+4/77729UWoZRo0Zlz5kzp8EFF1xQ5CJ69dVXd/z555+xaWlpGW3atOn4/PPPNwwn/9KlS+NCqfxXX3118yeeeGL7Oeeck1/eHI8++mhmIBAQHTp0yGjbtm3He+65JzU0X5cuXQqGDx/epmPHjh2HDRt2sCyXWteuXTNCn+u4ceOalrXW7bffvmfKlClN09PTMyqTURaiSZMmgVdeeWXbyJEjW4fOjVWrVkVVeqJiiKMhSLEy9OjRQy5durRK2x7NlWWPVtmUXJGR7/Oz1+3lr2X/R7uepbvklERKSazdRkp0tb67leJoO17FqSnZDCnZnl9IlK5FbOYvj9W//0an3n0A8Bkm0TadhtHhb0b3FnrwmiYOrfr3m+6AQUqME49hkuP1E6VrHBcTVbRPVT1eQohlUsoexV9bsWLFtq5du4aNsalLLFy4MOamm25qtmzZsg21vdbNN9/cJC4uzpg6dere2l6rNpg+fXrS0qVLY99+++3t/7Qs1WXFihXJXbt2bRnuPRVzpFDUAHn+AFG6Zrli9PKDaaWUFPgN3HbLDaOoGfymCSLy+IfKYNME7jIsR6aUuA0DZw0oRiH2ua0wj2hdw22a+EyzRAyVoua46667Gr/55psN33jjjbDxKoq6iVKOFIpq4jfNIstCJAghcGiCvW4PtuAF1a4JGkY5K4xJUZSNzzArFXBdGTQhMEyTgCkPc9sVBgJIWXNKWSijzuMW3HijnXHX+4jr7KdRdO0qR2PmjGm2et/qiFKyI6VTo06FM8+ZWSOFDGuLhx9+eM/DDz+850itN23atMwjtVZtcP3112cBYd2PxxIq5kihqCRGMCYk9CjwB6jsddGmaTg0zUoRBwr8BoWlfO1GsdgTRXjMYp+DO2DUWrxRCJ9plKi27TUM9nt8OCpZIqA8NCHQhODHbzU+fFdn4qVO9h40jvnYI4XiaEJZjhSKSuDy+dnn8ZVQhqSk0vVzgBJWIocmyPUFiLXZEEKQ7/Oz3+NDF5AaG11kYVIcImCaZBa6MYK6iiktN1RtoSHY6/aiC2gSE40uBPvcXuxClMiMqyl++lYjKlqybYvgtklO3n4vQMPYchOxqsXRbOG56667Gjdr1sw3adKk7IrGTp8+Pem+++5rmpKS4gcr+Pmzzz7bVtk177jjjsaPPvpojVuUDhw4oLdt27Zzdnb2ck3T+P7772MHDRrUYfPmzSvbtGnjz8rK0lu3bt05Ozt7uR7Glbpt2zb7xIkTm4XaoZRFTExMt8LCwv+Vfn3WrFn1MjIyPN27dy83cL2uo35xFYpKkOsP4NCsuKLQI8amV/viaNM0fIaJ1zSRUpLj8+PUNcxgUUDF4WR7fUGFyPocYiNIla8OTt0qB2BIK2jaW+Rmq5mf0fw8yAnmIUkJP36rM3CIyb0PB5g/18blozQ83rppSfzhhx8SzjnnnLxIx4cqYK9fv35tVRQjgOnTp0fcFiNEJP2/kpOTjYYNG/r/97//RQEsWrQoLj09vfCnn36KA1iwYEFsly5dCsIpRlCyT1xV+Pzzz+utXLmywpT/uo5SjhSKCAnFFtWGlQCsoN8Dbh+ugEFASvSge6WwnMKCNUnANHH5A+QfgYe3ErV8TCkPkyvP58flr9kg6EixC0GeP0Ch36CmvHg5B+H03g6GD3QQCMDaVYI9uwX9BxmMnuTjnke8fD3HxmNPHluK8n//+9+UBx98sBHA2LFjm/Xu3TsNYO7cufHDhw9vBZCdna35/X6tSZMmgQ0bNjh69+6dlpaWltGnT5+0TZs2RWxK++9//5vSqVOn9LS0tIybbrqpqCBjuJ5pV199darX69U6dOiQMXz48Fah3myhbe69996Um2++uQlAr1692o8ZM6ZZp06d0h988MGURYsWxfTs2bN9x44d0/v169fu77//tpeWpUePHq6ff/45DuD333+Pu+aaa/aG+rL98ssvcb1793YFAgGuuuqqpiGZn3jiiWQ41CcOID8/XzvzzDNbt2nTpuOgQYPadOnSpUPxVh7XXXddavv27TO6du3aYceOHbbvvvsu9vvvv693zz33NO3QoUPGmjVrnA8++GCjNm3adExLS8s4++yzW0f+6R3bKLeaQhEh7oCBqKVsKLAKFXoMgwMeDzZxKFD7SChHhpTsdXvxGiZHJCRcQJOYqAozsKSUZHl85PkDh93JObXyU/Z37oDGx4Gthn/lbJqGO2DFANVE6r5pwg0T7OzcLpBS8OG7OvuDnbX6nR4g3mHjtlt0umYYnDFQsOz/qr3kUUP//v1dTz75ZAqwb/ny5TE+n0/zer3i559/jjv55JPzAb744ouEU045JQ9g0qRJzUeNGpV13XXXZT3zzDNJkyZNavb9999vKT1vsLFsXHCbvc2aNfNv3rw5auXKleuklAwcOLDt119/HTd06FDXu+++uy0lJcVwuVyiW7duGZdeeunBF198cdebb77ZaP369WvBUkjK2w+fzydWr169zuv1it69e7f/6quvNjdp0iTw2muv1Z88eXLqRx99tK34+L59+7oWLlwYDxzYvn2788orrzw4c+bMhgB//PFH7O23377nmWeeSU5MTDRWr169zu12i549e3YYNmxYXvFz/oknnmhYr149Y8uWLWuWLFkS1adPnyIFzu12a3369HE999xzuyZOnNj0ueeea/j444/vHjhwYM7ZZ5+de+WVVx4EGDBgQOO///57VXR0tCze3Lauo5SjOoaUkmyvP+LgzoAp2VNYvmvarmk0cNprTWn4p3H7A+T6A3gNs0hpqS2idJ2AaRa5ajQhMAyrEKC9GhdiU1Lu5xgwJQFpEnOESgsETJPdhV6idK3cc8yUEo9hElPJ2kVffKpx1Wg7vfpKXnrTx3FNKt6mMggBJtSIG+/tGTrfztN54Ak/n36g88DdNgpc0LO3SaPjpOUytNu4YHj15T7a6NevX+Hll18em52drTmdTtmlSxfXokWLYn777bf45557bjvA/PnzE8eOHXsA4H//+1/s119/vQVg0qRJ2ffff3/TcPMOGzbsYPE6PBMmTGi6cOHChIyMjAyAwsJCbf369VFDhw51heuZ1rhx44LK7MfFF1+cDbBy5Urnpk2bok877bQ0ANM0adiw4WG+tv79+7umTZvWeP369Y6mTZt6Y2JipJRS5ObmamvWrInt379/wUsvvdRw/fr1MXPnzq0PVsXttWvXRnXs2LHoy7J48eK4G264YR9Az549PWlpaUXFGO12uxw5cmQuQPfu3Qu+//77hHCyt2/f3n3eeee1Gj58eM6oUaNyKrPfxzJKOapj+E1Jrs+PI0J/gERW2CW8MGAQb7fh0I895ciUkv1eH0jQhaj1bCggbAyLz6i6cmRIS/Hxm+VbhaKOYB0dm6YhpHVuVXSORVdCMQoE4KvPNW6YYKd9umTVcsGZpziZ+6OXZs1rSnrLalUT0T87tsOD/7Vx6ukGYycZZHSSXHiWneHnmzw0zbqm1pYb92jA6XTKZs2aeV988cXkXr16ubp27er+/vvv4//++29nt27dPGApRP379/+7OutIKbnxxht333rrrSUKXn755ZfxkfRMs9ls0ix2jno8nhJjQr27pJSibdu27uXLl68vT57OnTt78/PzbR9//HG9E0880QVW1ennn38+OTU11ZuYmGhKKcVTTz21fcSIESVirSqyYhWXWQv+ZthsNgKBQNgT6aefftr09ddfx8+ZMyfxySefPG7Dhg1r7PbDPIF1DhVzVMcoDATQhHVxiuQhEBWOCXU9P5aQwQt3rs+PISUOXYtIMaqNzHtNCNyVdK0ZpiV/wDTJ8VkXWXsFn2NE8xrg8UAEcacVYimbFZ9jkSpGC77X6JXuZOLlDlq2lnwy38cXP/goLIRR5zmY9qjOC9P0GpFdBOPBqsr/lgquG2fn3EFOpIQnnrPKQfQ9xWT9Li8vveWnQbAb17Fe+6pPnz6uF154IaV///75AwcOzH/rrbcaZmRkFGqaxtKlS6Patm3rsQV9o926dSuYMWNGfbAazvbo0SOinl5Dhw7NmzVrVnJubq4GsHXrVvuuXbtsZfVMA0u58Hq9AqBp06aB7Oxs2549e3S32y2++eabxHDrdOnSxZOdnW37/vvvY8HqZL906dKwpfCPP/541yuvvNKoX79+BcHjUPDyyy836tmzpwtg0KBBuS+99FLDkAwrV6505uXllfii9unTxzV79uz6AMuWLYvauHFjhYHWcXFxRmgewzDYsmWLY9iwYfkvvPDCLpfLpefm5irXGspyVKeQUpLnD1TLPRMOu6aR5w+Q6Dg2XGtSSvZ5vBT6jWAn9YqPl2HAHTfa+H5eb+YtokbdOHZNUBAIkCQdER3fAr+ffW5f0XOJRFQjkmjnDvhgluX62bBW4PUKNE3Str3kmpsC/GfUPxMkPONFnSW/a7RqI9mySfDV5xppHSSPPOPntMEmdjs0SJK8MdvPJefaeeIB6254yR8aL7/lJ+rIdW8B4GA2vPmaztLfNX78Vqd+A0nP3iaXjzdo1uKQVh1fyvmhHwFr5T/Jqaeemj99+vTGp512WkFCQoLpdDrlSSed5AKYO3du4uDBg3NDY19++eXto0ePbvnss882TkpKCrz99tvbIlnj/PPPz1uzZk1Uz549O4DV6f3dd9/dOmLEiNxXX321YevWrTu2bt3aE+qZBjBq1Kj96enpGZ06dSqcO3fu1ltuuWV3z54901NSUvyhjvaliYqKkrNnz95y/fXXN8/Pz9cNwxCTJk3a26NHj8PG9+nTx/Xzzz8nhpSj/v37uyZOnOjs27dvAcBNN910YNu2bc7OnTunSylFgwYN/PPmzSsRX3Xrrbfu/89//tOyTZs2Hdu0aeNp27atp379+uXeSY0aNSp70qRJLV9++eWU2bNnbxkzZkzL/Px8XUopxo0bty85Obludz4Oonqr1TKRNo78/Zdf6N2vXxnvihKuBRnsEF7ZwnMew2B3gadSLSuK95cqD3fA4LjYqBp1zUgpcRsm4coe//HLL/Q/9dSIlTGvYWJIK2ao9HGTwbgWGVynwG/g8gciPk6BAFw71s6cj3V03aRHb3jsWT9IaJ9RM98vt2GUGcBc/BwzJez3eHFoWgl3TKSfY2kWfK9x5Ug7Xg/06ivp1t2kQZLE5RL89J3GutWCz7/z0b1X1fezMrKFfq5mvqxzz2Q7yQ0lB/YLUptJzjjL4O4HAsSEqfGcmwO6Dh+9p3PXzXYaJEuGnWdwQg+Tjl0kcfFWnFJCPThnhEFivcofM8OAZ5/Q+fVnjewDgg4ZkgtHGZw22KSwEC4808H/lgpat5Wcda7JdbcEiIsvb18lXtOkRVxMifO8LvVW69u3b7v3339/W4sWLWrA3nfsEQgE8Pl8IiYmRq5Zs8Y5ePDgtC1btqyOior6d13Y/yFUb7V/kGyPj4JgllN5BEyTvcF+SqUxpaRRlJN4h3Xnm+fzk+PzkxobE3EMjJSSHK+/1kz0uhDkeP2kRNdM00+w4qP2FHrCHju/aRZZqyrCYxjsDgb86kLQLDa6hIxe0ySz0FOUli2o2FpkmlZsS/sMycyXdeZ8rPPfB/34XJt47NEM+vewGpTe/5ifCdfWwI2YBE/AOEw5klKyr9Br1UIKym8XWrXjVL7+QuOn7zQ+mKXTrr1k5mw/zVuW/L29+kYYfJKDCZc5+Hiej1Ztau/3OC8XXp5u46P3dHZut/ZtyDCDGe/6MU2oKEQisZ71/5VXGbRPl7z5ms6H7+q89drhP4F332yjfbqkQ1pbxtkF9RtAk6ay3DWkhHsm23jzVRvHdzdp1kKyeJHG5x/rdOth4i6EDesEr7/vZ+iwyCxtEtBFzX2f/o0sXrx40z8tw9FMfn6+dvLJJ7f3+/1CSsnTTz/9t1KMagalHNUiUkoKDSOigNLyGpaaUpLl9RNjs2Eiyfb5kRJyfH6SoyIr8+ExTAoDRq1VEA6lnHsq0WOsIryGpVSGOy4CQZbHZ/XTquDaUeA3sAfjW0LF+4pbuAr9BjZBxI09DQMmX2Nj9qxDX59rbgpw9U0Gq3/fS7N2bTEC8M08nftut7Nvj+COKYFqpZTbNQ1XwCCxVFN4vynxmTV3zAEW/aQxZqSDhETJ6UNMpr3op179w8cl1oPX3vEzcriDwSc5mP5a5Bf+yrBquWD8pXa2bxOcerrJRZdKEhIll44x0HXLIlQZ+p5i0vcUE8OAvzYLVq8QHNgvGHK2SXY2zJujs+JPwZw5qXzyifV96dnb5LNvfWWu9eLTOm++amPSDQHufdiKv/P54I1XdL74VCc6Bp55uXLHx5QS+zHuUlNUj/r165urV69e90/LcSyilKNaxGdajTCre+enCYGUJrsLPcH4Ecuykevz4zMjs0r4DIlDE7V2FxpqpnrA46VJbHSNZNgUBgxsZcwTOgaRBCrbhSiK29CEpQyFlCMpJfkRxmEFAvDeWzqzXtdZvULjuskB6teXuD1w422H5Bgx0roAnj/S5O5bJC88bWPxIo1BZxqcd6FJy9aVv7GzBeOOMgvdROs69YLxXYWBQI3WJfL74Z5bbbRoZbJgqa/CuJwu3STfLvYy4VIH4y6x8/RLfv5zqcmWTYKXntW5coJBxy5Vv5Fd+KPGFRfZqV8f5nzvo2fvyOfymyamlGUqvboO7dpL2rU/NGezFtC1m6XcLP5mCfvyTmTdGo3pT9h4/22dS688/Hz77EONB/9r55wLDO558FBigsMBV11ncNV1VbMcmjJ85qJCoah9lHJUi3iNmruLjtI1DGlZTEIun2hdI9LOEnatdvo/FccWLGK4z+2lUbSz6KItqLyCaMUbGeUW2dOEqHT5ALumke8PkOi0I7A+IxOJVk79ItOEzRsFd95kY/FCnYxOJs++6qswENlmg8eeDdCzt8nTj9p4fKqd156XvPe5j+O7V15hiNZ1TBMOBvwIIMFhtwLsa8gamJcLU++2sXGdxpsfVqwYhWjaDD6e5+PKi+zccJWDt1832bBO4MoXfPK+zqVjDOIT4JTTDE7sKyNu0vvNVxoTLrXTpp1k9lwfjRpHvi+huDwhrL+rclOQkBig7xkm51xg8sdijUem2IiOlpx6mklyI2vM4kWCG6+y07ufyTOv+KlJXcZEYq/DLjWF4p9E3ZbUIoU12CVcBGvsFJ9PK/ZaRY+KFKMD+2H5surLGqXreAyD7fmF/B18ZHl8FW9YCr9phUfXdIyUJgSmlEXy7S70lGmd8nhg+hM6HZs7ObW7k/8t1Xj2VR/f/16xYlScCy42+XWFj99WeYlPkFx4loMNayu/X6HPO0rXyPb6+Du/kIApa0TpXfE/Qb/jnbz3ps7YSQEGn1k5xT4mFt762M+9D/lxF8IJPUzm/+LlpFNN3npN55nHdM4b7KRNIycZzZxMe0SnrA4iUsKH72qMvdhOxy5WSn5lFCOwlN4Ehx2HpmFUM+lECHj4KT+mAdeOdXDWAAderxVDNGakgxatJW/MjlyZjBSJshwpFP8UynJUg4SyS5DWD5vbMIj6F/y4ud1w0dkO1q0RPDY9wGVjqhdAXDxGKFQ+IM5hq1QmW2V6b1WWSOJz9u+Fi89xsGaVxuAzDc442+TkAUa1Cgm2bC357FsfQ/o5GX+Zna9/9hEbV/l5NCGIqcGeGH8sFlw2wkFiPfh6kY+u3aqmTERFwaQbDSbdeOize+dTK8mowGUFsK9ZpfH3VsETD9pZ8IPO2IkBGsbbWL1C8MzjNnQdcrJh4U86fU42efsjX7kZXeGQ0lKs6znsFPgh2+vHVs2vYUZnycqtXuZ/oTHhMgcvPK3zyfs6Tie8+5kvbExWTXCs1zhSKI5Wjv4r978Ir2GSWeBmt9vDHrenSu6kI0VI9zBNuOcWG2tXa3Q9QXLbdXY6tnBy+okOvvxM45v5jXn0fht7dldtHSEEds0Kni7wB0o8ylOAyos3qm327IbzznDw1xbB2x/7eOsjP5dcXj3FKESTVHhhpp/NGwTnD3Hw4TuRu0ZrEimhsAB+/Fbj4nMcNGosmfO9N6xiZJU6MA57mJWwyMTGwX8uNbn/sQBvfOBn2ot+du0QTLzcwYjzT2ZQXyeLF2osXyZYu1rj/sf8zJ5becUILKtjrF23rGw2W4mAfb9pUtXyJXY7DDvfpN+pBk88YGfbX4JXZvlqtPJ2afmO5erYIXRd796hQ4eMtm3bdmzfvn3Gfffdl2LU4s1RWXz55ZfxQoju06ZNSw69tnjx4mghRPd77703JdJ5SjepreoYxT+LshzVID7TREMc0TYMleWDWTrPPqGz7S9Br74SjxtW/Klx/a0BbrkrwEvP6uzeJVi8SGP8pQ4gHYDXXtDpfLykQZLk3ocClQoqtmsaXsNgn+fwUgXhaveYwXijf6LjuisfLjvfwZ7dgtlzffTqU/NZsScPMJn+mp9nHrNxw1UOfv7R4NlX/TXeILU4Ulo1i1avEOQcFHz+sc6uHdaFt2Nnk9lzfUVxNKXxm5IoXS9hcTNMSY7PX6VebELAxZcbXHSZwR+LBV9/soOkJi24YrxRlHJfHQJSEhfMu3doAg1R5FrzGRKpUa1WN3dNDXDuII3b/hug90k1d36YUuI3TQJCFJ37dUE5cjqdZqjB665du2wXXnhh67y8PP3pp5/OPNKytGvXzv3JJ5/Uv/nmmw8AzJo1q0H79u3dR1oOxT+PUo5qEHfAOKqr2R7Mhrsn22jZSjLhWoPv52t4vYLnZvgYMdJECLjhVuuOLRCA+V9ouLJW0/u0jkx7xMaunbB4ocbgkxw8/7q/UnEp4TKGQs1HY3Qdp00rqlnkNUwkR87qtn2bIDsrVKvGzro1glmf+GtFMQpxwcUmI0b6eO5JnUem2JEmvPCGP+Jg5cpwYB/ceUdXli21yj4IITnlNJMrxptEx8AFF5evlBhSEmvTi+pswSFrUnUa4moa9OknibftoFPvsP1DK40pJZo4VKdKCEEDp539wbi3eIdOYSVbsZSmWw/Jmu3eKlm1yiNgSuLsNgwTvKZBA6fjyP6ejBnTjNWrw5TQrAadOhUyc+aOSIenpqYGZsyYsa1v374ZTz31VKZpmlxzzTVNf/3113ifzyfGjx+/79Zbbz3w5Zdfxk+dOrVJgwYN/Bs2bIju3Llz4eeff75V0zSuvvrq1G+++aaeruuyf//+ea+++urOzMxM25VXXtli165dDoBp06ZtHzx48GHNZVNTU335+fn6jh07bKmpqYEff/wxceDAgUUVuhcvXhw9adKkFm63W2vRooX3vffe29awYUNj0aJFMePGjWsJ0L9//6JeaIFAIKz81TqmiiOCUo5qCFkD1o5AAL6eq9F/oHlYC4GaYObLOgUuSxlK7ySZ8mjZY202OPs8k9W/59KytWT6a1bsyI7tMH6UgzEj7Tzzip8LLq66TyjUfNRrGrg8AaJ0Daeu4w4YNe7vlRKy9kP9pJJ1cb77WmP8KDvB9kXUT5K89KafAYNq39clBFx/q4Ep4bH77fTobTJmonXh3vG3YPkyQVJDSd+Tq66kFRbAZRc4WLfaztTHLfeg3WGlmVeG0hlxQgiSohzsKvBg/4ec84aUeI3Dm+kmOGwlYnXiHXYMKXEHrCDtAn/1XTY1rRgBGEiidZ3YKBtSHvttQ8oiIyPDZxgGu3btsn3wwQf1EhMTjdWrV69zu92iZ8+eHYYNG5YHsG7duujly5f/1bJlS3/37t07fPfdd3Fdu3Z1z5s3r/5ff/21WtM0Dhw4oANcddVVzW6++ea9Z5xxhmvTpk2OM844o91ff/21Jtz655577sFZs2bV79GjR2Hnzp0LnU5n0RfwiiuuaPX0009vP+uss1w33nhjk9tvv73JzJkzd4wdO7bls88+u33o0KGuq666qkjTf+aZZ5LDyX+0hlsoDqGUoxoiICWyGjWNfD7BVaPtzJuj0/l4k/sf87PsD40zzjZL1GGpKvv3wowXbZxxlkF6p6rP16y5lbZ9xX/sXD/eTkFBgMvHVf1iowsrk05ikuXxkRIdhStQ/f5vfywW/G+pxikDTD58V+fD93QOZgnS0k3uuC/ACT1M3ntbZ9rDNjp2kVx9o5/sLMF5/7GsKKGg3soioUQsTiQBtddPNlj2h8aUO6yv4/59gmcf15FS4HRKfl7mo0WrykuTnQWTLrez8n+C++5fzfhr0io9BwCCsJ9HqEWJZa2p/R/74p+JxLIwNoxyHBZgH84VVc/pIMFRtc/0iCHBoevWsfwnrp2VsPAcKb7//vuE9evXx8ydO7c+QH5+vr527dooh8MhO3fuXNCmTRs/QMeOHQu3bNniOO2001xOp9O86KKLWp599tk5F110US7Ar7/+mrBp06aipqzBBqtaYmLiYXdBo0ePzh4xYkSb9evXR19yySXZv/zySxxAVlaWnp+fr5911lkugPHjx2ddeOGFrQ8cOKDn5+frQ4cOdQGMGTMm68cff0wsT/6OHTuG7c2mOHpQylENEUn/tBCrVwgem2pjxZ8a2VnWa1KeimkKLhsb4KN3dc4/wyqF/PRjkkem+blwlBnW5VJYaFkBbDbrb03jsJTiXxZoXDvWjscNN98ZOHySShIXD7M+9TPhUjt33GAHCZePr97duCNYvXp7QSFSgqMK6UWBAKxeKfh6rs7zT+mYpnXAhJAMH2HSsbPJe2/pjBl5yGxy9nkG0170H2ap85gmuhBVathqyND/VgxaRT3wNA2efdXPxNF27r7Fcl1ddGmACy42ufw/dqbcaeON2ZG1ltqwVnDdeDtZ+wV+v9VTbNqLATq2y6r0foCl6NlE+FIQQgiibRoew8RxBJQjt2liK/aZJEU5Srj6KiKkwNk07YgpdJEipQwqoUePTP8Ua9eudei6TmpqakBKKZ566qntI0aMyCs+5ssvv4wvbtHRdZ1AICDsdjvLly9fN3fu3ISPP/64/ksvvdTo999/3yil5M8//1wXExNToX7cvHnzgN1ulwsXLkyYOXPm9pByVBXKkn/Dhg2VtN0qjjRKOaoh3AGzwh9bKeHdN3XuucVGQiKcNtgg5TjLvbJ/106Gnt+EgUNMRl5msGaVRs8TTe64ySqs986bJk9M99M+Q7Ljb0FiPclfWwSXnOvAboPju5ss/FHD77fSjq+/NUDj4yQvT7fx1ec6rduavPuZv1rViosTHQ2vv+9n9AXwwD02zjrXILlhNeesRguM3BzLfbTkN0sROf8igxtuC/Drzxo9e5t06mrt94TrDH5bpLF+jaBrd5M+/cIfDyEhJcYZcUuREFs0QbM46wa1MBBgT6GXSH4FGyTBB1/6+eEbA68XzjrHUrZvvC3Aw/fZ+X6+wcAhhyvgPh+sWyNo3kLy/ls6TzxkIz4e+vU3OZgNt98X4PgTJKt/r9RuFBGQssy2NmCVbSjwG7We9+ozTaI0jeNioqodixala1aB0aNIOTKkxKlrR5XC9k+QmZlpGz9+fIsrr7xyn6ZpDBo0KPell15qePbZZ+c7nU65cuVKZ8uWLcu8U8jNzdVcLpd20UUX5Q4cONDVpk2bzgD9+vXLe+SRRxo98MADe8GKHerbt2+Zgdb333//rj179thtxbIkkpKSjISEBGP+/PlxQ4YMcb3++utJffr0cSUnJxvx8fHGN998E3fGGWe43nzzzQahbSorv+LoQSlHNYCUksJAoNyCj2433HqtnU9m65x6usHzr/tLKBOrf99Kp95WpbsTekpO6GlZYj752sfsWTqP3GdjyCkOTu5v8t3XOlFREk2HpGRJRifJquUaI0cb1KtvxS2NH2VdkmNiJbfe42fiDUbYbuXVweGAqY8HGNDTwXNP2rj/sepbpSJhyybB9u0xdOptPV+3WnD9eDsb1gkefcZP/4FmkRsqrUNJi5bTCf0HmvQfWPb8IfdNdV17Tk2vVIVmIThMAZpwncFnH+rccJWdN2b7mPuJTnS01R+s/0CTmyba+fSDQ8rLwCEGT73gr3TRxLIwTInTUfZxqMgqVhNIKQmYkpRYZ40E6UfpGi5/4KgqZBKQkkS9bv4ce71erUOHDhmBQEDoui4vuuiirPvuu28vwE033XRg27Ztzs6dO6dLKUWDBg388+bN21LWXDk5OfrZZ5/d1hsMInzggQd2ALz66qs7xo0b1zwtLS3DMAxx4okn5vft23d7WfMMGjTosGBtgDfeeGPrpEmTWlx//fVa8+bNve+///42gNdff33buHHjWgohSgRkV1Z+xdGDqGrNj3+KHj16yKVLl1Zp2wULFtC/f/+aFQirYGFmgadcy8fdt9iY+bKNW//r54ZbjcMaWK7+/Tc69e5T5vZ7d8PVY+ws+0Nj/DUGLpfVNPPpl/00SS05NhCAzz/SMAKCs841qhU8WpFcADdeZePzj3QWLPVVqW9YZdi1Ewb1cVJYYPLsqwa/LtR49w2dhESrftBpg6sfSG0E48eaxkVXPLgUpc+xzEI3pimrVel480bBGf0cFBZYMUimCX6/oFdfk/9brHH5+ACNm0i6dZecenr4/Y/kcyyOGQx2NoHUmCiiyji3TSn5O7+QqAiaK5dFebJZiQ4m9Z126jtrxhPhMQx2V/B9rUiumsYdMGgcExWR9bSqv2NCiGVSyh7FX1uxYsW2rl27quwpRZ1kxYoVyV27dm0Z7r26eatSwxQEjHJTsH//VTDzZRtjJwW4+Y6qxeakHAcfz/NT4Ko4U8Zmo1pZZJXl1v8GmP+lzlWj7cz9wYfTWfE2lcXvh03rBbffYMfng+OauJl4eRw2m2TMRIOb7wxQv0HF80SCYcoa63IfZ7OR7fVV64vWNk3y6tt+fvxO45qbAyQ3hGces/HMYzqnnGbw0FOBSnemrwivYZLosBNt03GWYx2y+ttZff9spb4DPsPEiCAE2sTK9AyHlBBvtxrt1hROTSPaZrW5ORpqkoXin6KOgBVOoVBERq0qR0KIIcCzgA7MkFI+Wur95sBbQL3gmDuklPNqU6aaRkpJvi8QtkHqwWx4bKqNTz/QadHK5K77q+d2EqJ2UoirS2pTeOYVP1de5GDsxXauvilQrfTzEPv2wPuzdObN0Vm/RuDzCYSwUu0bJf6P35b2Ztj5NZPNVxwTWWMXqihdr5EMqdOHmJxezOV2238DnPcfg6bNZY0rRgAIiLXbylWMQsTYdHJ9fmwcEsSUEgNJ4+ioCgsZbtM0UmPKttLZNVGjNa+EEDSMdrK30FOmUgYllTYpLXdcRTFBftMkYMpD2WbB7cqT32daJQZUerdCcfRQa8qREEIHXgAGATuBJUKIuVLKtcWG3QN8KKV8SQiRAcwDWtaWTLVBYcAI3vkdfhGZepeNj9/XGT7C5MbbA8TE/gMCVoKqdi8HGHK2yb0P+Xn6MRs/fOPkjin+ooKSVSE7Cwaf5GTvHkHPPibjrjbo2MWkWw9JqzaS1b8HuPnO2mkxIGXNxdKEso+qc2zLoqaVwhCmlFaWXYSZUwkOO4UBA69hFLkPvYZVxDASC5zgyMQuFUcXguNiojDLOYR/axrNY61AvRyfn3y/v0JLU8CUNIp2Fo3L8/s56PWXq2RKCbE1ZKmsAqZpmkLTtH9XfIVCUU1MK525TBdLbVqOegGbpZR/AQghZgPnAMWVIwmEkqgTgSNeLr46mFKS7fWFDdzduF7w4bs64642jligcnUpMAxsiDLjSypi0o0Gl08wmHy1nUenWG6QQUNM0jvJSld+vvc2O1kH4MufvHTvdeR/t2uqG7rVW07DlFCNjhVHlFBvskiVOV0IUqKdHPD4CARjGGPtNhJq0BVWGwghKvxMQoUYExw28nz+cpVcw5TYNEGM7dCxq+ewW33syrFQxdr1sJbnI8Tq/fv3ZzRs2DBXKUiKuoJpmmL//v2JwOqyxtSmcpQKFC8qthM4sdSYKcC3QojrgFignByifxbDtCo5F8cTsEzo0aVq8gQC8OA9NqJj4LrJlVOMzGAwcG1Wx/UFm1vagkX8QuvahYZNCKtCdXB5E3lYg1i7VrZ7ISYGnn7FT1YWPDrFzqNTYPgIgyefP7yWUFl897XGJ7N1brojcMQVI0NK7MWOS03g1DXcfgP9H6jsJ6XEb8qwn2NZBExJrK1ygWM2TaNxTFTFA/+l2DWNGJtOQcAoU6EKSGgU5SihPAkhaBDloIbC4WqcQCAwbs+ePTP27NnTiaMqf0+hqFVMYHUgEBhX1oBay1YTQlwADJFSjgs+vww4UUp5bbExNwdleEoI0Qd4HegkpTRLzTUBmACQkpLSffbs2VWSyeVyERdXtXpeflMSrr5u6TKBBw44ePihjqxaWY8JV23mwv9EVnTWXeAiOjYOM7hG1coPVowEJBJdWM04teAqJlZQqE0catIJUOhyEVPsmIUqQGsVSGeakLkrmgULGjHr7ZaYpobNZnL5FVsZeXGZGbS4XDbGjelFfIKfF19ait0e/vwMHa+axgwem6oqR+HOMVNKAhEcs9rARCIQeApKfo4VcSQbnlbne1mblJardPXzcByJ41bV4zVgwIDDstUUCkV4alM56gNMkVKeEXx+J4CU8pFiY9ZgKVA7gs//AnpLKfeVNe8/kcq/1+3BHag4s2XRTxpXX2mnoAAen165vmOrf/+NtF4nYhMaTk0jP1BxfENVCJgmuiZIiY5iu6sQp2YFi3oMgyTn4RWHSx8zr2GSWegutzBgaf5cIvjxW51VywXfzrPqPBW4BOdeaDBmYslMv5sm2vjoPZ0vF/g4/oSyz83aSrN2BwyOi42q8rEPd455Aga73Z5KHbOaQEqJxzRpGhvNrwsX1koZi5qgtkpsVJdjTa5wqfwKhSI8telWWwK0E0K0AnYBI4FLSo3ZDpwOvCmESAeigP21KFOl8ZsmBX6DmAricH7+QePS8+20bif5+Gs/7dPDX9gNKfEZ5mG9k0wkppQkRTuwCUGBETgsk0ZAtRWmgJTE6lZjzhibXtT6IdIgZIcm0KhcPy2rqGUA04T775R8NUcnPl5yz2Q7336lcerpJqefYfLzjxqzZ9m44bZAuYpRbWFKKzaqOs2Dw1FecdDKEjAlftOyLzp1y/1n1SM6/HhJCfWd9moXs1QoFIq6Rq0pR1LKgBDiWuAbrDT9mVLKNUKIqcBSKeVc4BbgNSHETVhW6yvkUVaVstAfqNAZsm61YNwoO+3aSz7/zkdCYvhxocJ6jaIcOEspW9s0jaax0UWBwKkx0SUueIaU7C6sfq9CKSnKnIm12Sjwe5HCWieSi6gQgli71TLCWckIY02D+x8LcP9jAaSEV57Tee0FGwt/0nngHmvMmecY3HpPxXFakbg4KovflMTabDWeVWb1aKuZjDW/aZIU5UBKSbbPT5SmIZGkxkSHDXq3qfRwhUKhqDS1WucoWLNoXqnX7i3291rgpNqUoTpIKcnzB8q1qJgm3Hy1nehoeOezshUjsFxSSU47cWGyeAQlM6R0TZQI4LVDzXRAL9Zh3alrICylwFGJvk6xNhv5vurXbJp4vcHE6w327YHPP9b5e6vgngcrLmhoSCvAuDLNfiMlzl7zXwkhBE5dw29IwhmRdEFESlPoviHWZkMiyfL58QUVuiOdCq9QKBTHMqpCdjn4TCuQNroci8qH7+gsX6bx3AzfYW08ihPquh1nr3p6c7Su4w4YOKqYEx66uIasCTZNI8Fux2saxNkiPxVCF+Kaqt3TqDFMuDbymkVew8QuBC3ij/LCUcWIs9nI53CFUkpwGwbResWp834pibHpwUxGQbSuUeA3iLPXQklyhUKhqMMo5agccn2+MnNbpYS5n2hMvdtGz94mI0aWb8UwpMSpadVK0Y/SNVyBqltsQp2/i1+Ek6Iq369KL6dlRG0QMM2i+jkht+C/rXt5vMN+WLA7WArmQZ9VKLAindcwJcnFPq94mw2/WXPVvBUKhUJhoZQjLEuERJYIdvYEDFwBI6zVyOuFa8fa+fIznY6dTaa95K+wyKHflCRFVa8oniOMv8mUEm+E7iVTQv0achvF2nRySrWMqA0CpiQgoX4xxSLGbmNTra565BBCUN9hxx50mVY0tvg5GmO3YatGw1eFQqFQhEcpR0Cez0+e30+MTccWbAPiDhjYxeE9nQIBuPIiOz99p3PPA34m3mBE1ttKVD/TrHQrCiklHsOkgdMeccxJTVXideo6En+NzBUOr2kQ0hUax1Q9tf7fgBAirFWpIrRSypJCoVAoagalHGFZjqJ03bJSYMW+aCJ8C4m3XtP56TudR5/xc/n4yOJkTGkVFrRXM6VbE6Io7ghACitVu56z8q6x6uLQNZC10zPMMCVCClJinMHjptxGCoVCoThy1HnlyJRW1lOUrqFXcBE+sB+eeNDGKacZjB5XiQBi06RBDXXdbhTtLOF+OZKVjIujBzOwAlKGdawJIsvACofXNEo071QoFAqF4khS55WjgGllkUVyIb//TjuufJj6eCDiRqpF6dfVyFIrjibEUROMHGe3kevzl2g3AodqEFVFuQmYJk5dJ7YS2XMKhUKhUNQkdf4KFJAmYYoLH8YHs3Q+fl/n5jsDZVa/DofPNIm32Wq0SvLRQoLDHrbzumFKthcUVsnl5peS5BqysikUCoVCURXqfDCHzzArtALt2Q133GSj36kGN99ZuVR6E4h31C0dVNcEMbpelH4fKSErW4yyGikUCoXiH6TOK0dew6wwbufruToet+ChaRVXby6O1b2+5jLE/k3EO2yWy7ISBKQkRterVQtKoVAoFIrqUudv0T2GWWEW2fwvNNqkmaR1qNzFvrZ6df0biNJ1NGHVi4oUE1mlopQKhUKhUNQkdVo5CvXo0kTZlp3cHFi8SOOq6yK/yIcwpSSmFnp1/RvQhCA1NppKGo+qXe5AoVAoFIrqUjev3EEi6er+wzcagYBgyLDylSMpJeHqVDvrcGuHcHWiFAqFQqE42qnTVy9Tygoz1b7+QqdhI8kJPcseKKXEbZiEpgs9Ehz2f6wOkUKhUCgUiqpRxy1H5b9/MBu+/UrjsrEG5RlBQt3SG8dE1ayACoVCoVAojjjKclSOYWfOxzo+n+CiS8t3qRmmJLEKvbEUCoVCoVAcfdRpy5FRgVvtg3d0MjqZdOp6aJDHMEpsIgGnphFVh2OLFAqFQqE4lqjzylFZyVGbNwqWL9OY8qi/qEikKSWaEDSMcpYYa9dEnUzXVygUCoXiWKROK0cBKRFl+NWW/WFZgk4/41AOmt+UxNltRNtUQ1SFQqFQKI5V6rQvyDRlma1DVq0QxMZJWrc95EQzsQKvFQqFQqFQHLvUaeXIkBKtDMvRyuUaHbvIoiw1GYxPqst1ixQKhUKhqAvU6Su9IcNbjgwD1qwUdO56yKUWkJJom46mYosUCoVCoTimqbPKkZTSUo7CvPfXZkFhgaDL8YeUI8NULjWFQqFQKOoCdVc5AqQkbJbZquXWa52PlyXGq3YYCoVCoVAc+9TZq315fdVWLdeIipK063BojADVCkShUCgUijpAHVaOCFsd2+OB7+Zbwdi2YoUOpFDKkUKhUCgUdYE6W+fILKM09tS7bGzZqPHOp76i12QwNklXupFCoVAoFMc8EVuOhBDRQoj2tSnMkcSUHNY6ZOGPGm+8YuOq6wIlij+aWPFGqgq2QqFQKBTHPhEpR0KIYcByYH7w+fFCiLm1KFetUzrmyDThgXtsNGthcuf9gcPGOsrqM6JQKBQKheKYIlLL0RSgF5ADIKVcDrSqFYmOEIZplqhxNPcTjdUrNG69J4DTWWqslDhUpppCoVAoFHWCSK/4fillbqnXyulnf/QTkJToq/bCNBsdMkzOv8g8bKwE7Eo5UigUCoWiThDpFX+NEOISQBdCtBNCPAcsrkW5ah1TSkKesr27YfVKjQsuNtDLqPOoK7eaQqFQKBR1gkiVo+uAjoAXeA/IBW6sJZmOCIY0i+xGP/9oHYZTTz/cagSAVGn8CoVCoVDUFSpM5RdC6MBXUsoBwN21L9KRIVCsOvbPP+g0bCTJ6Hy4p1BKCarGUd3C8INu/6elUCgUCsU/RIWWIymlAZhCiMQjIM8RwzCt2kWmCT//oHHKaSbhwookYBOaajhbVzD8sH0dBHwVj1UoFArFMUmkRSBdwCohxHdAQehFKeX15W0khBgCPAvowAwp5aOl3n8aGBB8GgM0klLWi1CmKiOlRCIRCFavFGQdEJx6uhF2rMpUq2P4vFCQA3lZ0OC4f1oahUKhUPwDRKocfRp8REzQHfcCMAjYCSwRQsyVUq4NjZFS3lRs/HVAt8qsUVWKN51d+oel+PQ9JXy8kSklDl0pR3UGvxc0HbIyoV4j62+FQqFQ1CkiUo6klG8JIRxAWvClDVJKfwWb9QI2Syn/AhBCzAbOAdaWMf5i4L5I5KkuxQtArl4paJAsaZJaxlhQlqO6hMcFdieYAdixHkQlP3ufx3LLgbVtSgtwRNW8nAqFQqGoNYQspzt90SAh+gNvAduw2rU2Ay6XUi4sZ5sLgCFSynHB55cBJ0oprw0ztgXwO9A0GONU+v0JwASAlJSU7rNnz65Q5nC4XC7i4uKQgN800RBcPak78XEBHntiRdhtTCR2TQvXo7ZGCcl2tFHn5PJ5sGyLgqqU8nJ5/MRFBYO5TRPsjqMiuPto/Rzh6JXtWJNrwIABy6SUPWpBJIXimCNSt9pTwGAp5QYAIUQa8D7QvYbkGAl8HE4xApBSvgq8CtCjRw/Zv3//Ki2yYMEC+vfvj8cw2FPoQTd1tv/tZMxEg069+4Tdxh0waBoXXetFIEOyHW3UKbmkCZuWQlRs5S1GIbnW/03/Di2sJwGf5ZZr0akGhawaR+vnCEevbEouhaLuEukVwB5SjACklBuBim6Hd2FZmEI0Db4WjpFYytYRQUrrsWWjwOsVdOwSPt4olMZvU5lqdYOAPxiMVkOKsG4HdwH4VeabQqFQ/JuI1HK0VAgxA3gn+HwUsLSCbZYA7YQQrbCUopHAJaUHCSE6APWB3yKUpdqYQaVn9UpL6enUJbz7xATsQiuqh6Q4xvF5arYpjgi65jwusDeowYkVCoVCUZtEqhxNAq4BQqn7i4AXy9tAShkQQlwLfIOVyj9TSrlGCDEVWCqlnBscOhKYLSMJfqohDClBwpqVGk6npE1aGcqRSuOvG0gTDu4DbwHUtCJss0P2HkvxSki2YpAUCoVCcVQTqXJkA56VUk6DojR9Z/mbgJRyHjCv1Gv3lno+JUIZaoxQX7U1KwUdOkpsZRwFQ0qcKo3/2Mfvhb1bQbeBM7pm53ZEg7cQ9uZYbruUFjU7v0KhUChqnEiv/D8Axa8a0cD3NS/OkcGQVgHIjes1OmSU0U+NYHVsZTk69vEHA6ej4ywFqSYRwgrwjk2EnL2WgqRQKBSKo5pIr/xRUkpX6Enw75jaEan2CZiSQAD27YXUZuUMVA1n6wZ+LzUbbBQGTbPWyM+q3XUUCoVCUW0ivU0uEEKcIKX8E0AI0QNw155YtYuBZN9uDSkFTZqWf1HUlG507ON1H5lK2M6YYOXtlOrHNgX8YAQiHy+lVXdJWUIVCoWiQiJVjm4EPhJCZAafHwdcVCsSHQFME/ZkWhen45qUoxwJVMPZuoC3sObdaeHQbeApBJ/bUpSqis8D29daTXIjxVsIWTuhYfOqr6tQKBR1hHKvCEKInsAOKeWSYMr9VcD5wHxg6xGQr1YwpMmeXZalILVCy5FSjo55vIXgqDC/oGYQAgrzI1eOCvOsYpIhJJC9y/ojJiHydbWDcHAvNGhyZBRBhUKh+BdTkY39FSD0y9wHuAurmexBghWr/21IKTGB3buClqPU8MqRlBJB5EFZin8pRsDqo3akGszaHZAXYdyREezvlrnl0GP3Fuv1ylqeBJbJ1JVdaZEVCoWirlHRLaQupQz9ml4EvCql/AT4RAixvFYlqyVCuWmZuwRx8ZKExPDjJFYwtioAeYwTOMLVq20OcOdbMUO2CorMF+RasUKxlbAQlYczGvZuh/wcqJ9iZdApFAqF4jAqMozoQoiQAnU68GOx9/6VtnkzWAAyc5co02pkjQNdRWMf+wR8UOtthYshhLXe36stq1B5cUO5+2u2aKTNDnY7FByEfGVBUigUirKoSMF5H/hZCHEAKzttEYAQoi2QW8uy1R7CcquVF4wtkejHgFNNSkmASmQ1BREIbOIo03+lWbLvmZSWciGE1ccMwDSsR6R4C2tWxkiIjrNkdOfBrs3QNK2kW88IWEpbQS7ExNfs2jYHIKyWJgqFQqEIS7lXPynlQ0KIH7Cy074t1uJDA66rbeFqg9Ae7N4lyi0AacpjowDkbrmbgxxEVME60kyWVwTqCON1Q+YmaNrhkDUlZy/s2Qa6Bqntrde3r6tcFpeUYK9kMLarwDI2xcZWbrsQQlhB0dHxlgKUtetQFlnAB9vWgOGz0u5rw62r28DtUqn9CoVCUQYVmgaklL+HeW1j7YhzZAj4YO8eOC617DESif1f7lZzSzcHOUiMjKl07JRf+tktdteSZJVEStj3NxTkWJlaKa2s1w/uhagYQMLODYdieCqTxVUZGebOg+degY2bITYGrpkA4y6HqGpkukXHQ9Zuq++aM8ZSlEx/7exDiNC5EPCBI6pqc/g81qNaclByP6UJPm/Nt3BRKBSKSnKU+U1qH4lk7x5RYQHIUED2vxG3dOORHg5yEJu0VSmo3C7sFMgC/Pg5aB4EwIaNeC0eU5oUUEC8OOTy8UgPblmyLqiWn1O5QoVAjIjBXtqdF/CD66ClQBzcCwkNgxdS96Gg4lCRw+peWNessyxDPU84ZFX5cwVMfdT6v2MHuOU6WL0WHn8G3vsQrr0Kuh8P8XE0/vY7GPshXHkpjBtd8XqaBjYb7N0GcfWs/atNxagICX5P1ZWj/Tsg70D1svxMA1p2PvTc7YIDu6B5etXnVCgUihqgDipHsDuSApDy31njyCM9bJPbMKWJhka0qLqyEE00BgaZ0qr9KYWkqdmUAgrIIoumsikNtAZ4pZetciumPOSm1Lw+EndtQ1byGPqw00A0QBel3D3RcZbFwxEFuzZaSlfxMZV1jYUjNw9GXmn936wpXHgubN0Gn30JDZPhyQdhxDmgBxWCxX9YStMd9xVN0QEgLtZSnM4cBE2Oq3hdZ4ylGLjzLeXuiJx3wnJVxtar/KZSBuOhEqrnlivMA2/BoeeeAss6aARULSaFQvGPUvd+gSTsCdY4OhYKQAZkgEyZiRksUuDGjS71ailFITShoaERK6zYGkMa7BQ7QUIssWSKTPLMPDx40GRJRcyZl49dj8KIqlw9Hjde9uJGx1JA7NipL+of0hfsTuuiHvDVvPvl1TcsxejOW+DnX2Da8+B0wnVXwdXjDo8x6nsizPsENm2B9RvB7WGlT9Klf284fRg8+AS8OC2ytaPjanZfKsJmtxSyquD3Wlaf6sYr6XZLQQpRkGtZCX1uy92oUCgU/xB1TjmSwL491pW2UeN/f181N27yZB5OLMuJAwe6qJ2ChrrQrfglrPpPMTIGP35s2EpktolAAHtONkYVlBcnTozgPwAPHpzSSawopmQ5o0Gz1ax1YVcmzHgbhg2FSWOtx+49Vup7clLZ22katG9nPYDs9X9D86bW9s+8aFmfBpxSc3LWFLrNylgrndknIggC97lrpvqB3QEFQeVImpblzO6wLEhKOVIoFP8gdU45AjiwT2CzSerVL3/cvyHmyCVd6OjYRQUFBWsIrZgrK2RZKo3uyre00CpaFkJWoxAHOUiUjCrpaqvJj+bX3+G6W605JxdLwjyucdXnvGYCfPUN3DEFbrkWtmyDm68FZw3WLaoOus1qY7Jp6aHXJNCgMTRqUf627vySLs2qoulWdXIpg9Yo07IMFuZB/Woce4VCoagmdS6PVyLJOiBIblj2DbIpJZqoSvL7kceFCztHRjGKFHt+Lqa9ZmTS0TEwyJE5RWUYapQv58NlE6BeIsz9AFq1rJl5nQ546iHYuw8m3wMvzYDnXq6ZuWuK+PpW3FDoER0LOfstK055FOTWbHFKaVquUrDqMBXkUTsftkKhUERGnbMcSQlZ+wXJDcv+8fWZJokO+1HfOsQv/fjwFcUEHQ0II4BeWIgRXY2u86Vw4sSFC13q1BOVaHkhZUkN2O+34oN27ASfH35aBJ/OtTLN3ngJEmrYlXN8F3j7VUtRmv0JvPAaDBkInTJqdp2aQtMtN5vXDVHBc8pTYFl1QkgJnsKaK04pNGvNvGwr0F0LPY8gEy4qrmaVNIVCoQhS55QjCCpHjcpP44+xHf2HxoPHEvYo0uE0T7D2TQ0qlgKBEye55BIn47BVFFPl88Hkuy3l55yz4MLzrEDrm26H/cWavsbHwaUXwV23QEzNKXMlOKWv9X/7trDwV0uuLz60YpmORoSwLENRsVbQ+/Z1IEvFJel6zX2+jmgI7IP8LMtyBVaw+J6/yt/ONCGuPjRtf4Sy+xQKRV3i6NcAahiJpRy1SwuvHBlSYhcajqM4GjsgA+yT+6wssWp4RnPcOeR58mhev3mNyWbLz0fWQof7kJPTIz3ElWcpMwy46kb4YQH06wMffApvv2+9l9YW7r3Dcp3ZdGjVAqIPBY17TC8uqt5WI5rDA9ANaeKWbuLq1YOH74Xx18OLM+CGSVVep1ZxOC2rTVITqyAlZu3WXdJ161G8ua4zGsIcy8NwHbQe8Q1qTTyFQlE3qZvK0QFBcsPwcRU+w6SB8+h2qblxkyWzcOAgiqoV8duWvY1R74wiqyCLly54iXox9SjwFtCvdb+qC2aa2Fx5mI7acXXYsOHCRRzlKEeffmEpRvfdCWMvsyxGc+dBTg6MHV2uhSiHHHz4qqhwykOWvGJ48ZJDDrEyFnHGQBh+Jjz1nPUYOQIem3p0WT5sDkvh2LbKSvWPrYQb80gTFQOZm606UcmpliVJoVAoaoA6pxy5XBJ3YdkxRxKIsR/dh6VQFmLDhlOUX/hQSsnmA5tpk9yGHHcOn678lK5NurLPtY8p86dgmAatGrRizOwxRdvcNfAuxvcZXyW5bIUFiEAAnFWsulwBOjpevPhlGVW3vT6Y9hx07ghjLrVeS0yAy0ZWOLchTbx4ceKsUh86sBQhWUo7cks3fvwE8FuB8w/fB53SYfU6Kw7p5L5W+YBiclTINz+gPfmcpcBPvg45eGBRnasSSIn46hvYuZvE5MbQvnlkilhMguW2ikmocHxF8mpotaf72RxWzFLAC9l7lHKkUChqjKNbC6gF9u21Ll7JDQ9/L2CaOHUN+1HejDPSDLWPV3zMbV/cVqQQ7c471CutQ6MOTD9/OinxKUxbMI0OKR1YtGURD3//MGv3ruW2027juIQIqjuHkBJ71v6asRoVunEuWoqRmoJtzSai5i/CtnYzvpO6s/+pm3DjJmb7DrjoYigshKapcNYZsOkv2LW7StYYL1bQcXVzFIsrR1JCIYVoaPgJWJ9ZQjxMHAuBAPy9A/77oOX+q1+PXDOPHHKKttf3ZRH90xKkzYZ74ImYifEgJcc9/hSyoBDNGY247V4O9GqN22mg5bkwo6KQCbGIXBdJ975A7JyfAOgGVvXpK0ZVvBOaHlFbEJ/0s1fuDa+YBYkhhmSSa09B0m2WrO48q+GwfpTGcikUin8VdU852mf9SoezHPmlJNlxdP+4GtLAg4cYyg8g9ga8PP3z07Rs0JLdebuJskfx4eUfsiNnBzbNxlkZZ6EHL4BThkwB4MKuF9Lq51a89ttr/PLXL/xy/S8RyaR53Gg+LzaPh0BM5JlzW3J38Nq6T1iXvYV2BU4u35ZA57HXU/+a+3H89r+icYGWqQTaNif6029I7NIO1xXn0fTjT6HQDVdeBstXwvSXrWawl15kWWMqiVu60atZ2UJHx8RX9NyHDxOrjYtHeogpXrXcZoPHp8KQ8+H1t/HfMokccg5Zrjxeki6+E9uW7QAYTRuT88IUtIO5ODZv5+C0O/C3b0XDYZOIH3sbSRv+RssvQAqBr2837Gs3I3JduCaPJeeSM4ia+ATJDzwG3bpA186lRa800h8gb+ty9KRYHIkNiPriJ8wGifhO7nFoDJJCCjkoD5Z7vkop8Uhv0TE8rLdeRYQ0L3eB1Z9OoVAoqkmdU47277f+TyqVrSaDdVWibbVTXbqm8OErqlBdFqY0efW3V9mdt5t3Ln2Hns17AuDQHUV/h0PXdCYPmEy31G6M+2AcC7cspAXlFwQUPh8xf28FwKhEWnVmwT4u+OYmcnwuutRrw43Pr6D7336MV39G9xvk3X89ZlJ9zCaN8HfLACmpN/5uEh56GX+9GOr/+CNccI6VaQaQl2/1NNM0DGniKdYE13JBHpLNK30EKOmaK6QQR0Cg+wqoDKbdjgzut6UcmbhkAQKBR3oQCHR0Kx4piE/68eOHDs1wntEf/c13yBk3DD3BXmS5invmTWxbtpPz0v2Y9RJInPwoDc67GjO5PkajJHxnn47HYeAadRbxs77Ae3ofvAN6o+3eT9RXP+Hv2A7XHRMJdGqHDcmqW6/n1BsmI/5zOf5Jl+O/+kqI0H0cRakCnJ/OhTunkOz2IO02jOMaYdtu9d8ruOpiXDdfCU4HAkH0rhzM//uRAn8A+6a/kdFR5I09H5l4qF2KHz/75D4kEjt2mohKWCxDaDq4spVypFAoaoQ6pxyFLEcNSylHJmAXGraj3KXmkR5LkStDN9pyYAvjPhjHtuxtnNrmVE5qdVKl1zilzSkkRiUyb908JjUoP6tKdxcCYFTCYlTgL2TU93eQ7y/kqzNfoMenfxL/9xqmnR5Nv902OlxwOe7Lzy+5kRDkPnUn9S+5meQbH7deG3PZofeL1SjKl/kc5GBRYLWOThOaoAmBKSX75f6i9iRF+4GG7vPhS26IjLAtiTACOA7sL1IKRfBfljxULiDU1sWLF0OamBglXFGOa87nuPk/4XjrQ4zrrrC2+WoBMa9+gPuiM/EOPRWArHkziH3mTWJmfY7rtvHgsOPERu69V+O97HyMtFaHju+t40rKiSCQkEjmR09Q/+EZxE57BU/uPg7+96oK91EiiSWGJJKtXoM//4KcfA/ebu1xjzwbx9q/sK9cT+5NV2BfuprYV97H+fXPuO68Cn+nNBqcdzX6gYPWXA47+APEv/MlOc9Pwd+3GwAavqLj5MGDXwYqbz1yREFellUG4DAENDguIlehQqFQQB1Ujg6ELEfJJV83THlUW4280luUvm8r52N7btFzHHAd4KlznuLM9DOrtJZdtzO4/WDmr5/P2F5jyx1rc+Vhhr0ghUdKye2/Pc36g1t5f9DjdBTJxD31Ot7T+5B3Y1dOXPYyX52ZTvdw29ZL4OD7T5N4zRSyHHHY2yYSa7qJ0Q65qwxpkk8+0UQfSv8P/oshGg8eDIzDs/xMEzQDX/2kyNueGAaOA/tLFJsUiDIzCA/IA/iDlr+iMZ074xl0EgnPzELUq4+WnUvss2/h796R/P9ee2jfE+Nx3XcdBbeMRcZGF61lt0eXUIzKQiCwpzbH9cJURMPpJLz+KaJRCkarpnhP7gHR4WXW9uxHrv0/Ctb8jfOXP3H8sRx/h1Zkv/4QekI9fCMOjfWcNxjvkFOIe/AF6k26DxnlRDodZH80HTMl2bIwbdhK4k0PUe+qezj40XMEOrQ+bE2v9FZeOdJ0K/7o4N7D3wv4rWDtqKOnWKpCoTi6qXPK0f59EJ8giSp1LTCkxKkfnVYjKSW75W4KZAF27DgI777ak7eHr9Z+xeieozm/y/lhx0TK0PShfLTiI/48+Ced6BR+kGFgc7kiroa9IWcbb636hO/WfMctfa6gf2pPol94F+H2kH/beEa3bsxzq9/jyeVv8v7gJ8LOIRPjyXnnKdZsyCMdH4Xsp5HZiGjN+kDd0o2JWSKw2oaNPJlHNNHkybywyqXm9+GPi69cPzhdtwLQTaPCJrgOHAQIoKEf1jsu75m7SZxwDwn/fQYAz+B+5D5zN8QcXutHxlW/WGX+nROxLV9H/CNWOxOjcTLui4ch42JwnzMQmWxlfcU+OYO4598p2s7ftjm5k6+kcNQw9IR6Yef2ndyD7K9eI/qDeUTP/pL8/16Lv2eXovcDndpx8K3HaXD+1dQbdxcHfni7xPY2bBRSWH65hrJwlJElWZhvFbSsypwKhaJOUueUowP7BUnh0vgF/3iWmiGNsJk/hbLQqu8j4sJsdYhZS2dhmAaX97y82rKc1PokkmKTePfvd7nEvASbVvJUCbUJsZ4c7uOTUmJKE58ZYGPONmas+5SPtnzDS18Jpm1wUPif4eAPEPP2Z3j7dcdo35pY4JrOl/DA0pf5bc8K+jTuWqZ8AoEdOxoa+9hH6LBJ5GHKow0bHjxsl1ZwczjLjjACGPGVL3ZoRMdgc+VjVqAcaYRv0gsgY2PIef0RouZ8j79nF4zWzSotR6VwOjj40XPof21H33OAuGkziXv6DQCi35nDwbefwL7+L+KefwfPsNMoHH0ugQ5tkPGxwX2pAJsN96jhuEcND/u22aQReY/dRv3LbyP602/hhFOL3gvFZ/llAA2tZKxTlZHg81Y8TKFQKILUOeVo/z5ISg6jHEmw/4NVsSWSTXJTWOXIxCRKRpXbJiSrIItZS2cxuP3gGql47dAdTDljCtd9eh0zfp/BxL4Ti97T8/OIztxpyR1ssyKl5IXVs/l19594DB8bcraR7c0t2sap2bmz+flMWPEFms+H/em3MFo2Rd97gLxHJxeNG9PhPF5b+zEPLH2ZWQMfISmqXrly6mEsMeGIIsqq+xMIgPSFGSEwoiKoylwKIzoGe15Opbc7jCgnnovOqv48kWK3YbRvjdG+Ndmn9ASPF/uqjdQbeycNT74YAH/HduQ+cTtElV9Pqyr4TumJv3MaMS+9h3i5H+JgLnHPvIln2Gn4eqSxW+5GR6cRjSrvYiuNbgOfu+JxCoVCEaQOKkeCFq1LKiBSSoQAvZYrFfukr6iezmHv4UOTGlGiDNdABaI9+dOTuP1ubj3t1mpKeYiz0s9kdtL7PL1gGu3iUhncZgDCMMjashxnTAL1Y6y2DQEzwD3/9zxvrv+cDvVakeCIZUjzk0iNTUEXGm0Sm9GrUWdaz/gKzefHe3ofYt6ZA4D3pBPwndqraM1om5M7TxjHDb88yvEfXsB/u1/FhI4X1sj+aF4P0mbHdB5+sQ9EJRYpepXBdDg5qprbVQUhIDoKf68uZH/2IlFf/4zUdTwXDClXMdqat5Ptrj3YNRtp9VqSXIEiW3rNgmsuo97E/3LCHfeQkLUffdde7H+uwT/3FRACPz72y/3Uox46eomMw0qh28BbWLVtFQpFnaTOKUdZBwQn9Cr5miElDl2r1ZYhPuljq9xKQAbKLDToqMKPf6jy9Qf/+4CxvcfSJrlNdUUtwlZYwOSWV/PA1qlcNfdGLms9BK/h54O/fyDGFsX49BEkR9dn1oYvWJ+zlas7jeS/3a8Kexy1vQeInvU53lN7kTvtLuqPvhXfSd2ttO9S7syL2g6hS1Ia9y95iQeXvcrgZn1pmZBa7f0Rhom7WSpm6YCzamA6HFZA9jGC0bYFBdeNxh3wEqU7wp6peT4XDy59hXc2fYVZrEL2cTHJtIhPZWveThIcsfyn7RDGpp9PjC388fYOPomCSZdg++Y3zMR4vAP7EvPWZ9iXrMLfqwt2HPjxcUAeQCBoQpOqudk0G/g8FY9TKBSKIHVOOcrPtwKyi2NIiKlimq9f+smSWYe1jWggGuAUTvLMPAoosBqaSogto2lqVSoz+w0/w2cMZ0fODk5oegLXnXxdlfahLOy5B4mzx/Ph0Ge48ZfH+Hj7AjwBL1d2OJedrr08vXIWAM3jjuO1/lM4u8WpYRUj+7LV1JtwD7g9FNxwOTIxnuw5L5e7dnr91jzT73ZO+vQy7l3yAm+f/nC19kX4fBixMTWqGAFFQdl6oQsQYJrohQVIXcespTYqtc1Pu/6PMT/dS7/G3Zh+8p3Udx6KxfIaPi7/4W7+b98qxnY4j7Nbnoo74GXdwb9Yk72ZbfmZnNykOztde3ho2at8suU7Xus/hXb1wtTL0jRct09g1bkX07l9PLg9RH3+PTEzPya3lxXEbQ/Gj3mkG9+W9USnNCtRtiEiNA3MABiBCgPnj1bcfjf3/HgPF9aQFVWhUJTPv/OXooqYJnjcgtIleUyqnqnmwcM+ua9EELCJiQsXyTKZXezCJm1oaBX2QqssCzYvYEfODp4+92nO7Xxujc4tAgFsLhcIjTh7DDMG3I8pTTyGr8gSkOXJIWAaJEUlHhawXYQ/QMLND2PGxpDzwbMYbcsvKlmcxjHJ3Hz85Tyw9GVu+vVxHjrx+jKtECVk9/nQDH/J16SksHH5xQV///t3Hv/hcbILs+napCt3DLwjohYqnibNEIZVVFLu3oG7aVOid24vkeJ/pFmVtYkPN8+nX+NuDG7QkeJf9b/zM/lw8zdsyt1e9Np+dzabcrfTOCaJDTnbSI1txILMJZw2Zwz3dL+K1LgUVmdv5uu/F/Hb3hW8eMo9nN96YNH2/VMPLy66YNcSrl74IGd8eRVP9b2V81qfXr7Q0VEUXnoOcS+8g/HIKwiPB8fCJeQ+fTdJn80n+i3LFUuXTlbT3ksurER2obBS+o8y5cgfbMFSuihpcepTn4MFB5n2+zRaJrekM9WvcK5QKMqnVn8phBBDgGcBHZghpXw0zJj/AFOwer6ukFJeUlvyeL2WdeiweoUSbFUMxnZLNzZsh8UKuaWbHWIHMTIGW3UDSsvgkxWfkBybzFkZNRzIa5roBa7DXtaEVkI5qShYGiD6g6+w/Z3JwZmPVEoxCjEx40LyfS6eWfkO6w7+xfuDHi9hyTgMKdH8ftzNmiOL1V+SQiDL6PsmpeTZhc/y7MJnaZLQhC5NuvDDph/4c9efvHvpuxUGuJtOJzsO7qNJYhOkpmHExhGIi0PzeMtcs6qsO/gX7278ihxfPs+edDu6prPffZB7/+95WsQfx2XthzNn64889ufr+MwAr637hIbOevRI6Il3ex6rszexz52NQNA6oalV2BFIcMQxqFkf9hTsp3XzZjzR52a25u/itt+mcc2ih4rWT4qqx4MnXl9CMSqL/qk9+X74a0z8eSqTFj7Ay2s+ZEBqTxy6g+Et+9M28fDjWnDdZVatp1feR+oaZv1EGlx4HcIfIH/kEGJS26J9/R3irvsxAwG4/NDPhVU5vixppJXO76x80H1NIaUsYWE2Mdkpd+KW7jJ7JZqY7BP7yCnIASAxJhHyj4S0CkXdptaUIyGEDrwADAJ2AkuEEHOllGuLjWkH3AmcJKU8KIRoVFvyALjd1l1mTEypGBFBlStju3GHrZsTLaKJklG1FseUVZDFD5t+4IpeV2CPsNlmqEVKWTKZ0sTu8RC942+EaWI4nEAVYjWkJP6+Z3EsWoqWlYOvRyd8A3qXu0moYrWGVsLFqGs6d5wwjq5J7bnq56mcP/9G5gydXrSOMEpWutZ8Xvz162PEll/2IMSm/Zt47bfX+GjFR4zoMoIHz3yQKHsUKzJXcMV7V/Cft/7DGxe/wbIdy5BILu1+adHxM6XJnFVzmPH7DNbuXctlPS7j0thLAQjEJRDlysSIUDnK9uTy4urZJDrj2VeYxY+7/o8HT7yOAamHAuR+2vV/XPb9nUgkhjQ5+bgTyKjfhst+uJMsTw5+M8AzK626RKelnsjTJ93GysyVvLfzJxZsX0SrhCb0P647nRu048xmJ9E0NsU6jLoe1sJ1vLMD8898ke+2L0YXGp0btCUlOsna/0DQUqZp5VpvmsQ24pMhz/DWhrm8v/GrIvmmLX+L81sPpFdKZ1r5uwFBV1mUk/xHbsE79BSMJo2QcTHUG383gfatyXn4Rg5qAq49m0aj7sD+1LPsPrsrZoNEADQEDWlUduB2IFyW4pHBK71slVsPz0aVECPKr11VQAE7C6zs0OTY5HLHKhSKmqE2LUe9gM1Syr8AhBCzgXOAtcXGjAdekFIeBJBS7qtFefB4DrccmVKiC1HlTDU37jKLMtaGYuQzfNzxxR0s2LyAgBnggq4XRLxtIYVIIYmRMWilAlv90o9HeIjPOYAUGkZs6Ae78spR1IfziHn7c3zdMpBOJ/n3XFOue8mDp+jO2Ys3bB2ioS1O5q3TH+Li725jxrpPGRR1HrrHjWl3lLg4G7Gx+JIaViijN+Dl/m/u5/0/38em2RjfZzx3nH5H0XHp2qQrs0fP5tJ3LuXMVw9VGt9yYAudj+vMmj1r+G3bb6zft572jdrTv21/Zi2dReeunUkjLeLCmADugJfLf7ybpfvWWHWaNDuJjjiuXvgg3w17jaZxKazK2sS4n+6jfb2WfHDGU4z67vYi65BTdzD/7JfxGn4WZC5hYNPedG7QDiEEQ5r0on/PEaxbspOurcO4JKVEdxdgxIRXJh1uD0Oan1Tm56e7C60SCHrZMXt2zca49PMZl24VJt3vPsgTy9/gs79+4MMt3xCnx3KruIIrO5yLI6jo+0455KbL/uJVS5bQCwJc911P0pnjSJr6KnnT7rJ66mGwX+6zWp2UjuETBrLwANJulXUQdidUohdgdcmTeZjSPFwRiuAnQpMaWwus/oUpMSnkK9ORQlHr1KZylArsKPZ8J3BiqTFpAEKIX7Fcb1OklPNrS6CQWy06+pDlyJASZxWtRgEZsLqu10ihush49PtH+WzVZ5zX+TwGtx9M+0btI9ou1I8tiSSyyEKTJWWWQtJSNsOdv51Cp46Gr0xTf3noW3eScO+zePt1J+etx8u8aEokPnxIJE6cNBQNEQgOyAN48YZVOAek9mJQ0z68tvZj+nUdgpCxuJs2R9ojl/PbDd9y/zf3U+AtINeTy/g+45nQZ0LYO/L2jdrz4eUfMm3BNIZ3Gs7ibYt58//eBCDKFkV6SjpPDn+S87qchzfgZegrQ3l83eMkpyczoN0ADJuNFTv+pFV8KvWdh4KIV2Rv5rHVs4i2RTGy/dk8t+o9lu5bw2v9p9DvuBPQhOCAJ4fBX0zgih/v5qETr2fCgvup54zn3UGPkRxVj7u6j+eibydTzxHPZ0OeKQp4PqFh+mH7YdodSF2nsEWYNiOmSdyWDcH2KSXPCeH3Y0RHU9jy8BYfIWw52UTt3VOp3noNo+vzeJ+beaz3TazO3swdC1/kviUv8NaGOTx/8l2c0DCjwjmM9q0puG40cc+8CZpO3hO3oetW4999Ye6xhM1Az9kHuetJIomoqPrQ8sjE7kgpOcjBMm+iKsKBg8xCq7FvcmyyUo4UiiOAkLWUhiyEuAAYIqUcF3x+GXCilPLaYmO+BPzAf4CmwEKgs5Qyp9RcE4AJACkpKd1nz55dJZmWLbMxeXI/Hnx4BSeemA1YwdhVtRyZmFZ9ooprBleIx+UhKi58sLEhDZ7Z+Ax73HtYmbuSc1PPZVLb8hvChige4yAQOHBgYByWXaehoZkSfB5MzTouEonHYxIdFXkmX6eHHiP5/5ayeOYr+JIalCuXQKAh0NAP3UFLyj2ma/PXc8PqW7kgZQRjW16KFhW5hcaUJhOWTsBn+OhWvxv9GvajZ4PDA4nLlFlK1uevJ9YWS2p0KrooeVw25W/i4TUPk+nNpIGjAXZhY693Hy1jWjCt8+PE2eL4NPNzXtk6gwRbPD7Tj9t0E6fHMqnleAY3Khmw/H8Hl/LQpscpNNzE6rE80+kxWsZYSpA0DebumUdGQgfaxbUrZ6dNzKiocs8v4fcjTANKK/mmibTbLbdbOWheL0KaSETl2q8EKXQHWOVZzvNbX6IgUMgznR+neXQEVcKlpNW7s2kz6z3WXzORncMrjr2TSKvytgQcMeVaNF0uF3FxkblnK1rTi7davxOv//U6n+78lPknz8dd4K6SXAMGDFgmpexRZSEUijpEbVqOdgHFf+GaBl8rzk7gDymlH9gqhNgItAOWFB8kpXwVeBWgR48esn///lUS6M8/lwOQ3q0DnXpbyoE7YJAS4ySmCgUAc8wcdsldZabnV4aNCzeSdkpa2Pe+3/g93y78loyUDP7T6j88cOYDOPTI7kILKLC6sssALUQLErXEwwf53JC9G9z5YBjgjMYtrSy8TRv8Vpp1BNhWbyRp4S+4rruM9n3LD7724KE+9UnQSs4tJeyQO3DgCFveoDM9+Sq3Lx/v+IQf836kW9MTaJLYBIfu4Pwu59O+UXtWZq6kQUwDmtVvVsKq9/W6r9lRuIPp501nWKdhEe1TadpTtqUujTRaxrZkVeIqlmxfQq47l1GpXZm+cDpTtj9E8/rN+XLrlwzpMITHhj2G35XDb0s/5/TWp9Igul6YfR3AgIx2PPLnDMamn0/vlE5FB0l3F5J+4iSi9mRaLrwwF3kR8INmWYzKO7/0wkKid2wraf2REpu7EFfrthVa5oTPh+714Ny3x2qjUoEyVZpVG/IZd/wABnVsz1lfXcNt6+/i5ONOoFODdvRO6UKPRh3L3njqeHwbVtPugw+of805YfvRFcfExMAg1R2HOK4tJJYdw7NgwQKq+luTbWbjxqrK7cOHR3qIFtUIBs+F+gfrc2L/E1n689Iqy6VQKCKjNpWjJUA7IUQrLKVoJFA6E+1z4GLgDSFEMpab7a/aEqgo5qj4TZcAWxXdYm7cEbWuqC7vLXuPRnGN+Hzs5xEHX4NVeDJGxNBINCKTzPBKnJSwZyu4XWCzFzXvrLQLQEriHn4Zs14CheMvimiTcFl8QoBuamjugqJMqtK81fsOZsT8H3/aV7FqzxqW7VxGoa+QmX/MpEFsA7IKsgBIik3ivM7nMb73eOKj4pm+cDotG7TkzIwzw85bE9g1OyO7jWRkt5FFrzWv15yHv3+Y7TnbubT7pdw35D6r9EFUAiOOvwBbfl6YpjEWrROa8lr/KSVeE4aB6XQSSKyHeTALYRhhq3uLQAB/YsWKrREdjbTb0d0Fh16UEIiNi8hlKR0OAg4HemEBtvw8DF3Di9fKHgtaKyOhRXwTZg96nKdWvMWSfWv4fOuPADzVdzKj0s4Ov5EQuG4bT4MLriPmjU8ovObSctfQ0PDhw2/TcORnlascVRWP9LCb3djkoc8kXBxdZcguzKZ+TP0q1UNTKBSVp9aUIyllQAhxLfANVjzRTCnlGiHEVGCplHJu8L3BQoi1gAHcKqXMqi2ZipSjmCIZq5XGX0hhrStHO3N2smDzAq49+dpKZaWFYnqOE8cRK2JpTevD3EAA5GdDYS7E1ivxsi40omQUksgyfKI+nIdz8Z/kPXATMiEyk79e2s0gJUgTp8ePJy4Go2HZVbF77XVw6alji57nunN54dcX2HFwB0PTh1LoL2TBpgW8+X9v8tmqz2hWrxkb9m3gpQtfQq9iwc+qck7nczin8zlh3/PXq489N6dS8wm/D1+DJGv7uHgc2VnhlSPTwIyOwFohBAUtWh+W+VfZdipGTCz2nIN48ZJAAnEiliyZjQ9fxApSp6R2vHHagwAc9OZx9cIHue23aUTbojiv1elhkxz8PTrjGdyPuGkzMZseh+ec8uspCQQem4nuOgiGD8o5HwxplPleWeyRe7BJW43WNQspRzXhwlcoFBVTq3WOpJTzgHmlXru32N8SuDn4qHU8HuuHJTqYym8Cdk0r00JRHoY08OIlmpqrm+IzfHy34TsKfNYdfIGvgE9XfIoQgou6RWaNgaBFS+jEE08slrUorGLk88DereAM7xaMJRZJbtj3QkTPmoNj8TIci5bi63087lGRuauseKNSP/SFeWBzIGLr4UpJIMpWzgW11GeWGJ3IXQPvKvHayG4j2bR/E1d/fDWrd6/m2fOe5YwOZ0Qk35HCdEZh2u2Vqt4spMQMZsOZMbGIrANhJjaRmk4g0kBp/f/bu+84ucrq8eOfM31me8tmNz0hEEKkGSAgQqgSQBBRiChWFGkCwk+aX4qiSBEFG6JBiqggxQBfv1QDIkgnEAIJSSAhdXez2Wybeuc+vz/uzO7s7mybbCNz3nnNK7N37tx59s7szJmnnOPud25Rl8MbuyP9QlrS58aDhRsfxVKMW1xUUkmdqSNJctBfJMr8xfxh/tV88YmLOPvf13LXisXMn7Afe5Tvwl4Vu1EdqujYt+Xmyyn91mUUX3At3jffpe17p2PKS7Me14OHJmmmzYQJN71CrLws635RoqwwKwbVZqDPTPi52hbexqzxs7TnSKkRMrbSxQ6z7j1HSdsQ9OTWi5BeaTVUy/WXbV/Gmbedydpta7tsn14xnes/ez0TSgZWW8wYgxHDDJnRc9gqmYBoRgHO+rVOkOHJ3iPVX6FPV30jRT/+NXZpMdbsVAX3QUzIdWV+WNq28w1++t5AE0mzZcDH6cvMqpk8esajbG3fysTSiUNyzCElQqK0HP/WepLBAfw5phZQpMugdBTR7ZaN2x2LEiuvGPT8n4FImiRRiXYE3h18HtyeACXJUtxe53XgETfllFNv6nvksBqIQm+IR4/9FXevfJRF7z3E9W/e0XHbF2d8hi/vehyeVOBfdsu57HXrI4TufpjgXQ+RnD6J1qu/R/zTXecgu1P/CPgoq28m4i3GKuqZWNSFq+fvOBDDEL9sC2+jvKBcgyOlRkh+Bkep97ukyb1sSMzEnJzeQ/BelbST3LjyRrwBL4sWLupYnu8WN9VF1YMKwCwsggSzZ+Xe3gB1azt7KNxu8Pe+2svVzzf94J8fgYRF0323kJw28MAjvVKtSxHRRBQKy8Hlwmu8MISLKAPewIADI2NMxzDQcBYizpQsLIT6LbjD7f3vbAx2IIBJPYfG7cH2+537ZrRXAKukNPc2mSTRXnJcGTFMYlL2yf1FQGsjmVkgAgQIEaSdcMeHuyf1byA8Lg/f3P0kvrn7SbQlwizftprHP3qBRe89xN/XPNFl3+Dufo6/dAqnrw5x+CuNlHz3f2i671asOVlW9LlcWIEA/rrNWAWFOa20GwmJZIKWaIsOqyk1gvIsOHIhYuioPSrOsFou2mgb8Jt7f55a+RR10Tp+d/zvOHzm4Tt0rDhxxjM++41t2yBQMODkd25xvumng5kuojFC9y4mfsSBgwqMwFkx1OPcWRYUO0v/PXiG5dv3QCRIkJQkERMhxMDTBOwI2+cnWjvRWU4/kP27FbSNVdfginUNZGyPF5NjkkPb2EQkwjjGZX2Ne/BQ7OqlhEvpONhe32WYUATKKCdgOoegm2nGwhr031ChN8QB1XtyQPWenDH7ZN7fvrbjtvrINt5pXMWybas5ObCcyokW79wVovQ7V7D12XvBl6WH1O1BYjHc0cigcjX1JZ6Ms7l5MxNLJ+J2uWkKN3Hjkhvxur2cstcp7FHTc/XdpmYnj9G4onE96hQ2hZsANDhSagTlVXAUibgJFWR8wc6YjG2MYZvZRrmUD6jHoJ32IQuO7njlDqoD1Ry121E7fjDpZb5D0oJoOwQHV9FcEGzsHvNFAoufxtW4nfZvDjxDd5qNjZ+MyarpXFsBZyK3G/eoDR8kSFBDDdtlO1ET7aiZZ4whTLjP14aY3NtsFWfphRmgZDA0qIzcaWETzhqE2mJTTTVVrv4zjfcQKIAJu8Km98k8uIeOAiEA+E2QRrY6GbdsSEacAMCIi2SgM/gzGLx4s85XmlAwjgkFXSsOnbrLMYBTjuVnby7itLpHePzeMJ5HnsL6QvZVisblxtPSknNwFE1EWVG/gs0tm9ke2c5tL97GR00fEfAEmFU9iy0tW9gWdvKq3f3q3Ry7+7FcftTlTCiZwHt173Hd09fx/AfPAzCpdBI3nXgT+0/uLBvTGHbWqJSHdFhNqZGSV8FRLOYmkPryaoxxptukPuwsLOqpJ0iw3x6DhElgYQ3JapSn33+aVz96lTNnnLnDq6iSJolb3F0Dj7RYODUMOLg3Vyc46jaZ1hhCdzxAYtYMEgfuM+h2GkzXwDKZgGBBx9ynoQo6B90u48whK5ZiCilkFauwjZMBPUqUUimlXHpPbLmBDT2Sa45VlrHwiY8J0nMumyDZX0MDVVQG0/Z05pH1wgdUpv6OPJuXUjZlbzDgXr8SQ6gjIWXMxGiiadCTucsDJVw/70Ku9xXx7hP3Ir+8hbaj92ZycW2PfW2fD2/LdmLjqgc9tLaqYRVfvPOLNEc7Fy7sWrUrVx9zNeub1rN8y3JqS2q5/dTbmVw6mTtfuZPbXryN/3z4H7649xe559V7CHlDXLLfmRQXV3Hb63dx6l2nssf4PZhSNoVIIsIRuzqr73Qpv1IjJ6+Co0jURajA+fBKGvC5XR09ARYWMWI0mkbcuPnIfES8r2XsQzDf6O1Nb/O9h77HJ2o+wbE1O557J0GCIoqy925E2gYdGIHzQZnE7lJIxPfC63hXfuhMwM4xs7hXMo6YtCDYOUzjFjdipCMwGU7GmI5ioHHiFFHUMV+rylRRTz0BEwCBcTKua7u7qaWWNazpCLIG8tgjNa+puzhxqqjascSEffH1f1xv6iLiIRhM5RsqbIJoK6QK9rpx0+SUXhw0EeHST36bVV9t5uCbH+PG87/O7NPOYO5Rp3Td0eUCY/A2N5Eoq8h+sCzC8TBnP3A2HreH337ht0wpm4LH5WF65XQ8Lg/utlaCmzeCsaEVaN3IFTVHcdpn5vCtF65j0UuLOLJmLr/Z/yIq3CHilVWc8MlTuee1e3huzXOsqF/REWBBqudolF4vSuWbvAqOolE36dGHpDEEM8b2kyTxGi/N0kzERDDG9L0cdwffo2xjc9HiiygPlbNo4SKa3sjtA6BDMgnxdopdpSBZJva2NoI3t96A7t9WQ4seIFlZRvSzuc+P6tITYCc7kk+m+fA5deuGcY6FbWzapb0jQZ8HDxXS+eFYIRWECZMgwXjG9xkYgTOcmU4y2F/Pi2UsWmihzJSNygeeEUOh7HhpjCFXVObk3koFVx5x4zXenFIBpM389rm0Pfku/2/JB7Dkt6w6fSXFV14G3oy//0AQf90Wkm4PT215jRv+dQN12+vYa8NeHDf7OI6bfRwFvs73g5gV4/yHz2fN1jX8+ZRFHDzlwM4HjFtIMkpw43qSfn+PFA2Tg7vw6PG/5Y2t7zGvek9c4iJpWbgjYQqrazjrU2dx1qec8kAXPHwBi99ZDEBVQQ7DnEqpnORXcBRxE+rIcdR1pVo6Z4vXeJ1iqEOYwC2b59c8z+qtq7n5xJupKqyiiR0Ljvx1m/G1NBB0xSBbTiNjINTLJNo+dA+MvG8sx7/kJdou/hYEcg+2ugQ9toFuOY18+AgTHpYhtvSwqCUWtdRS4creW+AWN1Nl6qCO7cHjDLn2ExzFcbKXW8bKqcDvjkj3yO3Q0NlwyTInroACmmnOPeFqMED7Y3dQv2UTL114Bqfd8wyt65po//1PuW/DErbHW5lTPpMZhTVct/gi7l/7DNMrpjOvYh4rm1ZyyaOXcMMzN3DeIecxvWI6DW0N/H3p33lp3Uv8bO65HM14WPdhtwc12F5fr7mrAh4/B43fu3Nvtxt3NNyjAPAX9vpCR3BUFsyej0kpNfTyKziKuilKTxnplhk7buLOXIthDorSFr28iHGF4zhuj/6LZfbLtnG3NhMPFeJ2lQ3pSq/0ajXAKRFy4x+xK0oJf+PknI6XXvnWJegR6fEhUkABLbQM+Qe4bWziEqeSSnz4KJXSIT2+C2eotr8hQSOGQgppoWVQwZFt7I6aXUGCfT5G3MRJkHDuh027cXoUkySpoGLYhyxz4vWDPwht2zvmHQVJ0GbasEyYpN8LHm9Or4uC8bVU//oWzr36TG597A3Wf+5LXHtCE1szOogF4cKDzuGs+efz4QsfMvPTM3lt/WvcuORGrn786o79Qt4QNy34CV8r3junyfA9pHoPXYl4l9WIB049kNriWsKJMH7XGAxmldpJ5Vlw5GJcqucIAW/Gh0Oc+LCXAmmJtvDjJ3/Mss3LWFm/kovmXzTgArJ9cUcjJLEISVkuU4Cya2uHi65galkFritOxRT4CTy2BN9/36TlynMxBbl9IMSJU0xx1xxHmB6JKIulmC1sGfJ5RxEiVFNNpWvoa2qllVJKCy291tOyjDOZv0RKaDZ9ZyDvLj0x3MamzbT1maE9IQnGy3g8eNjCFia6OlMuDGVm9yFXswvEO5OVeo2hhIkYO4m7bj0tkiThTuTU4zanYibtP/wl5xRewS/ub+L9RUEab/8Rr9fAu01r2K94OvvsfhiJVKkeEWG/yftx31fvY/mW5UQSEUqDpUyrmEZwezNsrc/6OK7N9QQeW4JrW+fzm6wdhzV7F6xZ03v9+3HFuwZHbpebiw67iBUNK3QZv1IjKL+Co5ibUAhsY3AhuDN7jogP65vP8s3LOfvBs9m4fSOH7nIou1fvzulzT9/h4yZMgmR4Ky4xHcvOd/ygCTjrQnj+RabaNskn/o/Y/P0J/uNp4vvMJvLlE3I6rEn9K4gkIZjsWtOqW904j3goN+U00TRkH+SWsfCKlzIZ3uGJEimhiSbChLPebotNLbVO8JRDMJsOjlpMS6/7pCd7l1KKW5yM0CWSe7qAERUIOZcUAQpJzbfxlOLZ+B4NbEUkgAkM/rVxQPWezLjqHp47+lmOvOIvFP3gVooev4P5E/ZDLAtamkmUdw2eRYQ5NXO6bPNub0LWbiLw/lo8y1c5l5UfItEY0h5BjMGkcyvZNmI5Q/dGBFMQxAT8xI4+GGuPmRi/j/jRB+GKRKCoGFcsire5iXh5FZ/f8/PETGxEilwrpRz5FRxF3ARDJmtm7DjxYZn7YYzhr2/8lWueuIbyUDl/+9rfmDtpbv937OVYmZIkiRNjeosPj38ynqEIIpq2wwWXwHP/gRt+zBu+Aub882GCi58hesCeNNx+JcZvQy/Zk/tTaArwWjYk4s7wiUkt+8tS5qJMymiksSNFwY5KJ8gcimP1pUAK2IVd+twnXYjVjXvAvWPp0jB+/M5QZx+BVYIEIULD/ruOuKIKfNPn4rE2E9/0Hv5YFNvXy3BTH92olYFSKg/7HC0/GU/ZNy4ldNdDhL99KsbjwRNuR+Kplaq9pCMIPvlvyi+4Gs+qdQAYnxdrt2nEDp+HKSrALishesLhJCfVOHcwBtfmBrzvrcHz7mpcza246rcRfOhJ5C+PAtB+5qm0/eDb+OtsJ/AyBlc0SmTCZIzLaM+RUiMov4KjqItQgbNSrSgjODLGYGENuHL4QEUSES577DIWv7OYQ2Ycws0n3kxFwcCXCmeKmRiWWF3eIAVhYryKoNXuBBq5Stfl2rQZvnA61DfAT66EhSfTsmId3pOOwW5pxlcQYsIO1upy2Tb4os7yfXA+fDzejvklmfzip8bUsEk2OcvpU3L5Bp0OLEZqhdZA564VmALaaHNKpnTjwtUlaEqSJECgI9WAz/iwjJW1VIyFRQW5vdbGPF+AMu8E1kxsxrdhK+5opOc+to3t82O8fX/hiR82j9jh8yi8aRG+518jetx8EscdQsHa1biiFoWrexaeDd7xIAU33kFy4nharruY8D4zic+YiHR7LFcshivagklHsaU+2g7/JBy5X8c+0h5B2sKU3HQnBX/4O5EDdod5c0kGnWy17kiY0Pp1xMeV4g7phGylRkqeBUduAiELk8pxlGZhIciQL6le9NIiFr+zmO/P/z7nHHzODs2dsbCYJJMolm4rzqJ1O9bINR/CSafBvP3gw3WwvRn+fjfss1fHLiLgLhmiIRk77gRD8Vjq5yR4ex8OLHeVY9s2zeLM3TAY2hlADbJu0j0pvn6K6Y60UkqJS/Z8Wu20U2AKOl43cZyJ5GmFFLKd7VlX9Bkxw5fDaAzwiY8iXyXbpgWyDie7w+2EPlqL5fH0m4ur5acXUfiLP+F9fTkll9yI9bu/YI+rYFZlDZ7jD8T/7Cu4124AQGIJvG+9R/T4w2i+6VJMwEeCBLVS01EAF3ByG9mtMHXPziHC+nXQVAehjBV5RanL1T+G/y6j8ptXY9VWEVswn8jC40jOnIrE4xSuX49rejVjcYGhUjujvAmOEgmwLBehECDg6faNfKgzGxtjeOCtB5g3ZR7nffq8HT4W0ssk2pZt0NuwQn+SSbj4h87//34B4gm4+/ddAqMhl0xAUQUk0sGRBd6+S5pUuio7ggJjDHWmjg1scMpfDJCFxTjG9b/jCCtyFVFE9t9/q72VLWzBZZzXapJkl9xbhVJIg2noMdwKzhyZMblUfwiVizMnzTZZhr6Cgl3kJ9DeDsG+ewvt8VW0XP8DMAb/P58jeN//IrE4Nc88i/ufT2AXBLHm7AouFybkpu2cr9D+/W+A202MKGWUdQZGdhJiEbASUDGhy9wpKmqhuaFL3bkORYXw0L2Yfz6F9cJzhO5+mNCdDxL+2udpv/Ab4LbxxpMaHCk1QvImOAqnPkeDBabHMv4kSacu1hB2HL26/lXWNa3b4cAInA/2IMGeSQiTCYi0DLpeWoc77oHX34Rf/gwOPtCZb7RblurlQ8k2ECqB5q2pn3smgOyLiFBNNV68VMnAk+IJQpHkeJ5GSYVU4MHTsRxfkC4r4EKEqJXarIG9D9/ON9+omyBBJjCBpGQv2GvGFZFY9ybeRKLf4TUARIgdN5/YcfMBePfNOvaKbCKx9ywI9fxikk722TFUa2wIt0J5jZOSoKTbiki3F8ZPgw3vOznHupcqGV+NfPMryDdOYUPjSip+fi+hPz1I4LElbL/6TNyndJ0QrpQaPnkTHLWnRmKCQYPX5cKV0dVuYQ1Zz9GSVUv49X9+jWVbFPgKWLD7gh0+ZnoicQ+R1C+Vy3Dg0rfhZzfDZ46Akz7rHGPcCGTgFZz5UV6f02Nlm0Fn7hYR3LhzK4z6MSIifeZhcoubShm+lARjnYj0WesOP3wwsRnPR+twW4msu9gmSQIr65+QFfARnTUViEKk6wIEA/jwUEEhLmlLHSwJlROhalLvbSqqgOqpUL82e7JWDH5vgFD5BBp/ci6hU46l+JIbKLv4ZiJHHA8Vw/zlRSkF5GFw5A/2XKkWM7EhKeiYSCa45olrqG+rJ5KIcNq+pxHyDTwfUJtpyzovSUS6ljJJxGHLB073fS8ZePtU1wDnXATV4+DGH+cWXOXK4ARGgQKItjvBUi6/g1IDUBqopX6KRYGdfa5Z2ISpkEpC0vPv1L3pbUon79nrsb148HTpzZWuw2i9KRuf6u3N8oXMSkDDR5SEhbA/SXyvXWm98jzKT7sQ7z+fgpnz+j++UmqH5c2nUmdwRJfgyBhDM807tFItmoiyZusaXv7oZdY1rWPRwkXMGjeL8lAf32q7MTh5iibL5B63CdJ1SC0ehvbt4A+BZ5C5jTZshC99Exq3wd/+BKWlg7v/jrCTTiDk9oK/ANpSJVM8Y2uStNp5FEohm/weEqbn30mCBG4ppVwmZ/1S4nJ5CIWGYZ6aSN/zoApK8DRvpbT+XVrizST2/wTW+Ercjz4J51w69O1RSvWQd8FRqMAZVkuLEiVOvO8is/246vGruH/p/QDMnTSXw3Y5bNAr3wyGIooGtpoqHnWWvnsGmZfJGDj7+9DUDH+5A/bu/VvxkEtaziTsQOo8+4POzzW77FgaAqX64BMfZaaMtvTQVze1Ujv2yqi43FBWTUFhKdsbX4VtTUQWHEzhnx+Duk2j3Tql8kLeBEfpCdmFBV0nY7eaVmc1UI4jS6sbVvPAWw9w3OzjmDVuFgt2X5BzSoABB2jR9qxJE/v1xDOwdBncdC3sO4wr0rqLp/IaBQqhODVHJlAANTOhPMtcKqWG0ATXhNFuQk5cXj9l1XuzvmQlyZOPpOhP/4AHHoC9c0siq5QauLwJjtI9R0WFgicVvBhj2Ma2nIfUjDH8/NmfE/QGueaYa3JO8JjWWy2uHqLtPcpt9CuZhBt+CbtMh8/nVv4jK2Oc9mRZTp7aARCYMqfrfAyvHypqhq4dSu2ECqWQAn8VDZ85iHE/uRrP4UdARr02pdTwyLvgqLiwM9ljgoSTdTiHmmTheJgf/vOHPL7icS489MIdCows4yShzJbpuAdjOz0x/eRu6eHJf8HqD+C3N4NnCJ/2eNSZP1Tcx/yqYNHAJqoqpXqolmqirijmsstBvPDss6PdJKV2enkXHJUWdstvlON42k1LbuIfy/7BBYdcwDkHn7NDbUuQGHhJjPSS5P6G7tIlQdL+fB/UjocFR+XWyL7aUzMDCj4mRU2V+pjxi58pTOmZ50wpNWzG2EzE4ZMOjkqKOn9lGztrduGBWLJ6CYfNPIzzDz0ft2vHku3Z2AMvKpmIkXUJcJcD2s6KtNO/A23tsHYdPP8inHZKbnOVemPFnQSOoY9XckWlPm4GWqtPKTU08qbnqLoa9piznbLiztpkSbJn1u3PxuaNrN22ltPnnj40jRMG3oOVLrvRl/sfhhdfdq5/4StQWOgERad8Pvc2ZhONQO2MrEVjlVJKqY+rvPlUO+00+PWvllIUzKipZnILjl788EUADpp6UNcbjMEVi2a9SKK3DL02btzZgyNjQzTsJHtMi4adpb69adoO1/8C5u4Lf/w1tLQ6c41OPxXGD2HOlnjUmUdUtJNWfldKKZW38qbnKJs48YEPZ2V44cMXqCioYLdxu3XZ7mltJrBpU9b5QMbtIjxleo8aTwkSFNDLEv5tW6BhHSAwYVdnXk9rY+/lNt56x8lj1NICP7oc5syGow8f9O/XJyvuzDOyLJgyu2d9KKWUUupjLn8+2ZYto/Yf/+iy5NzCGnRwZIzhxbUv8qmpn+qSz0iSFv76OpIBP8lQqMdFbBtfY0OP41lYFJJl5Vk8Cls3QLDYSZJYvw62bXaKzXYvt9HWDpdfAycudIKW++92AqOhZsWd0iUl46BmmlM8UymllNrJ5E/P0ZNPsustt8AVl0OVM7yUsGMEGhvx2QMLkGxjc/l/bqKhrYGjq/fFX7el4zZJxBHb7rVOWDIQxNvclOpV6gyqDFGC4nECj7p1nXeItjlzeVwucPmcat8N67MHJNfeAH97EL52Glx49tCUBIm2O+U+7CSEW9K/JUzePVUXSimllNo55U9wNCGVJXfF21B1JAB2PEzh1q2IZ2ArQS59/TfcvepRztv9FL5YdQDSkpGMTZwAqFciJP0BPC0tHZsMBh8JvFLgZJBuaeiyf5fcQIECJ1DpPoy17F346wNwxlfhfy4Z0O8xIHbSGcrb2AYTZznbPF6nnptSSim1E8u/4Oj9d+Gg+eD2YGJtiMuNHeg/CeTDHzzDHase5cw9TuGK/c7G0O+C+p7cHuyMnqU4cUIU4XYVgquxs+5YNi5Xz8CocRtcehVUlMP5Zw+2Nb1LJp0M3IVlzuRvzWGklFIqj+TPnKN0cLRhA7Q2YhsbV7it3zIclm1x54rFXPTijew/bg4//OR3hqxJNjaFMshM12nvvAsLTob3V8F1V0HxEA51JRM6dKaUUipv5U/PUW2t839jEzRuIllUijccwfb2HZyc/5+f8eAHT3Ng9V787tD/wesa3CmzsbPmU7Kx8ePHLznUdftoA3z1TPD74B9/hT12H/wx+mIlNLGjUkqpvJU/wVEgQKK4GG/DNkjEMC0NuKwk+HrvPFu5fS0PfvA0393jFK6ae1aX1WkD5QydBZEsnXRFkkMAYlnwjbMgkYD773IKyQ45o3OLlFJK5a1hHVYTkWNEZKWIrBaRS7Pc/nURaRCRpanLGcPZnlhlJdTVg9ePvW1jv/v/6u17CXoCnPeJ03IKjAwGFy4qpJJKV0WPS069Rv/6N6xaAz+9apgCI5zJVN7BF+NVSimldgbD1nMkIm7gN8BRwAbgVRF5xBjzbrdd7zPGnDtc7cgUq6yksK7BqQfW0oRx9x7wvNO4ioc/fIYzZn+BikBpTo9nkSBECFcOgVWv/nI/VI+DY44cumNmspNOOgKPFrlUSimVn4ZzWG1/YLUx5gMAEfkbcCLQPTgaMbHKSvhgNQDJUCGWxLOegI3t9XzlmcsYFyzn3DkLc368JDZB6WN5/2Ct3whLnofvfRe8OxC8tG3vvQSJsZ2SIEMZ0CmllFIfI8MZHE0A1mf8vAE4IMt+J4vIIcD7wIXGmPXddxCR7wDfAaiurubZZ5/NqUG1xcWYxib+/c4aLLeQJIkLq8s+xhi+v/xqWmJhfrnH9Wz5yMsWWgf8GK54HH/jNiI147Gx+ZB6BlJTti0a59kV6/rcZ7ef38x4EV7a70Bi/ezbK4MTAPn6CNq2bIRVzrBjW1tbzud7OGm7BmestgvGbtu0XUrlr9GekP0o8FdjTExEzgTuAnoUAzPG3A7cDjB37lwzf/78nB5s5WOPIsZwaFmQxpoAEcJ46Trv59+bXuOd1nf52bwLOWHWJ3ocw7fkZYp/eDPSlEoA6fHQesVZRE89Dv//PUfRtb/FtbmBxt9fhRx1BBWu8p4NMQZicQh0Jp98+ekXOGB8Wc+yH1vqYenbsHkLPPEUnPNtDjxkbk6/P+AUsQ0VQ+0uA9r92WefJdfzPZy0XYMzVtsFY7dt2i6l8tdwBkcbgUkZP09MbetgjGnM+PGPwA3D2B5ilVXOlS31xGqqcdE5tBSxYrTG27j5rbupDVXxpZkLetw/ePfDFF95C4lZ04kfeygA3teXU/zDX+Bd9j6hPy8mvvsM7JIiKs67Drm5CObtB888C5Mmwp5z4EfXw1P/gm1N8MXPwcIvwLsrmPuj652VaBefB2d/20n4GInA6d+GlaucBhwwFy46b8dOQlKX6SullFJ9Gc7g6FVgpohMwwmKFgKnZe4gIjXGmM2pH08A3hvG9jhzjoDk5s0k9inHT2fPzcmPX8AbW52H/+kB5+N3d+1Rcm2so+intxGbfwDbb/tRR6+PNDVTcdy3Cf15MZETjqD+5xdSud2Ld+F34ezvd21ASTG0tsHnjodQEO57EO5/GIDmffahfOJ4uOEWePUN+MEFcPuf4P3VcMOPnTlGh30aPDv4lBn6HlJTSiml8tywBUfGGEtEzgWeANzAHcaY5SLyI+A1Y8wjwPdE5ATAArYBXx+u9gDEq5yeI7NlE7AHkpoMtK51M29sfY+Tph3BJ6tm85Vdj+9x36Kf3gZAy7UXdgRGceLYZX7q7vwRgReW0vrVz+J2+wiOq4Un/+FMnn7nXTjiUPjvK/B/T8FlFzm9SQBnn+Esyw8EeLu4kvmzpzm3XXOdk/0a4PyzYOHJQ3cSBGe1nlJKKaWyGtY5R8aYfwL/7LbtyozrlwGXDWcbMiWKi8Hnw968pXNjOMLmP/2Rb6yGC/c4guqJe1J8/k8xXg8t110MAT+Bh54k8L9LaLvg69gTxzttx2BjUyVVsFsV7LYfAcCDx1m67/XC0Yc7F4C9PgHf/VbXBk2c4FwAVqxzVoidvhD2/yS8vRxm7wazZw3dCbCT4PKAJ4f8SkoppVSeGO0J2SNLBKZMgg/W4sYFlkXpOddw4pKXOBHgkcuxS4qQ9jBiJXFv2Up83t4U/Opu4gfuQ/vZX+44lI2NFy+hoVyqn7bbTOcy1LQsiFJKKdWvvAqO4nYcZu2Ka+lSXLgpvP73+Je8xPkLhKoTTuT8ddX4//Vf2i4+A/dHmyi+/Of4/vsmiTm7OvOMfJ25hWySBBnDJTYSMYi2OwVk0wkd4zEorx3ddimllFJjXN4ERze8cAOXPn8p4V2/i/+xx3G3hAnds5gVR+7BrQcs54n9FhBesBvh734JgMT+exI9/jAkYWEKgs7qsQzJVOHYUZW0nKGy7mzbua1mBtStBdsPiTgUlUFJ1Yg3UymllPo4yZvgqCJYgcGwelqAOcYQfORfSDTGw7tY1Iaq2LNi1553Cvgxgd4DIK+McomNaDuESnoEbiBQVg0FJeDyQutW53rV5Cz7KqWUUipT3gRHE4qdic/rJ5UwBwj+7X8BuKPwQz4z+dicCst6RvP0GQMITNy191IgACUVzkUppZRSA5I33QgTipzgaHWFYHxevO+8T1t1KasL4yyYfPCAj+OOhJFwG/5wHHekHcKtEAsPV7N7l7ScfEV9BUZKKaWUGrT8CY5SPUeb400kZziJu9+eFqTYW8CB4/ce2EFsmxgxtk2sgom7OZfaXZxAZaRppmullFJqWOTNsFpZoAyfy8eW9gasGZPxvPcBj1Q1ceSkg/G6BngarATJUIiphXs6Q2ricYa3XJ5UDqER7MWxLAgWjtzjKaWUUnkib3qORIRKXyV17Q0kZ04B4MmaKMcMYkjNyRNUQkACeMSTPrBTyNWKD0Or+yAC/jGcSkAppZT6mMqb4Aig0l9JXVsD0ZOO5IGv7sW7tR4On7D/gO9vSOIOlPS8oaDYCZxGlAHvKKcSUEoppXZCeRcc1bc1YFeUc/FeDRw8cS6F3oH3vtjY+PxZgiN/gVPQdaQkLScwcufNqKhSSik1YvIrOPJVUtdaz3vNa1nXuoljJn2q80ZjcEWjvd85aZH0evB7Cnreli7kakYoQrISEND5RkoppdRwyKuuh0p/JfFknL98+CSC8JlJB3XcJpaFy0pgEm6Mt2dyR1c8gVVSiJcsiR/dHggUOMv6RZwgKRAavp6dpOU8nlJKKaWGXN4FRwB/Xfs0n6zanXGhzuSILitBorgYT1sbyW7BkSQS4BKiZSW9J36s3aVzUna0Heo/cuYiDQdjOnurlFJKKTWk8is48jnBUUuiveuQGoAxJErLcUciTqJHY2NIZc12uWifMAl8rt6DI1+gM2AJFEDjRkgmwT0My/tFwOMb+uMqpZRSKr+Coyp/Z9HVBRMO7LwhNVcoGQwRqxyHOxohXl6J8XSeHktsfMjAyoy43FBeC1s3DFOiRqPBkVJKKTVMxIzUJOIhIiINwLrc7kxV8bhiE7DxeFwuj22MDeASXLbBtCQSbdnu5nI7/xLRRDTSHGkdyEO5RVyVxaGKhJUcUAKkaMIqCXg9zQP5Lbxut6eupX3rQI47BCqBkXqswdB2Dc5YbReM3bbtbO2aYoyp6n83pdTHLjjaESLymjFm7mi3I5ux2jZt1+BouwZvrLZN26VU/sqrpfxKKaWUUv3R4EgppZRSKkO+BUe3j3YD+jBW26btGhxt1+CN1bZpu5TKU3k150gppZRSqj/51nOklFJKKdUnDY6UUkoppTLkTXAkIseIyEoRWS0il45iOyaJyBIReVdElovI+antV4vIRhFZmrocOwptWysiy1KP/1pqW7mIPCUiq1L/l41wm3bLOCdLRaRFRC4YrfMlIneISL2IvJOxLes5Esetqdfc2yKy7wi360YRWZF67IdFpDS1faqIRDLO3W0j3K5enzsRuSx1vlaKyGeGq119tO2+jHatFZGlqe0jcs76eH8Y9deYUnnFGLPTXwA3sAaYDviAt4DZo9SWGmDf1PUi4H1gNnA1cPEon6e1QGW3bTcAl6auXwpcP8rP4xZgymidL+AQYF/gnf7OEXAs8H+AAPOAl0e4XUcDntT16zPaNTVzv1E4X1mfu9TfwVuAH5iW+pt1j2Tbut3+c+DKkTxnfbw/jPprTC96yadLvvQc7Q+sNsZ8YIyJA38DThyNhhhjNhtj3khdbwXeAyaMRlsG6ETgrtT1u4DPjV5TOAJYY4zJLUP6EDDG/BvY1m1zb+foROBu43gJKBWRmpFqlzHmSWOMlfrxJWDicDz2YNvVhxOBvxljYsaYD4HVOH+7I942ceoEnQL8dbgev5c29fb+MOqvMaXySb4ERxOA9Rk/b2AMBCQiMhXYB3g5tencVNf4HSM9fJVigCdF5HUR+U5qW7UxZnPq+hagehTalbaQrh9Wo32+0no7R2PpdfdNnB6GtGki8qaIPCcinx6F9mR77sbS+fo0UGeMWZWxbUTPWbf3h4/Da0ypnUa+BEdjjogUAg8CFxhjWoDfATOAvYHNOF36I+1gY8y+wALgHBE5JPNGY4zBCaBGnIj4gBOAv6c2jYXz1cNonqPeiMgVgAXcm9q0GZhsjNkH+D7wFxEpHsEmjcnnrpsv0TUQH9FzluX9ocNYfI0ptbPJl+BoIzAp4+eJqW2jQkS8OG989xpjHgIwxtQZY5LGKYb7B4ZxOKE3xpiNqf/rgYdTbahLd9On/q8f6XalLADeMMbUpdo46ucrQ2/naNRfdyLydeB44MupD1VSw1aNqeuv48zt2XWk2tTHczfq5wtARDzA54H70ttG8pxle39gDL/GlNoZ5Utw9CowU0SmpXogFgKPjEZDUnMZFgHvGWNuztieOU/gJOCd7vcd5nYViEhR+jrOZN53cM7T11K7fQ1YPJLtytDlm/xon69uejtHjwBfTa0omgc0ZwyNDDsROQb4AXCCMSacsb1KRNyp69OBmcAHI9iu3p67R4CFIuIXkWmpdr0yUu3KcCSwwhizIb1hpM5Zb+8PjNHXmFI7rdGeET5SF5xVHe/jfOO7YhTbcTBOl/jbwNLU5VjgHmBZavsjQM0It2s6zkqht4Dl6XMEVADPAKuAp4HyUThnBUAjUJKxbVTOF06AthlI4Mzv+FZv5whnBdFvUq+5ZcDcEW7Xapz5KOnX2W2pfU9OPcdLgTeAz45wu3p97oArUudrJbBgpJ/L1PY7ge9223dEzlkf7w+j/hrTi17y6aLlQ5RSSimlMuTLsJpSSiml1IBocKSUUkoplUGDI6WUUkqpDBocKaWUUkpl0OBIKaWUUiqDBkdKdSMiyYzq60tF5NIhPPbUzCrwSimlxh7PaDdAqTEoYozZe7QboZRSanRoz5FSAyQia0XkBhFZJiKviMguqe1TReRfqUKqz4jI5NT2ahF5WETeSl0OSh3KLSJ/EJHlIvKkiARH7ZdSSinVgwZHSvUU7DasdmrGbc3GmE8AvwZ+mdr2K+AuY8yeOMVdb01tvxV4zhizF7AvToZlcEpP/MYYswewHSf7slJKqTFCM2Qr1Y2ItBljCrNsXwscboz5IFUcdIsxpkJEtuKUwEiktm82xlSKSAMw0RgTyzjGVOApY8zM1M+XAF5jzLUj8KsppZQaAO05UmpwTC/XByOWcT2Jzv1TSqkxRYMjpQbn1Iz//5u6/iKwMHX9y8DzqevPAGcBiIhbREpGqpFKKaVyp99YleopKCJLM35+3BiTXs5fJiJv4/T+fCm17TzgTyLy/4AG4Bup7ecDt4vIt3B6iM7CqQKvlFJqDNM5R0oNUGrO0VxjzNbRbotSSqnho8NqSimllFIZtOdIKaWUUiqD9hwppZRSSmXQ4EgppZRSKoMGR0oppZRSGTQ4UkoppZTKoMGRUkoppVSG/w/uM5FfaLnpkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "fig,ax=plt.subplots()\n",
    "df['logEpoch']=np.log10(df['Epoch'])\n",
    "# sns.lineplot('Step', 'AttentionModelwFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'AttentionModelwoFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'DenseModel_val', data=att_model_df, ax=ax)\n",
    "\n",
    "def get_mov_ave(y, window_size=3, percentiles=(10,90)):\n",
    "    assert window_size%2 ==1\n",
    "    w=int(window_size/2)\n",
    "    out=[]\n",
    "    lower=[]\n",
    "    upper=[]\n",
    "    l,u=percentiles[0],percentiles[1]\n",
    "    for i in range(w, len(y)-w):\n",
    "        out.append(np.average(y[i-w:i+w+1]))\n",
    "        lower.append(np.percentile(y[i-w:i+w+1],l))\n",
    "        upper.append(np.percentile(y[i-w:i+w+1],u))\n",
    "\n",
    "    while w>0:\n",
    "        w=w-1\n",
    "        win_size=w*2+1\n",
    "        out.insert(0, np.average(y[:win_size]))\n",
    "        lower.insert(0, np.percentile(y[:win_size], l))\n",
    "        upper.insert(0, np.percentile(y[:win_size], u))\n",
    "        out.append(np.average(y[len(y)-win_size:]))\n",
    "        lower.append(np.percentile(y[len(y)-win_size:], l))\n",
    "        upper.append(np.percentile(y[len(y)-win_size:], u))\n",
    "    return out, lower, upper\n",
    "\n",
    "def get_min_max(mov_ave, y):\n",
    "    assert len(mov_ave)==len(y)\n",
    "    mins=np.min(np.vstack([mov_ave, y]), axis=0)\n",
    "    maxs=np.max(np.vstack([mov_ave,y]), axis=0)\n",
    "    return mins, maxs\n",
    "\n",
    "colors1=[\"b\",\"g\",\"r\"]\n",
    "colors2=[\"powderblue\",\"palegreen\",\"lightsalmon\"]\n",
    "for idx, i in enumerate([\"LLDLwFW_valacc\",\n",
    "          \"LLDLwoFW_valacc\", \n",
    "          \"DenseModel_valacc\"\n",
    "         ]):\n",
    "    x_plot=df['Epoch']\n",
    "    y_plot=df[i]\n",
    "    mov_ave, lower, upper=get_mov_ave(y_plot, window_size=15)\n",
    "    plt.plot(x_plot, mov_ave, c=colors1[idx])\n",
    "    #mins,maxs=get_min_max(mov_ave, get_mov_ave(y_plot, window_size=3))\n",
    "    plt.fill_between(x_plot,lower, upper, color=colors2[idx], alpha=0.3)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim([0.45, 0.95])\n",
    "ax.legend([\"Locality-adaptive Deep Learner\", \"Locality-adaptive Deep Learner\\nw/o Feature Weights\", \"Dense Model\"], \n",
    "          bbox_to_anchor=(1,1))\n",
    "ax.set_title(\"10-D Synthetic Data with cluster-specific noise\")\n",
    "plt.grid()\n",
    "# savefile=os.path.join(SynthDataFolder, \"SynthData_10dim_LocallyAdaptiveDeepLearner\")\n",
    "# plt.savefig(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "n_estimators_list=[200,500,1000]\n",
    "n_repeats=10\n",
    "random_seeds=range(n_repeats)\n",
    "min_samples_list=list(range(1,11))\n",
    "RF_results=[]\n",
    "\n",
    "for n_estimator in n_estimators_list:\n",
    "    for min_samples in min_samples_list:\n",
    "        for i in range(n_repeats):\n",
    "            np.random.seed(random_seeds[i])\n",
    "            clf=ExtraTreesClassifier(n_estimators=n_estimator, \n",
    "                                     min_samples_leaf=min_samples,\n",
    "                                     bootstrap=True, \n",
    "                                     oob_score=True, \n",
    "                                     class_weight=\"balanced_subsample\")\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_score=clf.score(X_train, y_train)\n",
    "            test_score=clf.score(X_test, y_test)\n",
    "            RF_results.append([n_estimator, min_samples, \"train_accuracy\", train_score])\n",
    "            RF_results.append([n_estimator, min_samples, \"test_accuracy\", test_score])\n",
    "        \n",
    "RF_results=pd.DataFrame(RF_results, columns=[\"n_estimator\", \"min_samples_leaf\", \"metric\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_ave_LUbounds(df, x_col_name, percentiles=(10,90)):\n",
    "    l,u=percentiles[0], percentiles[1]\n",
    "    x_plot=np.unique(df[x_col_name])\n",
    "    ave=[]\n",
    "    lower=[]\n",
    "    upper=[]\n",
    "    if percentiles=='std':\n",
    "        for x in x_plot:\n",
    "            out=df[df[x_col_name]==x]['score']\n",
    "            mean, std=np.mean(out), np.std(out)\n",
    "            ave.append(mean)\n",
    "            lower.append(mean-std)\n",
    "            upper.append(mean+std)\n",
    "        return x_plot, ave, lower, upper\n",
    "    for x in x_plot:\n",
    "        out=df[df[x_col_name]==x]['score']\n",
    "        ave.append(np.mean(out))\n",
    "        lower.append(np.percentile(out, l))\n",
    "        upper.append(np.percentile(out, u))\n",
    "    return x_plot, ave, lower, upper\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "colors1=[\"b\",\"g\",\"r\"]\n",
    "colors2=[\"powderblue\",\"palegreen\",\"lightsalmon\"]\n",
    "for idx, n_estimator in enumerate(n_estimators_list):\n",
    "    out_df=RF_results.query(\"n_estimator == @n_estimator & metric == 'test_accuracy' \")\n",
    "    x_plot, ave, lower, upper=get_ave_LUbounds(out_df, x_col_name='min_samples_leaf',\n",
    "                                               percentiles='std'\n",
    "                                              )\n",
    "    ax.plot(x_plot, ave, c=colors1[idx])\n",
    "    ax.fill_between(x_plot, lower, upper, color=colors2[idx], alpha=0.3)\n",
    "\n",
    "ax.set_xlabel(\"Minimum samples in leaf nodes\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "# ax.set_ylim([0.45, 0.95])\n",
    "ax.legend([\"No. of trees=\"+str(i) for i in n_estimators_list], \n",
    "          loc=(\"lower right\"))\n",
    "ax.set_title(\"10-D Synthetic Data with cluster-specific noise\")\n",
    "\n",
    "plt.grid()\n",
    "# savefile=os.path.join(SynthDataFolder, \"SynthData_10dim_RF\")\n",
    "# plt.savefig(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "n_estimators_list=[200,500,1000]\n",
    "n_repeats=3\n",
    "random_seeds=range(n_repeats)\n",
    "gamma_list=[0.00001,0.00003,0.0001,0.0003, 0.001, 0.003, 0.01,0.03, 0.1,0.3]\n",
    "xgb_results=[]\n",
    "scale_pos_weight= (len(y_train)-np.sum(y_train))/np.sum(y_train) #Ratio of number of negatives to number of positives\n",
    "\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for gamma in gamma_list:\n",
    "        for i in range(n_repeats):\n",
    "            xgb_clf=XGBClassifier(n_estimators=n_estimators,\n",
    "                                  gamma=gamma,\n",
    "                                  max_depth=20,\n",
    "                                  scale_pos_weight=scale_pos_weight,\n",
    "                                  use_label_encoder=False, \n",
    "                                  eval_metric='logloss',\n",
    "                                  seed=random_seeds[i]\n",
    "                                 )\n",
    "            xgb_clf.fit(X_train, y_train)\n",
    "            train_score=xgb_clf.score(X_train, y_train)\n",
    "            test_score=xgb_clf.score(X_test, y_test)\n",
    "            xgb_results.append([n_estimators, gamma, \"train_accuracy\", train_score])\n",
    "            xgb_results.append([n_estimators, gamma, \"test_accuracy\", test_score])\n",
    "            del xgb_clf\n",
    "xgb_results=pd.DataFrame(xgb_results, columns=[\"n_estimator\", \"gamma\", \"metric\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_ave_LUbounds(df, x_col_name, percentiles=(10,90)):\n",
    "    l,u=percentiles[0], percentiles[1]\n",
    "    x_plot=np.unique(df[x_col_name])\n",
    "    ave=[]\n",
    "    lower=[]\n",
    "    upper=[]\n",
    "    if percentiles=='std':\n",
    "        for x in x_plot:\n",
    "            out=df[df[x_col_name]==x]['score']\n",
    "            mean, std=np.mean(out), np.std(out)\n",
    "            ave.append(mean)\n",
    "            lower.append(mean-std)\n",
    "            upper.append(mean+std)\n",
    "        return x_plot, ave, lower, upper\n",
    "    for x in x_plot:\n",
    "        out=df[df[x_col_name]==x]['score']\n",
    "        ave.append(np.mean(out))\n",
    "        lower.append(np.percentile(out, l))\n",
    "        upper.append(np.percentile(out, u))\n",
    "    return x_plot, ave, lower, upper\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "colors1=[\"b\",\"g\",\"r\"]\n",
    "colors2=[\"powderblue\",\"palegreen\",\"lightsalmon\"]\n",
    "for idx, n_estimator in enumerate(n_estimators_list):\n",
    "    out_df=xgb_results.query(\"n_estimator == @n_estimator & metric == 'test_accuracy' \")\n",
    "    x_plot, ave, lower, upper=get_ave_LUbounds(out_df, x_col_name='gamma',\n",
    "                                               percentiles='std'\n",
    "                                              )\n",
    "    ax.plot(x_plot, ave, c=colors1[idx])\n",
    "    ax.fill_between(x_plot, lower, upper, color=colors2[idx], alpha=0.3)\n",
    "\n",
    "ax.set_xlabel(\"Gamma\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "# ax.set_ylim([0.45, 0.95])\n",
    "ax.legend([\"No. of trees=\"+str(i) for i in n_estimators_list], \n",
    "          loc=(\"lower right\"))\n",
    "ax.set_title(\"10-D Synthetic Data with cluster-specific noise\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylim((0.45,0.95))\n",
    "plt.grid()\n",
    "# savefile=os.path.join(SynthDataFolder, \"SynthData_10dim_RF\")\n",
    "# plt.savefig(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimator</th>\n",
       "      <th>gamma</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_estimator    gamma         metric  score\n",
       "1            200  0.00001  test_accuracy   0.77\n",
       "3            200  0.00001  test_accuracy   0.77\n",
       "5            200  0.00001  test_accuracy   0.77\n",
       "7            200  0.00003  test_accuracy   0.77\n",
       "9            200  0.00003  test_accuracy   0.77\n",
       "..           ...      ...            ...    ...\n",
       "171         1000  0.10000  test_accuracy   0.78\n",
       "173         1000  0.10000  test_accuracy   0.78\n",
       "175         1000  0.30000  test_accuracy   0.74\n",
       "177         1000  0.30000  test_accuracy   0.74\n",
       "179         1000  0.30000  test_accuracy   0.74\n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results[xgb_results['metric']==\"test_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out Similarity Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just create tensorflow function and call it _on_epoch end\n",
    "    Args:\n",
    "      variant_tensor: The variant-dtype Tensor associated with the Dataset. This\n",
    "        Tensor will be a captured input to functions which use the Dataset, and\n",
    "        is used by saving code to identify the corresponding _VariantTracker.\n",
    "      resource_creator: A zero-argument function which creates a new\n",
    "        variant-dtype Tensor. This function will be included in SavedModels and\n",
    "        run to re-create the Dataset's variant Tensor on restore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnaryDataset(DatasetV2):\n",
    "  \"\"\"Abstract class representing a dataset with one input.\"\"\"\n",
    "\n",
    "  def __init__(self, input_dataset, variant_tensor):\n",
    "    self._input_dataset = input_dataset\n",
    "    super(UnaryDataset, self).__init__(variant_tensor)\n",
    "\n",
    "  def _inputs(self):\n",
    "    return [self._input_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sh_ds._variant_tensor_attr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(<unprintable>, shape=(), dtype=variant)\n"
     ]
    }
   ],
   "source": [
    "print(Sh_ds._variant_tensor_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.data.ops.dataset_ops import UnaryDataset\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class SimilarityBatchingDataset():\n",
    "    def __init__(self, \n",
    "                 numpy_dataset,\n",
    "                 numpy_targets,                 \n",
    "                 attentions,\n",
    "                 n_batch=8,\n",
    "                 clusterer_kwargs={\"n_clusters\":None,\n",
    "                                   \"affinity\":\"cosine\", \n",
    "                                   \"linkage\":\"average\"\n",
    "                                  },\n",
    "                 use_inter_op_parallelism=True,\n",
    "                 preserve_cardinality=False,\n",
    "                 use_legacy_function=False,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        self.numpy_dataset=numpy_dataset\n",
    "        self.numpy_targets=numpy_targets\n",
    "        self.attentions=attentions\n",
    "        self.n_batch=n_batch\n",
    "        self.clusterer_kwargs=clusterer_kwargs\n",
    "        self._use_inter_op_parallelism=use_inter_op_parallelism\n",
    "        self._preserve_cardinality=preserve_cardinality\n",
    "        self._use_legacy_function=use_legacy_function\n",
    "    \n",
    "    \n",
    "\n",
    "    def _partitions(self):\n",
    "        #Set n_clusters if not initialized\n",
    "        if self.clusterer_kwargs.get(\"n_clusters\") is None:\n",
    "            self.clusterer_kwargs[\"n_clusters\"]=20\n",
    "        \n",
    "        clusterer=AgglomerativeClustering(**self.clusterer_kwargs)\n",
    "        clusterer.fit(self.attentions)\n",
    "        cl_labels=clusterer.labels_\n",
    "        unique,counts=np.unique(cl_labels, return_counts=True)\n",
    "        print(f\"Fitted {len(unique)} clusters with distribution {np.sort(counts)[::-1]}\")\n",
    "        #Remember to shuffle partitions\n",
    "        partitions=[np.where(cl_labels==i)[0] for i in np.unique(cl_labels)]\n",
    "        m=len(partitions)\n",
    "        rand_order=np.random.choice(range(m), size=m, replace=False)\n",
    "        partitions=[partitions[i] for i in rand_order]\n",
    "        \n",
    "        return partitions\n",
    "\n",
    "    def get_rearranged_tensor(self):\n",
    "        arr=np.empty_like(self.numpy_dataset)\n",
    "        arr_targets=np.empty_like(self.numpy_targets)\n",
    "        self.partitions=self._partitions()\n",
    "        ptr=0\n",
    "        for p in self.partitions:\n",
    "            arr[ptr:ptr+len(p)]=self.numpy_dataset[p,:]\n",
    "            arr_targets[ptr:ptr+len(p)]=self.numpy_targets[p,:]\n",
    "            ptr+=len(p)\n",
    "        return arr, arr_targets\n",
    "    \n",
    "#     def get_similarity_batched_dataset(self):\n",
    "#         arr=np.empty_like(self.numpy_dataset)\n",
    "#         self.partitions=self._partitions()\n",
    "#         ptr=0\n",
    "#         for p in self.partitions:\n",
    "#             arr[ptr:ptr+len(p)]=self.numpy_dataset[p,:]\n",
    "#             ptr+=len(p)\n",
    "#         ds=tf.data.Dataset.from_tensor_slices(arr)\n",
    "#         return ds.batch(n_batch)\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 20 clusters with distribution [232 104  93  80  76  66  59  51  41  36  30  16   5   4   2   1   1   1\n",
      "   1   1]\n"
     ]
    }
   ],
   "source": [
    "attentions=LSwFW_model.layers[1](train_tensor).numpy()\n",
    "simbatched=SimilarityBatchingDataset(\n",
    "    train_data,\n",
    "    train_targets,\n",
    "    attentions,\n",
    ")\n",
    "rearranged_train_data, rearranged_train_targets=simbatched.get_rearranged_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSwFW_model.optimizer.lr.assign(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 20 clusters with distribution [132 120 118 108  89  66  63  51  44  33  24  16  14   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0555 - accuracy: 0.9889 - val_loss: 0.9279 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_simBatched\n",
      "Fitted 20 clusters with distribution [153 132 126 110  86  66  62  52  34  24  18  15   7   4   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0450 - accuracy: 0.9911 - val_loss: 0.9368 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.88000 to 0.89000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_simBatched\n",
      "Fitted 20 clusters with distribution [153 132 126 111  93  89  61  51  34  12  11   7   4   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0397 - accuracy: 0.9911 - val_loss: 0.9473 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [158 132 125 111 100  89  51  51  34  14   8   7   4   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0380 - accuracy: 0.9933 - val_loss: 0.9621 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 111  91  89  86  62  50  34  34  12  12   8   7   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0361 - accuracy: 0.9944 - val_loss: 0.9706 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 112  91  89  86  61  50  34  34  12  12   8   7   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0356 - accuracy: 0.9933 - val_loss: 0.9779 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 125 112  89  89  61  50  34  14   8   7   4   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0338 - accuracy: 0.9956 - val_loss: 0.9862 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 124 111  89  89  62  50  34  14   9   7   4   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0341 - accuracy: 0.9956 - val_loss: 0.9872 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 125 111  92  84  62  50  34  12  11   7   4   4   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0333 - accuracy: 0.9956 - val_loss: 0.9923 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 125 111  92  84  62  50  34  12  11   7   4   4   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0331 - accuracy: 0.9967 - val_loss: 0.9998 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [159 132 124 112  89  85  61  50  34  12   8   8   7   4   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0328 - accuracy: 0.9967 - val_loss: 1.0037 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 132 112 112  89  85  60  50  33  23  12  11   8   7   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0320 - accuracy: 0.9967 - val_loss: 1.0075 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [157 132 124 112  89  87  61  50  34  12   8   8   7   4   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0318 - accuracy: 0.9967 - val_loss: 1.0123 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 132 112 112  89  87  60  50  33  23  12   8   8   7   4   3   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0314 - accuracy: 0.9978 - val_loss: 1.0203 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 132 112 112  89  87  60  50  33  23  12   8   8   7   4   3   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0312 - accuracy: 0.9978 - val_loss: 1.0244 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 132 112 112  89  87  60  50  33  23  12   8   8   7   4   3   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0308 - accuracy: 0.9978 - val_loss: 1.0267 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [148 132 112 112  89  89  58  50  34  20  12  12  10   7   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0303 - accuracy: 0.9978 - val_loss: 1.0366 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [157 132 112 103  89  89  58  50  34  20  12  12  10   7   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0304 - accuracy: 0.9978 - val_loss: 1.0442 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [148 132 112 112  89  89  58  50  34  20  12  12  10   7   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0299 - accuracy: 0.9978 - val_loss: 1.0449 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [148 132 112 112  89  89  58  50  34  20  12  12  10   7   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0297 - accuracy: 0.9978 - val_loss: 1.0545 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [148 132 112 112  89  88  58  50  34  20  12  12  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0293 - accuracy: 0.9978 - val_loss: 1.0528 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [148 132 112 112  89  88  58  50  34  20  12  12  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0289 - accuracy: 0.9978 - val_loss: 1.0608 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [150 132 112 112  80  73  69  60  34  20  12  12  11   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0290 - accuracy: 0.9967 - val_loss: 1.0617 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [150 132 112 112  86  73  69  60  34  20  12   8   8   7   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0286 - accuracy: 0.9978 - val_loss: 1.0604 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [150 132 116 112  86  73  69  56  34  20  12   8   8   7   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0288 - accuracy: 0.9978 - val_loss: 1.0618 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [150 132 116 112  86  73  69  56  34  20  12   8   8   7   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0286 - accuracy: 0.9978 - val_loss: 1.0635 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [152 132 116 112  83  73  69  56  34  20  12  12   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0268 - accuracy: 0.9989 - val_loss: 1.0696 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [152 132 116 112  83  73  69  56  34  20  12  12   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0267 - accuracy: 0.9989 - val_loss: 1.0766 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [147 132 116 112  91  89  56  50  34  20  12  12   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0268 - accuracy: 0.9989 - val_loss: 1.0741 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [147 132 112 111  91  89  61  50  34  20  12  12   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0285 - accuracy: 0.9978 - val_loss: 1.0812 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [147 132 112 111  97  89  61  50  34  20  14   8   7   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0259 - accuracy: 0.9989 - val_loss: 1.0827 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [132 128 112 108  97  89  61  50  34  24  20  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0257 - accuracy: 0.9989 - val_loss: 1.0872 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [132 128 112 108  97  89  61  50  34  24  20  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0255 - accuracy: 0.9989 - val_loss: 1.0898 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [132 128 112 108  97  89  61  50  34  24  20  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0254 - accuracy: 0.9989 - val_loss: 1.0946 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [132 128 112 108 102  89  56  50  34  24  20  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0252 - accuracy: 0.9989 - val_loss: 1.0974 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 129 112 108 102  89  56  50  34  24  20  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0250 - accuracy: 0.9989 - val_loss: 1.0998 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 129 112 108 107  89  56  50  34  20  19  14   8   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0249 - accuracy: 0.9989 - val_loss: 1.1036 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [130 129 112 107 103  89  56  50  34  20  19  12  12   8   7   5   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0247 - accuracy: 0.9989 - val_loss: 1.1059 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 112 107 103  89  56  50  34  20  19  12  12   8   7   5   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0245 - accuracy: 0.9989 - val_loss: 1.1091 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 111 103 101  89  59  50  34  28  19  12  12   7   5   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0244 - accuracy: 0.9989 - val_loss: 1.1143 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 111 109 101  89  59  50  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0243 - accuracy: 0.9989 - val_loss: 1.1182 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 111 109 101  89  59  50  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0241 - accuracy: 0.9989 - val_loss: 1.1200 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 111 109 101  89  59  50  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0241 - accuracy: 0.9989 - val_loss: 1.1244 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [136 131 110 109 101  82  59  50  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0242 - accuracy: 0.9989 - val_loss: 1.1289 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 110 109 101  82  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0239 - accuracy: 0.9989 - val_loss: 1.1318 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 110 109 101  82  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0237 - accuracy: 0.9989 - val_loss: 1.1354 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 110 109 101  82  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0234 - accuracy: 0.9989 - val_loss: 1.1393 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 110 109 101  82  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0235 - accuracy: 0.9989 - val_loss: 1.1415 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 110 109 101  82  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0281 - accuracy: 0.9978 - val_loss: 1.1243 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 110 103  89  57  51  34  28  12   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0237 - accuracy: 1.0000 - val_loss: 1.1256 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 109 109 101  83  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0235 - accuracy: 0.9989 - val_loss: 1.1363 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 109 109 101  83  59  51  34  28  19  14   7   5   4   4   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0231 - accuracy: 0.9989 - val_loss: 1.1393 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 109 109 105  83  59  51  34  28  19  12   7   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0230 - accuracy: 0.9989 - val_loss: 1.1450 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 120 109 105  83  53  51  34  28  12   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0229 - accuracy: 0.9989 - val_loss: 1.1480 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [134 131 110 109 105  83  59  51  34  28  19  12   7   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0229 - accuracy: 0.9989 - val_loss: 1.1538 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 114 109 105  83  59  51  34  28  12   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0227 - accuracy: 0.9989 - val_loss: 1.1581 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 114 109 105  83  59  51  34  28  12   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0227 - accuracy: 0.9989 - val_loss: 1.1582 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 117 114 105  95  59  51  34  12   8   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0227 - accuracy: 0.9989 - val_loss: 1.1636 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 117 114 105  95  59  51  34  12   8   7   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0225 - accuracy: 0.9989 - val_loss: 1.1661 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [146 131 114 109 109  83  59  51  34  20  12   8   8   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0226 - accuracy: 0.9989 - val_loss: 1.1708 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [145 132 114 109  97  95  59  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0223 - accuracy: 0.9989 - val_loss: 1.1727 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [145 132 114 109  97  95  59  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0222 - accuracy: 0.9989 - val_loss: 1.1774 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 132 114 109 104  95  59  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0222 - accuracy: 0.9989 - val_loss: 1.1793 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [145 132 114 109  97  95  59  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0219 - accuracy: 1.0000 - val_loss: 1.1811 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 132 114 109 104  95  59  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0220 - accuracy: 0.9989 - val_loss: 1.1832 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 132 120 109 104  95  53  51  33  20  12   8   8   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0220 - accuracy: 1.0000 - val_loss: 1.1896 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 120 104 102  95  53  51  33  28  12   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0218 - accuracy: 0.9989 - val_loss: 1.1919 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 120 104 102  95  53  51  33  28  12   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0220 - accuracy: 0.9989 - val_loss: 1.1946 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 124 120 102  95  53  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0217 - accuracy: 0.9989 - val_loss: 1.1977 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 102  98  53  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0216 - accuracy: 1.0000 - val_loss: 1.2038 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 102  98  53  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0216 - accuracy: 0.9989 - val_loss: 1.2074 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 102  98  53  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0215 - accuracy: 1.0000 - val_loss: 1.2056 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 1.2111 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0213 - accuracy: 0.9989 - val_loss: 1.2124 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0211 - accuracy: 1.0000 - val_loss: 1.2130 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0210 - accuracy: 1.0000 - val_loss: 1.2161 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.2190 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0211 - accuracy: 0.9989 - val_loss: 1.2218 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.2240 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0207 - accuracy: 1.0000 - val_loss: 1.2252 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0208 - accuracy: 0.9989 - val_loss: 1.2293 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0206 - accuracy: 1.0000 - val_loss: 1.2311 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 1.2315 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 132 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 1.2344 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 131 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0205 - accuracy: 1.0000 - val_loss: 1.2379 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 131 121 120 103  98  52  51  33  12   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.2391 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 131 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0201 - accuracy: 1.0000 - val_loss: 1.2414 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 131 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0203 - accuracy: 1.0000 - val_loss: 1.2443 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0200 - accuracy: 1.0000 - val_loss: 1.2456 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0200 - accuracy: 1.0000 - val_loss: 1.2509 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.2483 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 121 103  98  52  51  33  11   8   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0197 - accuracy: 1.0000 - val_loss: 1.2547 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.2580 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.2635 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0194 - accuracy: 1.0000 - val_loss: 1.2700 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.2704 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.2729 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 121 119 103  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.2740 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 121 119 104  98  52  51  33  11  10   8   5   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0191 - accuracy: 1.0000 - val_loss: 1.2750 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 120 119 103  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.2794 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 120 119 103  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.2805 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 130 120 119 103  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0188 - accuracy: 1.0000 - val_loss: 1.2830 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 104  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.2838 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 104  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0186 - accuracy: 1.0000 - val_loss: 1.2862 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 104  98  52  51  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0199 - accuracy: 0.9989 - val_loss: 1.2875 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 103  98  52  52  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.2878 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 103  98  52  52  33  11  10   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.2925 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 120 119 105  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 1.2981 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [149 130 120 112 105  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [149 130 120 112 105  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0181 - accuracy: 1.0000 - val_loss: 1.2998 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [149 130 120 112 105  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.3001 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 121 120 103  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.3014 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [142 130 121 120 103  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.3058 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 121 120 103  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.3071 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 121 120 103  98  52  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0216 - accuracy: 0.9989 - val_loss: 1.3211 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 121 111 103  98  61  52  33  11   8   8   6   4   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0240 - accuracy: 0.9978 - val_loss: 1.3158 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 111 107 103  95  54  51  33  27  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.3002 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [145 128 111 103  98  87  61  58  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0176 - accuracy: 1.0000 - val_loss: 1.3024 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 111 104  98  94  61  52  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0178 - accuracy: 0.9989 - val_loss: 1.3055 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 111 104  98  94  61  52  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0177 - accuracy: 1.0000 - val_loss: 1.3021 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 111 104  98  94  61  52  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 1.3045 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [144 128 120 104  98  94  52  52  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.3049 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 120 104  98  94  52  52  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.3087 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 120 105  98  94  52  51  33  28  11   8   8   6   4   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.3049 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 120  98  94  81  52  51  33  28  28  11   8   8   6   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.3060 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 120  98  94  81  52  51  33  28  28  11   8   8   6   3   3   2\n",
      "   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0168 - accuracy: 1.0000 - val_loss: 1.3071 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 120  98  94  81  52  51  33  28  28  11   8   8   6   3   3   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0167 - accuracy: 1.0000 - val_loss: 1.3108 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 112 108  98  94  60  51  33  28  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0166 - accuracy: 1.0000 - val_loss: 1.3087 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 112 108  98  94  60  51  33  28  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.3095 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 112 104  98  94  60  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 1.3105 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.3111 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 1.3120 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 1.3124 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.3132 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.3135 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 1.3151 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 131 116 104  98  94  56  51  33  28  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.3148 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  94  56  51  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.3158 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  94  56  51  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.3113 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  94  56  51  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.3125 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  94  56  51  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.3129 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  94  56  51  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.3132 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 103  98  94  56  52  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.3137 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 103  98  94  56  52  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.3142 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 103  98  94  56  52  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.3154 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 103  98  94  56  52  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 1.3158 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 103  98  94  56  52  33  21  11   8   8   6   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.3189 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  33  21  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 1.3186 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  33  21  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.3171 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  33  21  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.3202 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  33  21  11   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0146 - accuracy: 1.0000 - val_loss: 1.3196 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  32  21  11   8   8   6   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0145 - accuracy: 1.0000 - val_loss: 1.3203 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  32  21  11   8   8   6   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.3180 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  32  21  11   8   8   6   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  32  21  11   8   8   6   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.3200 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  32  21  11   8   8   6   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0141 - accuracy: 1.0000 - val_loss: 1.3198 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 107  98  94  56  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 1.3201 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.3211 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  97  56  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.3202 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 116 104  98  97  56  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 1.3205 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 1.3214 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  11   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.3211 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 1.3221 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 1.3236 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.3236 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.3241 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 1.3245 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [141 138 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.3237 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.3255 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.3237 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 107  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.3255 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 104  98  97  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 1.3256 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 116 104  98  97  56  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.3245 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 104  98  97  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 1.3253 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 104  98  97  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0123 - accuracy: 1.0000 - val_loss: 1.3254 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 108  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.3245 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 116 108  98  94  56  52  31  21  12   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.3258 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 120 108  98  94  52  52  31  21  12   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.3249 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [151 128 116 108  98  94  56  52  31  21  12   8   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.3213 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 115 108 101  98  56  52  31  21  14  12   8   8   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.3229 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 115 108 101  98  56  52  31  21  14  12   8   8   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.3186 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 115 108 101  98  56  52  31  21  14  12   8   8   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.3196 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 114 113 106  93  52  40  31  28  14  12   8   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.3198 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 109 109 106  93  56  52  31  21  14  12   8   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.3195 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 109 109 106  93  56  52  31  21  14  12   8   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.3177 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 109 109 106  93  56  52  31  21  14  12   8   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.3156 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [131 128 114 107 106  93  52  40  31  26  21  14  12   8   7   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 114 107  99  93  52  40  31  26  21  14  12   8   7   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.3190 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [129 128 114 105  99  93  52  47  39  31  28   8   8   6   3   3   2   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.3121 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 129 123 115  99  93  52  38  31  28  14   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.3118 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 129 123 115  99  93  52  38  31  28  14   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.3074 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 115  99  91  70  66  52  38  31  26  14   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 1.3037 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 115  99  91  70  66  52  38  31  26  14   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.3046 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 116 115  99  93  52  38  35  31  14  12   8   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 115  99  93  83  61  52  32  31  21  14  12   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.3004 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 125 123 109  99  93  55  52  31  21  14   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 1.2973 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 125 123 109  99  93  55  52  31  21  14   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.2997 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 124 123  99  93  83  55  52  31  26  21  18   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.2929 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 125 123 109  99  93  55  52  31  21  14   8   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.2976 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 124 123  99  93  83  55  52  31  26  21  18   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.2891 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 124 123  99  93  83  55  52  31  26  21  18   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.2941 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 124 123  99  93  83  52  38  32  32  31  18   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.2925 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 118 115  99  93  52  38  32  31  14  10   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 118 115  99  93  52  38  32  31  14  10   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.2848 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 118 114 100  93  52  38  32  31  14  10   8   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.2874 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [143 123 118 100  93  83  55  52  31  31  23  14  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.2770 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 118 114 100  93  91  55  52  47  31  23  14  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.2825 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 118 105 100  93  83  55  52  47  35  31  23  10   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.2734 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 118 100  93  83  55  52  35  31  23  14  10   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.2741 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 121 109 105 101  93  53  52  49  31  23  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.2831 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 121 109 105 101  93  53  52  49  31  23  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.2744 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 121 100  93  80  53  52  37  31  23  14  10   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.2639 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 121 100  93  80  53  52  37  31  23  14  10   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.2657 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 120 100  93  80  53  52  37  31  23  14  10   7   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.2703 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 120 114 101  93  91  52  52  49  31  15  14  10   8   8   7   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.2665 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 120 110 100  93  91  52  52  49  31  23  14  10   8   7   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.2616 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 120 110 106  99  93  52  52  49  31  15  10   8   8   7   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.2548 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [128 120 106  99  93  86  52  52  47  35  31  15  10   8   7   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0111 - accuracy: 0.9978 - val_loss: 1.2796 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [175 117 100  99  98  82  52  38  38  31  31  11   8   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0710 - accuracy: 0.9911 - val_loss: 1.7743 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 119 118 103  93  88  51  51  31  26  14   7   6   4   4   4   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.1148 - accuracy: 0.9722 - val_loss: 1.1030 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [138 128 111 106  97  95  50  41  36  31  17  15   9   8   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0688 - accuracy: 0.9822 - val_loss: 1.1153 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [135 131 117 105  99  93  57  48  31  23  18  10   9   9   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0359 - accuracy: 0.9922 - val_loss: 1.0555 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 107 103  98  93  49  44  31  25  22  11  11  10  10   8   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0145 - accuracy: 0.9989 - val_loss: 1.0303 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 107 105  98  98  49  45  31  25  22  10   9   9   8   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.0249 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 106 102  97  49  45  31  25  22  11  10   9   5   4   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.0274 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.0337 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.0404 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.0468 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.0522 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.0573 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.0596 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.0634 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.0661 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.0703 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 109 106  97  97  51  45  31  25  22  11   9   9   5   4   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.0728 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 105 103  97  97  51  45  31  26  22  11  11   9   9   5   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.0748 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.0778 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.0804 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.0820 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0838 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0858 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 128 106 103  97  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0887 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 103  98  97  51  45  31  26  18  11  11   9   9   7   5   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0903 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 105 105  97  51  45  31  25  18  11  10   9   7   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.0964 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 105 105  97  51  45  31  25  18  11  10   9   7   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.0946 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 105 105  97  51  45  31  25  18  11  10   9   7   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.0965 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 105 105  97  51  45  31  25  18  11  10   9   7   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.1013 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 106 105 105  97  51  45  31  25  18  11  10   9   7   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1038 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 105 105 103  97  51  45  31  25  18  11  10   9   8   7   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1080 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 105 105 103  97  51  45  31  25  18  11  10   9   8   7   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1105 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 105 105 103  97  51  45  31  25  18  11  10   9   8   7   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1114 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 105 105 103  97  51  45  31  25  18  11  10   9   8   7   3   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1137 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [140 127 105 105 103  97  51  45  31  25  18  11  10   9   8   7   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1154 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [134 131 107 103 102  97  51  45  31  25  18  11  11  10   8   7   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1170 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [134 131 107 106 102  97  51  45  31  25  18  11  11  10   7   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1191 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [134 131 107 106 102  97  51  45  31  25  18  11  11  10   7   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1212 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [134 131 108 107 102  97  55  51  31  22  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1216 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 108 107 102  97  92  52  51  31  22  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1238 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 108 107 102  97  92  52  51  31  22  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1252 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 108 107 102  97  92  52  51  31  22  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1270 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 107 106 102  97  89  51  45  31  25  18  11  11  10   7   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1290 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 107 106 102  97  89  51  45  31  25  18  11  11  10   7   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1311 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 107 106 102  97  89  51  45  31  25  18  11  11  10   7   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1317 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 106 105  97  91  51  45  31  25  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1329 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 113 106 105  97  89  51  45  31  25  18  11  10   7   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1337 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 103  97  91  51  45  31  25  18  11  10   8   7   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1358 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 103  97  91  51  45  31  25  18  11  10   8   7   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1378 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 109 105 103  97  91  51  45  31  26  18  11  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1390 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 109 105 103  97  91  51  45  31  26  18  11  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1402 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 109 105 104  97  91  51  45  31  26  18  11  10   8   6   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1424 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 103  97  89  51  45  31  26  18  11  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1439 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 105  97  89  51  38  32  31  18  10  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1476 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 105  97  89  51  38  32  31  18  10  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1476 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 105  97  89  51  38  32  31  18  10  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1495 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 105  97  89  51  38  32  31  18  10  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1514 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 109 105 105  97  89  51  38  32  31  18  10  10   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1531 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 105 105 105  97  86  51  38  32  31  18  10  10  10   8   7   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1530 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 105 105 105  97  86  55  51  31  18  15  10  10  10   8   7   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.1570 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 107 105 105  97  90  55  51  31  18  15  10  10   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.1570 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 107 105 105  97  90  55  51  31  18  15  10  10   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.1579 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 107 105 105  97  90  55  51  31  18  15  10  10   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.1598 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 107 105 105  97  90  55  51  31  18  15  10  10   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.1617 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 107 105 105  97  90  55  51  31  18  15  10  10   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.1622 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 113 105 102  97  90  55  51  31  18  15   9   8   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.1637 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 113 105 102  97  90  55  51  31  18  15   9   8   8   7   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.1708 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116 106 102  97  86  55  51  31  20  15   9   8   8   6   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.1740 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116 106 102  97  86  55  51  31  20  15   9   8   8   6   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1688 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 114 105 102  97  90  55  51  31  18  15   9   8   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 113 105 103  97  90  55  51  31  18  15   9   8   8   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1714 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 113 103 100  97  89  55  51  31  18  15  10   9   8   7   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1718 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115 103 100  97  85  55  51  31  20  15  10   9   8   7   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1742 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115 103 100  97  85  55  51  31  20  15  10   9   8   7   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1760 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 100  99  97  93  55  51  31  20  15  10   9   8   7   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1767 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 100  99  97  93  55  50  31  20  15  10   9   8   7   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1775 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 101  99  97  93  55  50  31  20  15  10   9   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1787 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 101  99  97  93  50  37  33  31  20  10   9   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1805 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 101  99  97  93  50  37  33  31  20  10   9   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.1809 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 101  99  97  93  50  37  33  31  20  10   9   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.1816 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 101  99  97  93  50  37  33  31  20  10   9   8   6   6   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.1865 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 106  99  97  93  51  37  33  32  23   9   8   6   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.1848 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 106 103  97  89  51  37  33  32  23   9   8   6   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.1871 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 110  99  97  92  49  44  32  26  25   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1871 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 110  99  97  92  49  44  32  26  25   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1932 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111 110  99  97  92  49  44  32  26  25   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1892 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 110  99  97  92  49  44  32  26  25   9   9   5   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1903 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 107  99  97  92  49  44  32  26  25   9   9   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1922 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 107  99  97  92  49  44  32  26  25   9   9   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.1928 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 110  99  97  94  49  44  32  26  20   9   9   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.1967 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 112 110  99  97  94  49  44  32  26  20   9   9   8   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.1941 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 110  99  97  94  49  44  32  26  25   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.1973 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 110  99  97  94  49  44  32  26  25   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.1993 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 102  97  91  51  44  31  26  20   9   9   7   6   6   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.1985 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 102  97  91  51  44  31  26  20   9   9   7   6   6   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2024 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 111 105 102  97  91  51  44  31  26  20   9   9   7   6   6   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2037 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 113 111 103 102  97  49  44  31  26   9   9   9   6   3   3   2   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2035 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 113 110 102  97  95  51  44  31  26   9   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2055 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 113 104 100  97  95  51  44  31  26  10   9   9   9   6   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2045 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 113 113  99  97  95  51  44  31  26   9   9   9   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2074 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 109 108  99  97  95  51  44  31  26  10   9   9   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2091 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 116 109  99  97  95  51  44  31  26   9   9   8   6   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2080 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 116 109  99  97  95  51  44  31  26   9   9   8   6   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2094 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 116 112  99  97  95  51  44  31  23  11   9   9   6   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 112 108 103  98  91  49  44  31  23  11  10  10   9   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2107 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 112 108 103  98  91  49  44  31  23  11  10  10   9   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 112 108 103  98  91  49  44  31  23  11  10  10   9   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2128 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 112 110  98  94  82  50  44  31  23  14  13   9   9   8   6   5   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2145 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 112 110  98  94  82  50  44  31  23  14  13   9   9   8   6   5   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  98  94  82  50  44  31  23  13  10   9   8   6   5   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2142 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  98  94  82  50  44  31  23  13  10   9   8   6   5   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2147 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  98  94  82  50  44  31  23  13  10   9   8   6   5   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2148 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  98  94  82  50  44  31  23  13  10   9   8   6   5   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2156 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2167 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2163 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2190 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2186 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2191 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2213 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  99  98  88  50  44  31  23  10   9   8   6   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2235 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [185 117 115  98  98  88  50  44  31  23  10   9   8   7   5   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2180 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 117 115  98  92  88  50  44  31  23  13  13  10   9   7   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2194 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 117  98  92  88  50  44  31  23  13  10   9   8   7   5   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2243 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115 115 105  98  88  50  44  31  28  10   9   8   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2242 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115 115 105  98  88  50  44  31  28  10   9   8   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2234 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 114  98  92  88  50  45  31  28  13  10   9   8   7   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2248 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 114  98  92  88  50  44  31  29  13  10   9   8   7   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2263 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 119 118  98  92  88  50  50  31  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2243 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 119 118  98  92  88  51  50  30  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2253 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 118  98  92  88  51  51  30  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2265 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 118  98  92  88  51  51  30  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2251 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 118  98  92  88  51  51  30  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2247 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 118  98  92  88  51  51  30  23  13  10   9   7   3   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2249 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 112  98  92  88  51  51  30  23  13  11   9   9   4   3   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2261 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 118 112  98  92  88  51  51  30  23  13  11   9   9   4   3   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2256 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116 112  98  91  88  51  51  30  23  15  10   9   7   6   4   3   3\n",
      "   3   2]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2303 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 117 112  98  91  88  51  50  30  23  15  10   9   7   6   4   3   3\n",
      "   3   2]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2258 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 117 112  98  91  88  51  50  30  23  15  10   9   7   6   4   3   3\n",
      "   3   2]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2284 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 117 112  98  91  88  51  50  30  23  15  10   9   7   6   4   3   3\n",
      "   3   2]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2340 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 121 112  98  91  88  51  50  30  23  15  10   9   9   3   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2333 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115  98  91  88  62  59  51  50  30  23  15  10   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2309 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115  98  91  88  62  59  51  50  30  23  15  10   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2314 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115  98  91  88  62  59  51  50  30  23  15  10   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2309 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115  98  91  88  62  59  51  50  30  23  15  10   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2307 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 115  98  91  88  62  59  51  50  30  23  15  10   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2305 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 105  98  96  91  62  60  51  50  30  23  15  10  10   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2354 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 105  98  96  91  62  60  51  50  30  23  15  10  10   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2317 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 105  98  96  91  62  60  51  50  30  23  15  10  10   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2341 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111  98  91  87  62  60  51  50  30  23  15  14   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2298 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111  98  91  87  62  60  51  50  30  23  15  14   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2377 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111  98  92  87  62  59  51  50  30  23  15  14   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2416 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 111  98  92  87  62  59  51  50  30  23  15  14   9   9   3   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2337 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178  98  88  87  63  62  60  53  51  50  30  23  15  14   9   9   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.2412 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178  98  88  87  63  62  60  53  51  50  30  23  15  14   9   9   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.2378 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178  98  88  87  63  62  60  53  51  50  30  23  15  14   9   9   3   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.2391 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116  98  88  87  62  60  51  50  30  23  15  14   9   8   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.2420 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116  98  88  87  59  59  51  51  30  22  15  15   9   8   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.2379 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116  98  88  87  59  59  51  51  30  22  15  15   9   8   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.2446 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 116  98  88  87  60  59  51  50  30  22  15  15   9   8   5   3   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 0.9989 - val_loss: 1.1598 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [178 106 102  93  91  60  59  51  50  30  15  14  11  10   9   8   5   3\n",
      "   3   2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.1093 - accuracy: 0.9867 - val_loss: 1.1305 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [177 130 103 101  66  61  60  53  51  30  16  10  10   9   8   6   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0703 - accuracy: 0.9822 - val_loss: 1.4773 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 130 103  87  66  65  62  60  32  29  28  23   9   8   8   6   3   3\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0929 - accuracy: 0.9733 - val_loss: 1.0366 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 132 106 100  66  55  53  51  44  32  22  21  12   8   7   7   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0467 - accuracy: 0.9856 - val_loss: 1.1042 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [177 125 118 106  66  51  51  49  46  31  23  22   8   7   5   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0209 - accuracy: 0.9956 - val_loss: 1.0026 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 122 104  98  92  53  52  50  46  30  22  22   8   6   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0163 - accuracy: 0.9967 - val_loss: 0.9680 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [177 129 103  98  86  53  50  49  46  30  23  22   8   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.9707 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 134 103 100  84  53  50  50  42  32  23  22   8   6   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0121 - accuracy: 0.9989 - val_loss: 0.9570 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 134 103  98  86  53  50  49  42  32  23  22   9   8   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0115 - accuracy: 0.9989 - val_loss: 0.9610 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106 100  89  52  50  50  43  32  24  23   8   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0108 - accuracy: 0.9989 - val_loss: 0.9600 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106 100  89  52  50  50  43  32  24  23   8   6   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0106 - accuracy: 0.9989 - val_loss: 0.9594 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106  98  91  50  50  49  43  32  23  22   9   8   5   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0102 - accuracy: 0.9989 - val_loss: 0.9644 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106  98  91  50  50  49  43  32  23  22   9   8   5   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0102 - accuracy: 0.9989 - val_loss: 0.9637 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106  98  91  50  50  49  43  32  23  22   9   8   5   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.9732 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 127 106  98  91  50  50  49  43  32  23  22   9   8   5   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0100 - accuracy: 0.9989 - val_loss: 0.9729 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 129 106  98  94  50  50  49  43  32  23  14   8   8   8   5   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 0.9989 - val_loss: 0.9829 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "Fitted 20 clusters with distribution [176 129 106  98  94  50  50  49  43  32  23  14   8   8   8   5   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.9824 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89000 to 0.90000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_simBatched\n",
      "Fitted 20 clusters with distribution [176 129 106  98  94  50  50  49  43  32  23  14   8   8   8   5   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.9884 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 106 100  92  50  50  49  39  32  23  22   8   8   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.9981 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 105  98  94  53  50  50  39  32  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.0087 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 105  98  94  53  50  50  39  32  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.0152 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 105  98  93  53  50  50  43  30  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.0217 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 105  98  93  53  50  50  43  30  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.0247 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 105  98  93  53  50  50  43  30  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.0293 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 105  98  93  53  50  50  43  30  23  16   8   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.0335 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  74  55  54  50  50  43  30  23  23   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.0368 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  74  55  54  50  50  43  30  23  23   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.0419 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  74  55  54  50  50  43  30  23  23   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.0432 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  74  55  54  50  50  43  30  23  23   8   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.0472 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  74  55  54  51  50  43  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.0501 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  55  54  52  51  42  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.0505 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  54  54  52  51  43  30  24  22   8   8   3   2   2\n",
      "   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0518 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  54  54  52  51  43  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0528 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  54  54  52  51  43  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0555 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  55  54  52  51  42  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0556 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  55  54  52  51  42  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0609 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  55  54  52  51  42  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0618 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 105  98  93  73  55  54  52  51  42  30  24  22   8   8   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0634 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0648 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0763 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0712 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0691 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0768 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0762 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0781 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 102  98  93  54  52  51  42  30  24  21   8   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0792 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0792 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0796 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0820 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0862 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0853 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0868 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0928 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 102  98  93  54  52  52  46  30  24  21   8   8   4   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0913 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 108  98  93  52  50  48  46  30  24  22   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0930 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 108  98  93  52  50  48  46  30  24  22   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0923 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 108  98  93  52  50  48  46  30  24  14   8   8   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0960 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0963 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0984 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0983 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.0991 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1003 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1007 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1035 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1045 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.0998 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  49  46  30  24  21   8   8   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1024 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1062 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1072 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1085 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.1097 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 101  98  93  56  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1118 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 106  98  93  52  51  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1162 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 102  98  93  55  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1163 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 102  98  93  55  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1143 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 102  98  93  55  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1152 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 124 102  98  93  55  52  50  42  30  24  21   8   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1177 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  50  42  30  24  21   9   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1174 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  50  42  30  24  21   9   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1172 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  50  42  30  24  21   9   8   5   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1184 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  51  42  30  24  24   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1192 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  51  42  30  24  24   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1209 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 102  98  93  55  52  51  42  30  24  24   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1220 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 104  98  93  55  52  51  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1203 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 123 104  98  93  55  52  51  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1221 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1215 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1250 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1235 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1234 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1263 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1246 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  55  51  50  42  30  24  22   9   8   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.1198 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 104  98  93  52  50  42  37  34  30  22   9   8   8   4   3   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1330 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1303 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1310 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1328 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1320 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1308 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1322 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1324 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 104  66  52  50  41  36  33  30  22  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1312 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 124 103  66  52  50  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1328 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 124 103  52  50  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1333 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 124 103  52  50  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1329 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 124 103  52  50  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1350 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 124 103  52  50  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1370 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1365 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 124 103  52  50  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1366 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1362 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1372 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1389 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1375 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1381 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  51  51  43  41  36  33  30  23  11   9   8   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1375 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  52  51  43  41  36  33  30  23  11   9   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1396 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 123 103  51  51  43  41  36  33  30  23  11   9   8   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1405 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 128 103  52  51  43  41  36  33  30  23  11   8   6   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1399 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 128 103  52  51  43  41  36  33  30  23  11   8   6   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1500 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 128 103  51  51  43  41  36  33  30  23  12   8   6   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1479 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 128 103  51  51  43  41  36  33  30  23  12   8   6   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1474 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1480 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1468 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1460 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1502 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1481 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1492 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1491 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1481 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  36  33  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1474 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 102  51  50  46  45  36  34  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1502 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 102  51  50  46  45  36  34  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1495 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 102  51  50  46  45  36  34  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1502 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 102  51  50  46  45  36  34  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1499 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 102  51  50  46  45  36  34  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1528 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 123 102  51  50  46  43  36  34  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1453 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  37  32  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1559 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 119 103  51  50  46  45  37  32  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1583 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 122 103  51  50  45  42  37  32  30  23  12   8   6   4   4   4\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1578 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 122 103  93  50  43  37  32  30  23  12   8   6   4   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1536 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 122 103  93  50  43  37  32  30  23  12   8   6   4   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1594 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 126 103  51  50  43  42  37  32  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1599 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 126 103  51  50  43  42  37  32  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1580 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 126 103  51  50  43  42  37  32  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1607 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 122 103  93  50  43  37  32  30  23  12   8   6   4   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1598 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 126 103  51  50  43  42  37  32  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1591 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 126 103  51  50  43  42  37  32  30  23  12   8   6   4   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1603 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1597 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1627 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1616 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1603 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1642 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 103  93  50  44  37  32  30  22  12   8   6   4   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1640 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121  98  94  50  44  37  32  30  22  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1651 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121  98  94  50  44  37  32  30  22  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1691 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 106  92  50  44  37  32  30  16  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1654 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 106  92  50  44  37  32  30  16  12   8   8   6   4   4   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1704 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 106  92  50  44  37  32  30  16  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1702 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 106  92  50  44  37  32  30  16  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1735 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 121 106  92  50  44  37  32  30  16  12   8   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1690 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  92  51  43  37  32  30  16  12  10   8   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1698 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  92  51  43  37  32  30  16  12  10   8   8   4   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1792 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  92  50  43  37  32  30  16  12  10   8   8   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1697 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  92  50  43  37  32  30  16  12  10   8   8   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1698 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  92  50  43  37  32  30  16  12  10   8   8   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1763 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 118 106  57  52  50  44  37  36  30   8   8   8   6   4   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1787 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 120 106  55  52  50  44  37  36  30   8   8   8   6   4   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1738 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 120 106  55  52  50  44  37  36  30   8   8   8   6   4   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1778 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 148 120 106  55  52  50  44  37  36  30   8   8   8   6   4   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1864 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 120 106  52  51  44  43  37  36  30  16   8   8   8   6   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1827 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 117 106  52  50  44  43  37  36  30  16  10   8   8   8   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1872 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 116 106  52  50  45  43  37  36  30  16  10   9   8   6   4   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1841 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 117 108  52  50  44  43  37  30  30  16  12  10   9   8   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1848 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 117 108  52  50  44  43  37  30  30  16  12  10   9   8   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1880 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 120 108  52  50  44  43  37  30  30  16  12   9   8   6   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 0.9978 - val_loss: 1.1239 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 118 110  52  50  44  42  37  30  28  16  12  10   9   8   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0244 - accuracy: 0.9978 - val_loss: 1.1221 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 118 113  66  52  50  45  42  32  26  18   8   8   6   6   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0479 - accuracy: 0.9922 - val_loss: 1.3588 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 128 108 104  67  53  51  51  46  30  26  22  13   8   5   4   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.1176 - accuracy: 0.9733 - val_loss: 1.0054 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 148 113 112 107  52  51  49  31  16   9   8   8   7   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0727 - accuracy: 0.9800 - val_loss: 1.2311 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 119 107  51  51  47  41  39  31  31  17   9   8   8   6   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0526 - accuracy: 0.9878 - val_loss: 1.1700 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 124 109  83  66  51  42  35  31  16  12   8   6   4   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0276 - accuracy: 0.9922 - val_loss: 1.0726 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 116 115 103  66  51  39  31  31  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.0671 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 121 114  96  66  51  41  31  30  12   9   8   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.0835 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 128 120 116  96  66  51  38  33  31  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.0875 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 131 120 116  93  66  51  38  33  31  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.0927 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 131 120 116  93  66  51  38  33  31  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.1001 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 131 129 111  98  66  51  34  34  31  12   8   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1015 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 131 129 111  94  66  51  38  34  31  12   8   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1116 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 131 130 111  94  66  51  39  32  31  12   8   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1187 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 131 130 111  94  66  51  39  32  31  12   8   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1250 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 118 115 101  93  51  39  31  15  12   9   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1313 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 118 115 101  93  51  39  31  15  12   9   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1348 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 118 115 101  93  51  39  31  15  12   9   5   3   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1375 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  20  12   9   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1409 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  20  12   9   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1431 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 121 116  97  66  51  39  31  31  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1489 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  14  12   9   7   5   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1535 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  14  12   9   7   5   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1564 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  14  12   9   7   5   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1620 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  14  12   9   7   5   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1672 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 116 100  93  66  51  49  39  31  14  12   9   7   5   3   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1711 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 121 115 115 100  93  51  39  31  15  12   9   6   5   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1729 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 120 117 101  95  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1771 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 101  94  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1799 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 100  79  66  51  49  39  31  17  15  12   9   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1819 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 100  79  66  51  49  39  31  17  15  12   9   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1830 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 100  79  66  51  49  39  31  17  15  12   9   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1871 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 100  79  66  51  49  39  31  17  15  12   9   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1915 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 101  94  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1911 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 101  94  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1971 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 101  94  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1983 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 101  94  66  51  49  39  31  15  12   9   5   4   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2049 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 117 100  79  66  51  49  39  31  17  15  12   9   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2039 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 116 100  94  66  51  49  39  31  17  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2064 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 116 100  94  66  51  49  39  31  17  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2134 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2148 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2176 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2197 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2204 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2247 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115 113 100  94  51  39  31  15  12   9   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2295 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2315 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2348 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 113 100  94  66  51  49  39  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2395 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 113 103 100  66  51  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2423 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 113 103 100  66  51  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2428 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 113 103 100  66  51  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2441 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 113 103 100  66  51  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2461 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 120 113 103 100  66  51  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2508 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 118 115 100  98  88  52  51  31  15  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2522 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 113 103 100  66  52  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2587 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 113 103 100  66  52  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2591 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 113 103 100  66  52  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2588 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 113 103 100  66  52  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2586 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 116 103 100  66  52  49  35  31  15  12   6   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2602 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 113 103 100  66  52  49  35  31  15  12   9   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2621 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 116 103 100  66  52  49  35  31  15  12   6   5   4   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2637 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 116 101  87  66  52  49  35  31  15  15  12   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2647 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 119 116 101  87  66  52  49  35  31  15  15  12   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2671 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115 102  87  66  51  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2682 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115 102  87  66  51  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2719 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2711 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2739 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2757 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2771 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2781 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2801 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2825 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 102  87  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2854 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 101  88  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2883 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 101  88  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2859 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 101  88  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2877 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 114 101  88  66  52  49  35  31  15  15  12   6   6   5   5   4\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2920 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 116 114  87  66  51  40  32  31  15  12   6   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2923 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 116 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2930 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 116 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2966 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 116 114  88  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2964 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 116 114  88  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3000 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 116 114  88  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3027 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 129 116 114  88  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3011 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 116 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3034 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 116 113  87  66  51  41  31  31  15  12   7   6   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3050 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 113  87  66  51  41  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3059 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 113  87  66  51  41  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3057 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 113  87  66  51  41  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3078 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3099 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3101 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 114  87  66  51  40  31  31  15  12   7   6   5   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3134 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 115  87  66  51  40  31  31  15  12   7   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3125 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 115  87  66  51  40  31  31  15  12   7   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3132 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 107  87  66  51  40  31  31  15  12   9   7   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3159 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 107  87  66  51  40  31  31  15  12   9   7   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 107  87  66  51  40  31  31  15  12   9   7   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3162 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 115 107  87  66  51  40  31  31  15  12   9   7   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3213 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3221 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3205 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3258 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3288 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3407 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3374 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3359 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3342 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3380 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 118 115  97  85  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3397 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3384 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3386 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 116 115  97  87  66  51  49  40  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3422 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 115 113  97  87  66  51  47  45  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3425 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 115 113  97  87  66  51  47  45  31  15  15  12   6   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3425 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 119 109  97  87  66  51  47  45  31  15  14  12   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3444 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 119 111  97  87  66  51  47  43  31  15  14  12   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3464 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 119 111  97  87  66  51  47  43  31  15  14  12   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3456 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 119 111  97  87  66  51  47  43  31  15  14  12   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3479 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 110  87  51  46  43  31  30  14  12   9   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3495 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 110  87  51  46  43  31  30  14  12   9   9   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3495 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3508 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3498 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3563 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3562 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3572 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 111  87  51  46  43  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3569 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 109  87  51  46  45  31  30  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3588 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 111  87  55  51  46  31  17  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3605 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 111  87  55  51  46  31  17  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3641 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 111  87  55  51  46  31  17  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3649 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 111  87  55  51  46  31  17  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3675 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 111  87  55  51  46  31  17  14  12   8   6   5   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3658 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 108  87  55  51  46  31  17  14  13  12   8   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3673 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 108  87  55  51  46  31  17  14  13  12   8   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3660 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 106  87  55  51  46  31  17  16  13  12   8   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3715 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 106  87  55  51  46  31  17  16  13  12   8   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3750 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 106  87  55  51  46  31  17  16  13  12   8   6   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3766 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 111  93  51  50  46  31  17  16  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3735 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 111  93  51  50  46  31  17  16  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3763 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  92  55  51  46  31  16  14  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3827 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  92  55  51  46  31  16  14  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3863 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  92  55  51  46  31  16  14  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3833 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  92  55  51  46  31  16  14  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3821 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  92  55  51  46  31  16  14  12   8   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3848 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3872 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3889 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3926 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3897 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3923 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111 101  53  46  31  31  29  22   8   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.4088 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3931 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 111  85  53  46  31  31  29  22  16   8   6   5   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3949 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 115  85  53  46  31  31  29  22  15   6   5   5   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3993 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 116 115  85  53  46  31  31  29  22  15   6   5   5   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4050 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3959 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4114 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4114 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4151 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 114  85  53  53  31  31  29  22  15   6   5   4   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.4092 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 113  85  53  53  51  30  17  15  12   6   5   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4145 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 118 115  85  53  51  46  31  16  15  12   6   6   5   5   4   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4186 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 115  90  53  51  46  31  16  15  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4109 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 115  90  53  51  46  31  16  15  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4176 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 106  90  53  51  46  31  22  16  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4217 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 106  90  53  51  46  31  22  16  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4282 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 106  90  53  51  46  31  22  16  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4286 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 106  90  53  51  46  31  22  16  12   6   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4240 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 106  85  53  53  51  30  22  17  12   6   5   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4276 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 106  85  53  53  51  31  22  17  12   6   5   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4221 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 106  85  53  53  51  30  22  17  12   6   5   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4304 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4299 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4334 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4388 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4256 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 115  90  53  51  46  30  16  15  12   6   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4450 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 117 106  90  53  51  46  31  22  17  12   7   6   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4440 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4449 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 115 102  90  53  53  51  30  17  15  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4378 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4423 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4500 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 106 104  90  53  53  51  30  22  17  12  11   6   5   5   4   3\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4586 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 106  90  53  51  46  31  22  16  12  11   6   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4508 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 109 106  90  53  51  46  31  22  17  12  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4469 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 109 106  90  53  51  46  31  22  17  12  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4537 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 111 106  90  53  51  46  31  22  16  12  11   6   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4440 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 109 106  90  53  51  46  31  22  17  12  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4442 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 108 106  89  53  51  46  30  22  17  12  11   7   6   5   5   4\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4473 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 109 106  90  53  51  46  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4471 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 109 106  90  53  51  46  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4563 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 108 106  90  53  51  47  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4523 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 108 106  90  53  51  47  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4489 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 106  90  53  51  43  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.4551 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 106  90  53  51  43  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4360 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 106  90  53  51  43  31  22  16  13  11   7   6   5   4   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4615 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 103  90  53  51  50  31  20  16  13  11   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4548 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 103  90  53  51  50  31  20  16  13  11   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4571 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 103  90  53  51  50  31  20  16  13  11   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4598 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 103  90  53  51  50  31  20  16  13  11   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4585 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 112 103  90  53  51  50  31  20  16  13  11   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4846 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 103  90  53  51  46  31  20  16  13   7   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4754 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 120 103  90  53  51  46  31  20  16  13   7   6   6   5   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4349 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 123 103  90  53  51  43  30  20  16  13   7   6   6   5   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4481 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 123 103  98  94  90  53  51  30  20  16  13   7   6   6   5   4   3\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4624 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 119 103  92  53  52  46  30  26  20   7   6   6   5   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4534 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 149 118 103  92  53  53  52  30  26  20   6   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0292 - accuracy: 0.9956 - val_loss: 1.2493 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 149 119 104 104  53  53  30  30  24  22  12   6   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0799 - accuracy: 0.9811 - val_loss: 1.5199 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 120 116  75  67  61  31  27  27  24  16  12   7   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.1706 - accuracy: 0.9644 - val_loss: 1.4143 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [226 126 113  88  68  65  51  50  31  22  14  10   9   8   6   4   4   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0794 - accuracy: 0.9744 - val_loss: 1.1504 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 128 119 115  90  66  50  49  28  18  14  12   9   8   5   4   3   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0421 - accuracy: 0.9889 - val_loss: 1.0553 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 132 126 117  83  66  51  49  30  19  11  10  10   6   5   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0227 - accuracy: 0.9944 - val_loss: 0.9684 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [228 125 119 117  67  53  51  49  30  15  13  10   8   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.9703 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 134 126 111  85  66  51  49  30  25  13  10   8   5   4   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.9732 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 135 124 113  87  66  51  49  30  25  11   9   8   5   4   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9840 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [228 127 124 120  66  53  53  49  30  15  10   9   4   2   2   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 0.9989 - val_loss: 0.9988 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 124 120 103  66  53  49  30  15  13   9   4   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 135 127 124 103  66  53  49  30  10   9   4   4   2   2   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 124 120 103  66  53  49  30  15  13   9   4   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.0064 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 127 124 120 103  66  53  49  30  15  10   9   4   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0296 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 122 121 103  66  53  49  30  15  10   9   8   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.0402 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 122 121 103  66  53  49  30  15  10   9   8   4   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.0393 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 124 121 101  66  57  53  49  30  15  14  11   9   8   4   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.0510 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 124 117 101  66  57  53  51  30  15  14  12   9   8   4   2   2   2\n",
      "   2   2]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 124 121 101  66  57  53  49  30  15  14  11   9   8   4   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.0671 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 149 124 119  66  54  49  30  18  15  11   9   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 149 124 119  66  54  49  30  18  15  11   9   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0767 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 149 124 119  66  54  49  30  18  15  11   9   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.0772 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 149 124 119  66  54  49  30  18  15  11   9   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.0911 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [229 149 124 119  66  54  49  30  18  15  11   9   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.0907 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 122 119  85  66  51  49  30  18  15  11  10   8   8   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.0946 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 122 119  85  66  51  49  30  18  15  11  10   8   8   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.0985 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 132 124 119  85  66  51  49  30  16  15  11   8   8   2   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.1032 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 132 124 119  84  66  51  49  30  16  15  11   8   8   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1043 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 132 124 119  82  66  51  49  30  16  15  11   8   8   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1063 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 131 124 119  80  66  54  49  30  16  15  11   8   8   3   2   2   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1102 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 119  82  66  54  49  30  16  15  11   8   8   3   2   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1046 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 121  82  66  54  49  30  16  15  10   8   8   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1118 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 121  82  66  54  49  30  16  15  10   8   8   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1290 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 112  82  66  54  35  31  30  16  11   8   8   7   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1165 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 119  82  66  54  35  31  30  16  10   8   8   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1214 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 129 124 119  82  66  54  35  31  30  16  10   8   8   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1251 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 124 120 119  82  66  54  35  31  30  16  10   9   9   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1288 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 120 117  89  66  54  35  31  30  13  10   9   9   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1299 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 120 117  89  66  54  35  31  30  13  10   9   9   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1352 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 120 117  89  66  54  35  31  30  13  10   9   9   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1356 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 120 117  89  66  54  35  31  30  13  10   9   9   8   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1360 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 130 124 117  89  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1386 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1437 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1460 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1455 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1475 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1532 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1510 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1557 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1712 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1644 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1622 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1662 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  96  65  54  35  31  30  13  10   8   6   3   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1694 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1666 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1614 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1659 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124 123 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 117 111  96  65  54  35  31  30  15  10  10   9   8   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1686 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 117 111  96  65  54  35  31  30  15  10  10   9   8   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1722 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 117 111  96  65  54  35  31  30  15  10  10   9   8   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1767 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 117 111  96  65  54  35  31  30  15  10  10   9   8   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1772 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 117 111  96  65  54  35  31  30  15  10  10   9   8   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1783 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 117 115  96  65  54  35  31  30  13  11  10   8   6   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1778 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 117 115  96  65  54  35  31  30  13  11  10   8   6   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1811 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 119 115  96  65  54  49  30  15  13  11  10   8   6   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1807 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 119  98  65  54  49  30  15  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1857 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 119  98  65  54  49  30  15  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1865 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 119  98  65  54  49  30  15  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1842 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 119  98  65  54  49  30  15  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1863 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 119  98  65  54  49  30  15  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1887 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1881 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1902 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1914 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1936 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1925 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1944 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 122 117  98  65  54  35  31  30  13  10   8   6   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1967 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 117 108  98  65  54  35  31  30  15  13  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1952 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 117 108  98  65  54  35  31  30  15  13  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1987 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 110 108  98  65  54  35  31  30  20  15  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1995 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 110 108  98  65  54  35  31  30  20  15  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2008 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 110 108  98  65  54  35  31  30  20  15  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2023 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 110 108  98  65  54  35  31  30  20  15  10   8   6   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2008 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 123 110  93  65  54  35  31  30  20  10   8   6   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2042 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 123 110  93  65  54  35  31  30  20  10   8   6   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2050 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 123 110  93  65  54  35  31  30  20  10   8   6   5   3   2   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2057 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 110 109  93  65  54  35  31  31  20  15  10   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2060 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2050 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2038 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2096 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1950 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2051 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2117 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2134 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2105 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2153 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2158 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 114 109  93  65  54  35  31  31  15  14  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2126 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 115 109  93  65  54  35  31  31  15  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2172 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 115 109  93  65  54  35  31  31  15  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2151 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 115 109  93  65  54  35  31  31  15  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2183 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 125 115 109  93  65  54  35  31  31  15  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2210 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 115 109  90  65  54  35  31  31  16  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2221 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 115 109  90  65  54  35  31  31  16  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2230 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [177 125 115 109  90  65  54  35  31  31  16  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2245 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2240 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2256 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2268 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 126 115 109  90  65  54  35  31  31  16  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2256 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2285 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 126 115 109  90  65  54  35  31  31  16  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2304 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2191 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2269 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [176 125 125 115  90  65  54  35  31  31  13  12   8   6   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2284 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2350 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2364 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2386 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2382 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2463 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 112  90  65  54  35  31  31  15  11  10   9   8   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2444 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 126 115 110  91  65  54  35  31  31  15  13  12   8   6   5   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2517 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2528 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2553 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2496 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2474 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2459 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 115 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2498 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 116 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2525 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 116 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2509 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 116 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2554 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 118 116 110  91  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2742 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2555 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2510 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2610 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2575 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2600 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2603 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2603 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 116  86  65  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2636 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  35  31  31  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2648 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2626 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2678 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2641 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2677 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2704 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2739 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115  86  66  54  49  31  17  15  13  12   9   8   8   5   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2815 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 130 118 116  86  66  54  49  31  17  13  12   8   8   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2754 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 130 118 116  86  66  54  49  31  17  13  12   8   8   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2742 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 130 118 116  86  66  54  49  31  17  13  12   8   8   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2651 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 130 118 116  86  66  54  49  31  17  13  12   8   8   5   5   4   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2716 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 126 116  91  66  54  49  31  17  13  11   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2755 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2732 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2740 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2774 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2807 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 116 111 106  66  54  49  31  17  13  11   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2822 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 116 111 106  66  54  49  31  17  13  11   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2805 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2824 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2892 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2818 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2800 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2894 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2997 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2877 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2901 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2887 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2919 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2876 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2880 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  49  31  17  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2980 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 116 110 109  65  54  49  31  17  11  10   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2998 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 116 110 109  65  54  49  31  17  11  10   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2916 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 116 110 109  65  54  49  31  17  11  10   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2979 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 117 110 109  65  54  53  31  23  10   8   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2898 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 110 109  65  54  34  31  30  11  10   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2939 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 110 109  65  54  34  31  30  11  10   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2956 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 122 118 118 107  66  54  34  31  30  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2986 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 122 118 118 107  66  54  34  31  30  11   8   8   5   5   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2975 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  34  31  30  11   8   8   5   5   4   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3027 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  51  31  21   8   8   5   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2938 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  51  26  21   9   8   8   5   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2972 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 118 116 104  66  54  51  26  21   9   8   8   5   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2985 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3081 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2960 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3070 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3011 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3006 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.2978 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 128 118 116 104  65  54  51  30  21   8   8   5   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3200 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 118 116 115 104  65  54  51  30  21  15   8   8   7   5   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3503 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 121 119 114 103  65  54  48  30  26  15   8   5   5   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2619 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 126 120 118  94  66  54  48  30  21  13   8   8   5   4   3   3   2\n",
      "   2   1]\n",
      "29/29 - 1s - loss: 0.0483 - accuracy: 0.9911 - val_loss: 1.5963 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [173 116 114 106  94  71  67  54  30  22  16   8   8   7   5   4   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.1321 - accuracy: 0.9700 - val_loss: 1.5930 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 128 122 106  82  67  62  47  30  22  16  10   9   7   6   6   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.1288 - accuracy: 0.9711 - val_loss: 1.1125 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [188 173 121 115  52  51  48  42  31  26  26   6   5   5   3   2   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0703 - accuracy: 0.9778 - val_loss: 1.1515 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [175 124  99  97  91  66  52  50  45  31  22  19  10   6   4   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0526 - accuracy: 0.9856 - val_loss: 0.9281 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [173 125 117  99  94  94  50  45  31  27  15   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0339 - accuracy: 0.9878 - val_loss: 0.7385 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 107  93  68  67  64  53  53  31  30  11   7   6   5   4   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.7745 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 113  93  68  67  66  52  48  31  28  11   7   6   5   4   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.8240 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 120 109  91  68  67  51  31  28  11   7   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 0.9989 - val_loss: 0.8622 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 113 109  93  68  67  53  31  28  11   7   6   5   4   4   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 0.9989 - val_loss: 0.8615 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 109  93  68  52  51  31  28  23  11   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 0.9989 - val_loss: 0.8744 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113  93  90  68  52  50  32  31  29  23  11   6   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 0.9989 - val_loss: 0.8834 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 110  93  68  52  50  31  28  23  11   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 0.9989 - val_loss: 0.8989 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 123 117 110 100  92  50  47  31  23  11   6   4   4   2   2   1   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 0.9989 - val_loss: 0.9115 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 110 100  93  50  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 0.9989 - val_loss: 0.9236 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 110 100  93  50  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 0.9989 - val_loss: 0.9504 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 110 100  93  50  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 0.9989 - val_loss: 0.9536 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 110 100  93  50  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.9640 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 110 100  93  50  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.9804 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 109 100  93  51  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.9904 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 113 109 100  93  51  48  31  23  11   9   6   4   4   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 0.9989 - val_loss: 0.9960 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 109  98  93  51  48  30  23  11   6   4   4   3   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.9969 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 109  98  93  51  48  30  23  11   6   4   4   3   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 0.9989 - val_loss: 1.0074 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 109  98  93  51  48  30  23  11   6   4   4   3   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 0.9989 - val_loss: 1.0087 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 115 109  98  93  53  48  30  23  11   7   6   4   4   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 0.9989 - val_loss: 1.0171 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 122 117 109  98  93  51  48  30  23  11   6   4   4   3   2   2   1\n",
      "   1   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0062 - accuracy: 0.9989 - val_loss: 1.0165 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 115 109  98  93  53  48  30  23  11   7   6   4   4   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.0440 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 117 115 109  98  93  53  48  30  23  11   7   6   4   4   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 0.9989 - val_loss: 1.0184 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 117 115 109  66  53  48  30  23  11   7   6   4   4   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 0.9989 - val_loss: 1.0528 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 118 115 113  66  51  48  30  22  10   7   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 1.0473 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 116 115 113  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 1.0128 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 116 115 113  66  53  48  30  23  11   6   6   4   3   2   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 0.9989 - val_loss: 1.0469 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 125 117 114 113  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 0.9989 - val_loss: 1.0601 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 116 115 114  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 0.9989 - val_loss: 1.0270 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 116 115 114  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 0.9989 - val_loss: 1.0728 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 116 115 114  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 0.9989 - val_loss: 1.0397 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 116 115 114  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 1.0584 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 116 115 114  66  53  48  30  23  10   6   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 0.9989 - val_loss: 1.0523 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  48  30  23  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.0785 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  48  30  23  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 1.0470 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  48  30  23  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.0844 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  48  30  23  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 0.9989 - val_loss: 1.0870 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 114 100  66  50  48  30  22  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 0.9989 - val_loss: 1.0770 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  48  30  23  21  10   6   4   3   3   2   1\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.0823 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  46  30  23  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.0933 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  46  30  23  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.0981 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  46  30  23  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.0897 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  46  30  23  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1019 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 116  94  66  53  46  30  23  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1098 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "Fitted 20 clusters with distribution [174 124 120 117  94  66  53  35  33  30  21  10   6   5   3   3   2   2\n",
      "   1   1]\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1185 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#For a trained LSwFW_model (n_epochs=200; shuffling) \n",
    "checkpoint_path=os.path.join(\"210210_TrainingLocalitySensitivewFW\",\n",
    "                                      \"LocalitySensitivewFW_label\" )\n",
    "LSwFW_model.load_weights(checkpoint_path)\n",
    "\n",
    "simbatched_checkpoint_path=os.path.join(\"210210_TrainingLocalitySensitivewFW\",\n",
    "                                      \"LocalitySensitivewFW_simBatched\" )\n",
    "try:\n",
    "    os.mkdir(simbatched_checkpoint_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Set callbacks\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=simbatched_checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',\n",
    "                     \n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "csv_filename = os.path.join(simbatched_checkpoint_path,\n",
    "                            \"training_log_simBatched_lr0_001.csv\"\n",
    "                            )\n",
    "csvlogger_callback = tf.keras.callbacks.CSVLogger(filename=csv_filename, append=True)\n",
    "\n",
    "#Assign learning rate\n",
    "LSwFW_model.optimizer.lr.assign(0.001)\n",
    "\n",
    "n_epoch=1000\n",
    "for i in range(n_epoch):\n",
    "\n",
    "    #Set training tensors\n",
    "    attentions=LSwFW_model.layers[1](train_tensor).numpy()\n",
    "    simbatched=SimilarityBatchingDataset(\n",
    "        train_tensor,\n",
    "        train_targets,\n",
    "        attentions,\n",
    "    )\n",
    "    rearranged_train_tensor, rearranged_train_targets=simbatched.get_rearranged_tensor()\n",
    "    \n",
    "    #Fit model\n",
    "    LSwFW_model.fit(rearranged_train_tensor, \n",
    "                rearranged_train_targets,\n",
    "                epochs=1,\n",
    "                batch_size=n_batch,\n",
    "                validation_data=(test_tensor, test_targets),\n",
    "                shuffle=True,\n",
    "                verbose=2, \n",
    "                callbacks=[csvlogger_callback,\n",
    "                           cp_callback\n",
    "                          ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0545 - accuracy: 0.9867 - val_loss: 0.9221 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_simBatched\n",
      "29/29 - 1s - loss: 0.0433 - accuracy: 0.9933 - val_loss: 0.9312 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0396 - accuracy: 0.9933 - val_loss: 0.9443 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0388 - accuracy: 0.9933 - val_loss: 0.9630 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0372 - accuracy: 0.9933 - val_loss: 0.9682 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0357 - accuracy: 0.9956 - val_loss: 0.9714 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0341 - accuracy: 0.9944 - val_loss: 0.9867 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0335 - accuracy: 0.9956 - val_loss: 0.9884 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0337 - accuracy: 0.9967 - val_loss: 0.9938 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0328 - accuracy: 0.9967 - val_loss: 0.9989 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0326 - accuracy: 0.9967 - val_loss: 1.0052 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0324 - accuracy: 0.9967 - val_loss: 1.0044 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0319 - accuracy: 0.9967 - val_loss: 1.0076 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0317 - accuracy: 0.9967 - val_loss: 1.0144 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0312 - accuracy: 0.9967 - val_loss: 1.0232 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0306 - accuracy: 0.9967 - val_loss: 1.0275 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0302 - accuracy: 0.9978 - val_loss: 1.0315 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0299 - accuracy: 0.9967 - val_loss: 1.0312 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0310 - accuracy: 0.9956 - val_loss: 1.0366 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0313 - accuracy: 0.9967 - val_loss: 1.0342 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0294 - accuracy: 0.9989 - val_loss: 1.0437 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0290 - accuracy: 0.9978 - val_loss: 1.0508 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0296 - accuracy: 0.9967 - val_loss: 1.0502 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0313 - accuracy: 0.9967 - val_loss: 1.0508 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0279 - accuracy: 0.9989 - val_loss: 1.0668 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0275 - accuracy: 0.9989 - val_loss: 1.0691 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0269 - accuracy: 0.9989 - val_loss: 1.0761 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0266 - accuracy: 0.9989 - val_loss: 1.0803 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0264 - accuracy: 0.9989 - val_loss: 1.0818 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0261 - accuracy: 0.9989 - val_loss: 1.0816 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0272 - accuracy: 0.9967 - val_loss: 1.0825 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0257 - accuracy: 0.9989 - val_loss: 1.0843 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0255 - accuracy: 0.9989 - val_loss: 1.0860 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0252 - accuracy: 0.9989 - val_loss: 1.0883 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0252 - accuracy: 0.9989 - val_loss: 1.0928 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0250 - accuracy: 0.9989 - val_loss: 1.0947 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0247 - accuracy: 0.9989 - val_loss: 1.0983 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0246 - accuracy: 0.9989 - val_loss: 1.1032 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0246 - accuracy: 0.9989 - val_loss: 1.1093 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0244 - accuracy: 0.9989 - val_loss: 1.1134 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0244 - accuracy: 0.9989 - val_loss: 1.1175 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0245 - accuracy: 0.9989 - val_loss: 1.1187 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0241 - accuracy: 0.9989 - val_loss: 1.1226 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0241 - accuracy: 0.9989 - val_loss: 1.1248 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0242 - accuracy: 0.9989 - val_loss: 1.1315 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0238 - accuracy: 0.9989 - val_loss: 1.1337 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0235 - accuracy: 0.9989 - val_loss: 1.1381 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0236 - accuracy: 0.9989 - val_loss: 1.1414 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0234 - accuracy: 0.9989 - val_loss: 1.1437 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0235 - accuracy: 0.9989 - val_loss: 1.1456 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0233 - accuracy: 0.9989 - val_loss: 1.1485 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0230 - accuracy: 0.9989 - val_loss: 1.1520 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0229 - accuracy: 0.9989 - val_loss: 1.1550 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0230 - accuracy: 0.9989 - val_loss: 1.1584 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0228 - accuracy: 0.9989 - val_loss: 1.1632 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0228 - accuracy: 0.9989 - val_loss: 1.1664 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0225 - accuracy: 0.9989 - val_loss: 1.1681 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0227 - accuracy: 0.9989 - val_loss: 1.1696 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0225 - accuracy: 0.9989 - val_loss: 1.1744 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0225 - accuracy: 0.9989 - val_loss: 1.1793 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0224 - accuracy: 1.0000 - val_loss: 1.1820 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0222 - accuracy: 0.9989 - val_loss: 1.1843 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0222 - accuracy: 0.9989 - val_loss: 1.1886 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0220 - accuracy: 0.9989 - val_loss: 1.1906 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0221 - accuracy: 0.9989 - val_loss: 1.1933 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0218 - accuracy: 1.0000 - val_loss: 1.1975 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0220 - accuracy: 0.9989 - val_loss: 1.1992 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0218 - accuracy: 0.9989 - val_loss: 1.2040 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0216 - accuracy: 0.9989 - val_loss: 1.2057 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0217 - accuracy: 0.9989 - val_loss: 1.2086 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0215 - accuracy: 0.9989 - val_loss: 1.2111 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0214 - accuracy: 1.0000 - val_loss: 1.2122 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 1.2185 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0214 - accuracy: 0.9989 - val_loss: 1.2189 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0212 - accuracy: 0.9989 - val_loss: 1.2236 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0304 - accuracy: 0.9967 - val_loss: 1.1965 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 1.2060 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0214 - accuracy: 0.9989 - val_loss: 1.2209 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0213 - accuracy: 1.0000 - val_loss: 1.2255 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0210 - accuracy: 1.0000 - val_loss: 1.2288 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0207 - accuracy: 0.9989 - val_loss: 1.2315 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0206 - accuracy: 0.9989 - val_loss: 1.2346 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 1.2361 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0203 - accuracy: 1.0000 - val_loss: 1.2374 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.2379 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.2400 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0201 - accuracy: 1.0000 - val_loss: 1.2416 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0199 - accuracy: 1.0000 - val_loss: 1.2429 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0200 - accuracy: 1.0000 - val_loss: 1.2454 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0197 - accuracy: 1.0000 - val_loss: 1.2476 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0197 - accuracy: 1.0000 - val_loss: 1.2502 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.2503 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.2528 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0194 - accuracy: 1.0000 - val_loss: 1.2530 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0193 - accuracy: 1.0000 - val_loss: 1.2554 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.2580 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.2594 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.2595 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.2621 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.2632 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.2644 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.2690 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0188 - accuracy: 1.0000 - val_loss: 1.2693 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0193 - accuracy: 0.9989 - val_loss: 1.2643 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0208 - accuracy: 0.9989 - val_loss: 1.2711 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 1.2770 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 1.2765 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 1.2808 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.2804 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.2843 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.2869 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.2873 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.2878 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0177 - accuracy: 1.0000 - val_loss: 1.2884 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0176 - accuracy: 1.0000 - val_loss: 1.2900 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0176 - accuracy: 1.0000 - val_loss: 1.2909 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0175 - accuracy: 1.0000 - val_loss: 1.2921 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0174 - accuracy: 1.0000 - val_loss: 1.2930 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 1.2966 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0172 - accuracy: 1.0000 - val_loss: 1.2975 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.2949 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0170 - accuracy: 1.0000 - val_loss: 1.2992 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.3014 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.3016 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0168 - accuracy: 1.0000 - val_loss: 1.3014 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0167 - accuracy: 1.0000 - val_loss: 1.3027 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0166 - accuracy: 1.0000 - val_loss: 1.3045 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.3054 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 1.3093 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 1.3085 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.3121 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.3093 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.3111 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 1.3134 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.3116 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 1.3154 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.3160 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0156 - accuracy: 1.0000 - val_loss: 1.3175 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.3183 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.3195 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.3142 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.3214 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.3207 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.3224 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 1.3258 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.3259 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 1.3281 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.3292 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0146 - accuracy: 1.0000 - val_loss: 1.3302 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.3332 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0145 - accuracy: 1.0000 - val_loss: 1.3319 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.3327 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 1.3348 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.3350 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0141 - accuracy: 1.0000 - val_loss: 1.3343 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 1.3368 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.3443 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.3424 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 1.3364 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 1.3374 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 1.3411 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 1.3378 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.3417 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 1.3423 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 1.3415 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 1.3418 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.3377 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.3423 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.3418 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 1.3400 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.3388 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 1.3425 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0123 - accuracy: 1.0000 - val_loss: 1.3408 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.3397 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.3351 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.3390 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.3358 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.3364 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.3383 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.3367 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.3365 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.3362 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.3341 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.3426 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.3323 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.3409 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.3323 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.3313 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.3364 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 1.3336 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.3272 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.3350 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 1.3310 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.3292 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.3255 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0129 - accuracy: 0.9978 - val_loss: 1.3052 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0117 - accuracy: 0.9989 - val_loss: 1.3585 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0120 - accuracy: 0.9989 - val_loss: 1.2880 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0188 - accuracy: 0.9978 - val_loss: 1.3304 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0137 - accuracy: 0.9978 - val_loss: 1.3050 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0269 - accuracy: 0.9967 - val_loss: 1.2673 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0607 - accuracy: 0.9922 - val_loss: 1.1645 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0871 - accuracy: 0.9856 - val_loss: 1.2803 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0940 - accuracy: 0.9800 - val_loss: 1.0396 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0321 - accuracy: 0.9922 - val_loss: 0.9926 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0166 - accuracy: 0.9978 - val_loss: 1.0339 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0139 - accuracy: 0.9989 - val_loss: 1.0612 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0128 - accuracy: 0.9989 - val_loss: 1.0740 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.0783 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.0941 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.0922 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.0943 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.1044 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0123 - accuracy: 0.9989 - val_loss: 1.1104 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0127 - accuracy: 0.9989 - val_loss: 1.1129 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.1118 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.1162 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.1191 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.1213 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.1236 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.1260 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.1285 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.1238 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.1320 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.1357 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.1383 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.1413 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.1431 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.1468 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.1482 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.1507 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.1531 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.1557 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.1571 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.1595 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.1609 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.1637 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.1668 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.1674 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.1648 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.1680 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.1698 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.1721 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.1736 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.1735 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.1748 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.1779 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.1793 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.1835 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.1849 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.1830 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.1848 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.1846 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.1874 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.1880 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1881 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1894 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.1907 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1908 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1929 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.1912 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1921 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1922 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1942 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1964 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1977 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1975 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1999 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1992 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.2020 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.2017 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 1.2038 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.2033 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.2043 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.2065 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.2082 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.2115 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 1.2103 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.2136 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.2136 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.2147 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.2154 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.2196 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.2194 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.2191 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.2232 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.2215 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.2235 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.2289 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.2283 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.2282 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.2334 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.2275 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2350 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2335 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2382 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2334 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2400 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2348 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2410 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.2397 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2376 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2397 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2396 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2474 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2547 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2494 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2418 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2492 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.2430 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2435 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2488 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2415 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2502 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2485 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2502 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2494 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2502 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2533 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2506 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2533 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2542 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2557 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2506 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2591 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2607 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2530 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2639 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2616 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0737 - accuracy: 0.9922 - val_loss: 1.1973 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.1186 - accuracy: 0.9778 - val_loss: 1.1898 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0380 - accuracy: 0.9889 - val_loss: 1.1558 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0333 - accuracy: 0.9889 - val_loss: 1.0655 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0271 - accuracy: 0.9933 - val_loss: 1.1078 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0128 - accuracy: 0.9989 - val_loss: 1.0678 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.0701 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1057 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.1185 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.1585 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.1653 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.1680 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.1755 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.1875 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.1874 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.1955 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.1988 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2026 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2146 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2120 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2180 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2208 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2261 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.2277 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2337 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2382 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2408 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2442 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2484 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2531 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2551 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2585 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2632 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2633 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2684 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.2717 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2768 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2802 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2835 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2861 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2873 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2897 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2912 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2930 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2931 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2967 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2997 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2969 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.3024 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3000 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3044 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3066 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3081 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3087 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3101 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.3127 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3102 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3136 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3137 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3157 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3151 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3168 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3189 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3180 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3182 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3198 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3226 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3217 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3242 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3231 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3245 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3295 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3290 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3300 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3291 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3289 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3311 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3302 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3293 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3306 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3338 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.3333 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3303 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3319 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3328 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3326 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3359 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3351 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3367 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.3375 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.3374 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.3390 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.3377 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.3373 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3392 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3449 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3359 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3410 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3413 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.3399 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.3460 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.3404 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.3486 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.3369 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3454 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3467 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3429 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3492 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3453 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3454 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3501 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3510 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3522 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3523 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3537 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3423 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.3548 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3183 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.3561 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0268 - accuracy: 0.9967 - val_loss: 1.3195 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0892 - accuracy: 0.9856 - val_loss: 1.1774 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0320 - accuracy: 0.9900 - val_loss: 1.1586 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0317 - accuracy: 0.9944 - val_loss: 1.1599 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0150 - accuracy: 0.9967 - val_loss: 1.0493 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0213 - accuracy: 0.9956 - val_loss: 1.1728 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0211 - accuracy: 0.9956 - val_loss: 1.1347 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0074 - accuracy: 0.9989 - val_loss: 1.0709 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0373 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.0416 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.0448 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.0598 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.0625 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0652 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.89000 to 0.90000, saving model to 210210_TrainingLocalitySensitivewFW\\LocalitySensitivewFW_simBatched\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0630 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0673 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0697 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0734 - val_accuracy: 0.8900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0750 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0884 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0850 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0828 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0848 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0897 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0913 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0911 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0927 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0927 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.0965 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.0986 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1003 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1025 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1045 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1063 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1081 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1114 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1101 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.1129 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1152 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1187 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1177 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1181 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1236 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1239 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1244 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1284 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1292 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1320 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.1331 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1344 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1360 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1385 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1390 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1401 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1404 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1447 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1448 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1529 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.1526 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1522 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1533 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1504 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1535 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1517 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1560 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1539 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1542 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1558 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1553 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1555 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.1653 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1606 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1577 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1578 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1595 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1581 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1585 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1592 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1580 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.1608 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1599 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1585 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1626 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1681 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1697 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1715 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1706 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1660 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.1567 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1570 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1591 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1563 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1602 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1644 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1609 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1653 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1630 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1621 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1601 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1543 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1613 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1604 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1594 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1613 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1619 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1581 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1635 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1685 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1672 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1697 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1650 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1703 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1670 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1716 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.1711 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1693 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1696 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1713 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1694 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1709 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1709 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1666 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1685 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1705 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1594 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1681 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1676 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1679 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1711 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1691 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1731 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1688 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1733 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1721 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1672 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.1879 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0576 - accuracy: 0.9933 - val_loss: 1.5750 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.1635 - accuracy: 0.9722 - val_loss: 1.4347 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0742 - accuracy: 0.9789 - val_loss: 0.9580 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.9859 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0132 - accuracy: 0.9989 - val_loss: 1.0244 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0109 - accuracy: 0.9989 - val_loss: 1.0291 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0108 - accuracy: 0.9989 - val_loss: 1.0419 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 0.9989 - val_loss: 1.0584 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0093 - accuracy: 0.9989 - val_loss: 1.0667 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0091 - accuracy: 0.9989 - val_loss: 1.0742 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0089 - accuracy: 0.9989 - val_loss: 1.0889 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 0.9989 - val_loss: 1.0954 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0087 - accuracy: 0.9989 - val_loss: 1.1069 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0084 - accuracy: 0.9989 - val_loss: 1.1222 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0083 - accuracy: 0.9989 - val_loss: 1.1268 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0082 - accuracy: 0.9989 - val_loss: 1.1353 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0081 - accuracy: 0.9989 - val_loss: 1.1450 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 0.9989 - val_loss: 1.1427 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0080 - accuracy: 0.9989 - val_loss: 1.1593 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.1708 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.1840 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0076 - accuracy: 0.9989 - val_loss: 1.1880 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 0.9989 - val_loss: 1.1925 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 0.9989 - val_loss: 1.2043 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0071 - accuracy: 0.9989 - val_loss: 1.2096 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 0.9989 - val_loss: 1.2057 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 0.9989 - val_loss: 1.2251 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0067 - accuracy: 0.9989 - val_loss: 1.2505 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0075 - accuracy: 0.9989 - val_loss: 1.2062 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0073 - accuracy: 0.9989 - val_loss: 1.2222 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 0.9989 - val_loss: 1.2247 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0062 - accuracy: 0.9989 - val_loss: 1.2329 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 1.2396 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.2517 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.2588 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.2626 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2781 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2852 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2941 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.2607 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0291 - accuracy: 0.9944 - val_loss: 1.1970 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0524 - accuracy: 0.9900 - val_loss: 1.2041 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0171 - accuracy: 0.9967 - val_loss: 1.1555 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0135 - accuracy: 0.9956 - val_loss: 1.1348 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0114 - accuracy: 0.9967 - val_loss: 1.1429 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0113 - accuracy: 0.9967 - val_loss: 1.1764 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0222 - accuracy: 0.9944 - val_loss: 1.1309 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0166 - accuracy: 0.9944 - val_loss: 1.1327 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0085 - accuracy: 0.9989 - val_loss: 1.1477 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.1641 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0072 - accuracy: 0.9989 - val_loss: 1.1692 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 0.9989 - val_loss: 1.1804 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0068 - accuracy: 0.9989 - val_loss: 1.1878 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0065 - accuracy: 0.9989 - val_loss: 1.1902 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0064 - accuracy: 0.9989 - val_loss: 1.1956 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 1.2042 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.2061 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.2106 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.2157 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0058 - accuracy: 0.9989 - val_loss: 1.2260 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2220 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.2320 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2363 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.2414 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.2452 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.2504 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.2561 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.2621 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.2646 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.2736 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2800 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2850 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2943 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.2953 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.3059 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.3077 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.3105 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3164 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3227 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3231 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3283 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3325 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3354 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3356 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.3396 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3370 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3410 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3487 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3480 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3507 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3536 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3544 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3565 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3570 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3562 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3624 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3617 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3592 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3623 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3624 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3632 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3625 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3662 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3678 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3667 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3682 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3669 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3748 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3732 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3758 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3769 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3770 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3756 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3741 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3785 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3759 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3772 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3765 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3751 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3796 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3752 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3751 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3793 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3819 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3790 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3755 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3757 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3728 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3768 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3772 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3737 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3739 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3764 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3740 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3802 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3749 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3762 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3759 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3747 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3722 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3794 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3762 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3735 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3744 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3720 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3755 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3760 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3770 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.3736 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3777 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3747 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3724 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3771 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4265 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3791 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3644 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3662 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3717 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3710 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3666 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3660 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3736 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3754 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3691 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3768 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3833 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3636 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3742 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3745 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3736 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3777 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3767 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3733 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.3765 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3861 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3774 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3832 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3780 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3833 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3850 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3841 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3828 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3824 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3806 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3774 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3724 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3846 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3899 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3856 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3721 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3581 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3808 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3542 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3794 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3609 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3671 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3635 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3661 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3740 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3632 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3494 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.3344 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0606 - accuracy: 0.9911 - val_loss: 1.7479 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.2369 - accuracy: 0.9644 - val_loss: 1.3855 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0951 - accuracy: 0.9778 - val_loss: 1.3393 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0418 - accuracy: 0.9911 - val_loss: 1.1088 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0197 - accuracy: 0.9944 - val_loss: 1.1070 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0123 - accuracy: 0.9989 - val_loss: 1.1795 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0094 - accuracy: 0.9989 - val_loss: 1.1520 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0070 - accuracy: 0.9989 - val_loss: 1.1511 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 1.1380 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1247 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.1376 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.1436 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.1432 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.1489 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1515 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1531 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.1559 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1557 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1594 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1672 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1629 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1665 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1646 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1666 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1684 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1733 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1733 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1742 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1786 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1771 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1812 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1807 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1862 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1849 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1852 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1859 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1903 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1889 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1916 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.1934 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1932 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1963 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1949 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1979 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.1977 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2001 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2007 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2032 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2011 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2018 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2083 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2092 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2108 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2135 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2131 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2138 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2150 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2152 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2180 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2181 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2178 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2204 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2203 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2246 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2250 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2317 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2304 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2296 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2290 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2297 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2321 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2316 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2331 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2319 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.2357 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2359 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2391 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2376 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2388 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2398 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2437 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2430 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2373 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2433 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2446 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2477 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2462 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2497 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2516 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2542 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2548 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2585 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2597 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2585 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.2590 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2619 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2634 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2629 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2610 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2774 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2718 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2680 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2695 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2667 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2587 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2612 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2638 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2651 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2661 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2678 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2703 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2721 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2732 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2734 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2778 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2743 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2701 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2744 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2825 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2863 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2818 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2718 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2826 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2895 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2908 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2876 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.2906 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2907 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2928 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2991 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2913 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2970 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2991 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2958 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2960 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3028 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3030 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3007 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3033 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2962 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3026 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3166 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3148 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3172 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3153 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3176 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3149 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3120 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3193 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3171 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3174 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3184 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3136 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3212 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3258 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3263 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3291 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3199 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3259 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3244 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3285 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3304 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3347 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3453 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3361 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3370 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3340 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3428 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3396 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3484 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3445 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3512 - val_accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3559 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3526 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3648 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3544 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3592 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3601 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3518 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3672 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3558 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0097 - accuracy: 0.9989 - val_loss: 1.6434 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.1400 - accuracy: 0.9767 - val_loss: 1.9156 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.1575 - accuracy: 0.9689 - val_loss: 1.2012 - val_accuracy: 0.8700\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.1175 - accuracy: 0.9733 - val_loss: 1.1712 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0400 - accuracy: 0.9867 - val_loss: 1.3480 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0338 - accuracy: 0.9911 - val_loss: 1.1475 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0248 - accuracy: 0.9956 - val_loss: 1.1375 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0166 - accuracy: 0.9978 - val_loss: 1.1138 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0132 - accuracy: 0.9978 - val_loss: 1.1110 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0090 - accuracy: 0.9978 - val_loss: 1.0618 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.0666 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.0922 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 1.0957 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n",
      "29/29 - 1s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 1.1017 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.90000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#For a trained LSwFW_model (n_epochs=200; shuffling) \n",
    "checkpoint_path=os.path.join(\"210210_TrainingLocalitySensitivewFW\",\n",
    "                                      \"LocalitySensitivewFW_label\" )\n",
    "LSwFW_model.load_weights(checkpoint_path)\n",
    "\n",
    "simbatched_checkpoint_path=os.path.join(\"210210_TrainingLocalitySensitivewFW\",\n",
    "                                      \"LocalitySensitivewFW_simBatched\" )\n",
    "try:\n",
    "    os.mkdir(simbatched_checkpoint_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Set callbacks\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=simbatched_checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',\n",
    "                     \n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "csv_filename = os.path.join(simbatched_checkpoint_path,\n",
    "                            \"training_log_NosimBatched_lr_0_001.csv\"\n",
    "                            )\n",
    "csvlogger_callback = tf.keras.callbacks.CSVLogger(filename=csv_filename, append=True)\n",
    "\n",
    "#Assign learning rate\n",
    "LSwFW_model.optimizer.lr.assign(0.001)\n",
    "\n",
    "n_epoch=1000\n",
    "for i in range(n_epoch):\n",
    "    #No rearranging of tensors\n",
    "    #Fit model\n",
    "    LSwFW_model.fit(train_tensor, \n",
    "                train_targets,\n",
    "                epochs=1,\n",
    "                batch_size=n_batch,\n",
    "                validation_data=(test_tensor, test_targets),\n",
    "                shuffle=True,\n",
    "                verbose=2, \n",
    "                callbacks=[csvlogger_callback,\n",
    "                           cp_callback\n",
    "                          ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processed in excel by combining training and validation accuracy columns from individual training_log.csv\"\n",
    "import pandas as pd\n",
    "SynthDataFolder=\"SynthData_10dim_clusternoise_AddBN\"\n",
    "df=pd.read_csv(os.path.join(\"SynthData10dim_results.csv\"),\n",
    "               index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "fig,ax=plt.subplots()\n",
    "df['logEpoch']=np.log10(df['Epoch'])\n",
    "# sns.lineplot('Step', 'AttentionModelwFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'AttentionModelwoFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'DenseModel_val', data=att_model_df, ax=ax)\n",
    "\n",
    "def get_mov_ave(y, window_size=3, percentiles=(10,90)):\n",
    "    assert window_size%2 ==1\n",
    "    w=int(window_size/2)\n",
    "    out=[]\n",
    "    lower=[]\n",
    "    upper=[]\n",
    "    l,u=percentiles[0],percentiles[1]\n",
    "    for i in range(w, len(y)-w):\n",
    "        out.append(np.average(y[i-w:i+w+1]))\n",
    "        lower.append(np.percentile(y[i-w:i+w+1],l))\n",
    "        upper.append(np.percentile(y[i-w:i+w+1],u))\n",
    "\n",
    "    while w>0:\n",
    "        w=w-1\n",
    "        win_size=w*2+1\n",
    "        out.insert(0, np.average(y[:win_size]))\n",
    "        lower.insert(0, np.percentile(y[:win_size], l))\n",
    "        upper.insert(0, np.percentile(y[:win_size], u))\n",
    "        out.append(np.average(y[len(y)-win_size:]))\n",
    "        lower.append(np.percentile(y[len(y)-win_size:], l))\n",
    "        upper.append(np.percentile(y[len(y)-win_size:], u))\n",
    "    return out, lower, upper\n",
    "\n",
    "def get_min_max(mov_ave, y):\n",
    "    assert len(mov_ave)==len(y)\n",
    "    mins=np.min(np.vstack([mov_ave, y]), axis=0)\n",
    "    maxs=np.max(np.vstack([mov_ave,y]), axis=0)\n",
    "    return mins, maxs\n",
    "\n",
    "colors1=[\"b\",\"g\",\"r\",\"y\",\"k\"]\n",
    "colors2=[\"powderblue\",\"palegreen\",\"lightsalmon\",\"bisque\",\"lightslategray\"]\n",
    "for idx, i in enumerate([\"LLDLwFW_valacc\",\n",
    "#           \"LLDLwoFW_valacc\", \n",
    "#           \"DenseModel_valacc\"\n",
    "                         \"LLDLwFW_simBatched_valacc\",\n",
    "                         \"LLDLwFW_NosimBatched_valacc\",\n",
    "                         \"LLDLwFW_simBatched_lr_0_001_valacc\",\n",
    "                         \"LLDLwFW_NosimBatched_lr_0_001_valacc\"\n",
    "         ]):\n",
    "    x_plot=df['Epoch']\n",
    "    y_plot=df[i]\n",
    "    mov_ave, lower, upper=get_mov_ave(y_plot, window_size=15)\n",
    "    plt.plot(x_plot, mov_ave, c=colors1[idx])\n",
    "    #mins,maxs=get_min_max(mov_ave, get_mov_ave(y_plot, window_size=3))\n",
    "    plt.fill_between(x_plot,lower, upper, color=colors2[idx], alpha=0.3)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim([0.45, 0.95])\n",
    "ax.legend([\"Locality-adaptive Deep Learner\", \n",
    "           \"Similarity Batching\",\n",
    "           \"No Similarity Batching\",\n",
    "           \"Similarity Batching_lowLR\",\n",
    "           \"No Similarity Batching_lowLR\"\n",
    "          ], \n",
    "          bbox_to_anchor=(1,1))\n",
    "ax.set_title(\"10-D Synthetic Data with cluster-specific noise\")\n",
    "plt.grid()\n",
    "# savefile=os.path.join(SynthDataFolder, \"SynthData_10dim_LocallyAdaptiveDeepLearner\")\n",
    "# plt.savefig(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try turning learning rate down to 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try on unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "fig,ax=plt.subplots()\n",
    "df['logEpoch']=np.log10(df['Epoch'])\n",
    "# sns.lineplot('Step', 'AttentionModelwFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'AttentionModelwoFeatWeights_val', data=att_model_df, ax=ax)\n",
    "# sns.lineplot('Step', 'DenseModel_val', data=att_model_df, ax=ax)\n",
    "\n",
    "def get_mov_ave(y, window_size=3, percentiles=(10,90)):\n",
    "    assert window_size%2 ==1\n",
    "    w=int(window_size/2)\n",
    "    out=[]\n",
    "    lower=[]\n",
    "    upper=[]\n",
    "    l,u=percentiles[0],percentiles[1]\n",
    "    for i in range(w, len(y)-w):\n",
    "        out.append(np.average(y[i-w:i+w+1]))\n",
    "        lower.append(np.percentile(y[i-w:i+w+1],l))\n",
    "        upper.append(np.percentile(y[i-w:i+w+1],u))\n",
    "\n",
    "    while w>0:\n",
    "        w=w-1\n",
    "        win_size=w*2+1\n",
    "        out.insert(0, np.average(y[:win_size]))\n",
    "        lower.insert(0, np.percentile(y[:win_size], l))\n",
    "        upper.insert(0, np.percentile(y[:win_size], u))\n",
    "        out.append(np.average(y[len(y)-win_size:]))\n",
    "        lower.append(np.percentile(y[len(y)-win_size:], l))\n",
    "        upper.append(np.percentile(y[len(y)-win_size:], u))\n",
    "    return out, lower, upper\n",
    "\n",
    "def get_min_max(mov_ave, y):\n",
    "    assert len(mov_ave)==len(y)\n",
    "    mins=np.min(np.vstack([mov_ave, y]), axis=0)\n",
    "    maxs=np.max(np.vstack([mov_ave,y]), axis=0)\n",
    "    return mins, maxs\n",
    "\n",
    "colors1=[\"b\",\"g\",\"r\",\"y\",\"k\"]\n",
    "colors2=[\"powderblue\",\"palegreen\",\"lightsalmon\",\"bisque\",\"lightslategray\"]\n",
    "for idx, i in enumerate([\"LLDLwFW_valacc\",\n",
    "#           \"LLDLwoFW_valacc\", \n",
    "#           \"DenseModel_valacc\"\n",
    "#                          \"LLDLwFW_simBatched_valacc\",\n",
    "#                          \"LLDLwFW_NosimBatched_valacc\",\n",
    "                         \"LLDLwFW_simBatched_lr_0_001_valacc\",\n",
    "                         \"LLDLwFW_NosimBatched_lr_0_001_valacc\"\n",
    "         ]):\n",
    "    x_plot=df['Epoch']\n",
    "    y_plot=df[i]\n",
    "    mov_ave, lower, upper=get_mov_ave(y_plot, window_size=15)\n",
    "    plt.plot(x_plot, mov_ave, c=colors1[idx])\n",
    "    #mins,maxs=get_min_max(mov_ave, get_mov_ave(y_plot, window_size=3))\n",
    "    plt.fill_between(x_plot,lower, upper, color=colors2[idx], alpha=0.3)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim([0.45, 0.95])\n",
    "ax.legend([\"Locality-adaptive Deep Learner\", \n",
    "#            \"Similarity Batching\",\n",
    "#            \"No Similarity Batching\",\n",
    "           \"Similarity Batching\",\n",
    "           \"No Similarity Batching\"\n",
    "          ], \n",
    "          bbox_to_anchor=(1,1))\n",
    "ax.set_title(\"10-D Synthetic Data with cluster-specific noise\")\n",
    "plt.grid()\n",
    "# savefile=os.path.join(SynthDataFolder, \"SynthData_10dim_LocallyAdaptiveDeepLearner\")\n",
    "# plt.savefig(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
